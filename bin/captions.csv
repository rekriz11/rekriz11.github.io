Input.num_items,Input.title_0,Input.url_0,Input.id_0,Input.type_0,Input.num_0,Input.dropbox_0,Input.filename_0,Input.title_1,Input.url_1,Input.id_1,Input.type_1,Input.num_1,Input.dropbox_1,Input.filename_1,Input.title_2,Input.url_2,Input.id_2,Input.type_2,Input.num_2,Input.dropbox_2,Input.filename_2,Input.title_3,Input.url_3,Input.id_3,Input.type_3,Input.num_3,Input.dropbox_3,Input.filename_3,Input.title_4,Input.url_4,Input.id_4,Input.type_4,Input.num_4,Input.dropbox_4,Input.filename_4,Answer.caption_0,Answer.caption_1,Answer.caption_2,Answer.caption_3,Answer.caption_4
5,Moses&colon; Open source toolkit for statistical machine translation,http://cis.upenn.edu/~ccb/publications/moses-toolkit.pdf,moses-toolkit,Figure,3,https://www.dropbox.com/request/74RK0QcO1OhiPaaOpzoN,moses-toolkit-figure-3.pdf,Findings of the 2011 Workshop on Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-the-wmt11-shared-tasks.pdf,findings-of-the-wmt11-shared-tasks,Figure,1,https://www.dropbox.com/request/61mdnFHXY23xK31YGUrj,findings-of-the-wmt11-shared-tasks-figure-1.pdf,Findings of the 2011 Workshop on Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-the-wmt11-shared-tasks.pdf,findings-of-the-wmt11-shared-tasks,Table,1,https://www.dropbox.com/request/61mdnFHXY23xK31YGUrj,findings-of-the-wmt11-shared-tasks-table-1.pdf,Findings of the 2011 Workshop on Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-the-wmt11-shared-tasks.pdf,findings-of-the-wmt11-shared-tasks,Table,2,https://www.dropbox.com/request/61mdnFHXY23xK31YGUrj,findings-of-the-wmt11-shared-tasks-table-2.pdf,Findings of the 2011 Workshop on Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-the-wmt11-shared-tasks.pdf,findings-of-the-wmt11-shared-tasks,Table,3,https://www.dropbox.com/request/61mdnFHXY23xK31YGUrj,findings-of-the-wmt11-shared-tasks-table-3.pdf,Example of graph of decoding steps,Statistics for the training and test sets used in the translation task. The number of words and the number of distinct words (case-insensitive) is based on the provided tokenizer.,"Participants in the shared translation task (European language pairs; individual system track). Not all teams participated in all language pairs. The translations from commercial and online systems were crawled by us, not submitted by the respective companies, and are therefore anonymized.","Participants in the shared system combination task. Not all teams participated in all language pairs.
∗ The Quaero Project entry combined outputs they received directly from LIMSI, KIT, SYSTRAN, and RWTH.",Participants in the featured translation task (Haitian Creole SMS into English; individual system track). Not all teams participated in both the ‘Clean’ and ‘Raw’ tracks.
5,Findings of the 2011 Workshop on Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-the-wmt11-shared-tasks.pdf,findings-of-the-wmt11-shared-tasks,Table,4,https://www.dropbox.com/request/61mdnFHXY23xK31YGUrj,findings-of-the-wmt11-shared-tasks-table-4.pdf,Findings of the 2011 Workshop on Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-the-wmt11-shared-tasks.pdf,findings-of-the-wmt11-shared-tasks,Table,5,https://www.dropbox.com/request/61mdnFHXY23xK31YGUrj,findings-of-the-wmt11-shared-tasks-table-5.pdf,Findings of the 2011 Workshop on Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-the-wmt11-shared-tasks.pdf,findings-of-the-wmt11-shared-tasks,Table,6,https://www.dropbox.com/request/61mdnFHXY23xK31YGUrj,findings-of-the-wmt11-shared-tasks-table-6.pdf,Findings of the 2011 Workshop on Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-the-wmt11-shared-tasks.pdf,findings-of-the-wmt11-shared-tasks,Table,11,https://www.dropbox.com/request/61mdnFHXY23xK31YGUrj,findings-of-the-wmt11-shared-tasks-table-11.pdf,Findings of the 2009 Workshop on Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-the-wmt09-shared-tasks.pdf,findings-of-the-wmt09-shared-tasks,Figure,1,https://www.dropbox.com/request/WS20ewvmCqwfCemCkv5y,findings-of-the-wmt09-shared-tasks-figure-1.pdf,"Examples of some of the Haitian Creole SMS messages that were sent to the 4636 short code along with their translations into English. Translations were done by volunteers who wanted to help with the relief effort. Prior to being distributed, the messages were anonymized to remove names, phone numbers, email addresses, etc. The anonymization guidelines specified that addresses be retained to facilitate work on mapping technologies.","Training data for the Haitian Creole-English featured translation task. The in-domain SMS data consists primarily of raw (noisy) SMS data. The in-domain data was provided by Mission 4636. The other data is out-of- domain. It comes courtesy of Carnegie Mellon University, Microsoft Research, Haitisurf.com, and Krengle.net.","A summary of the WMT11 ranking task, showing the number of systems and number of labels collected in each of the individual and system combination tracks. The system count does not include the reference translation, which was included in the evaluation, and so a value under “Labels per System” can be obtained only after adding 1 to the system count, before dividing the label count (e.g. in German-English, 4, 620/21 = 220.0).","Participants in the evaluation shared task. For comparison purposes, we include the BLEU and TER metrics as baselines.",Statistics for the training and test sets used in the translation task. The number of words is based on the provided tokenizer and the number of distinct words is the based on lowercased tokens.
5,Findings of the 2009 Workshop on Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-the-wmt09-shared-tasks.pdf,findings-of-the-wmt09-shared-tasks,Figure,2,https://www.dropbox.com/request/WS20ewvmCqwfCemCkv5y,findings-of-the-wmt09-shared-tasks-figure-2.pdf,Findings of the 2009 Workshop on Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-the-wmt09-shared-tasks.pdf,findings-of-the-wmt09-shared-tasks,Figure,3,https://www.dropbox.com/request/WS20ewvmCqwfCemCkv5y,findings-of-the-wmt09-shared-tasks-figure-3.pdf,Findings of the 2009 Workshop on Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-the-wmt09-shared-tasks.pdf,findings-of-the-wmt09-shared-tasks,Figure,6,https://www.dropbox.com/request/WS20ewvmCqwfCemCkv5y,findings-of-the-wmt09-shared-tasks-figure-6.pdf,Findings of the 2009 Workshop on Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-the-wmt09-shared-tasks.pdf,findings-of-the-wmt09-shared-tasks,Table,1,https://www.dropbox.com/request/WS20ewvmCqwfCemCkv5y,findings-of-the-wmt09-shared-tasks-table-1.pdf,Findings of the 2009 Workshop on Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-the-wmt09-shared-tasks.pdf,findings-of-the-wmt09-shared-tasks,Table,2,https://www.dropbox.com/request/WS20ewvmCqwfCemCkv5y,findings-of-the-wmt09-shared-tasks-table-2.pdf,This screenshot shows an annotator editing the output of a machine translation system.,This screenshot shows an annotator judging the acceptability of edited translations.,The percent of time that each system’s edited output was judged to be an acceptable translation. These numbers also include judgments of the system’s output when it was marked either incomprehensible or acceptable and left unedited. Note that the reference translation was edited alongside the system outputs. Error bars show one positive and one negative standard deviation for the systems in that language pair.,Participants in the shared translation task. Not all groups participated in all language pairs.,Participants in the system combination task.
5,Findings of the 2009 Workshop on Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-the-wmt09-shared-tasks.pdf,findings-of-the-wmt09-shared-tasks,Table,3,https://www.dropbox.com/request/WS20ewvmCqwfCemCkv5y,findings-of-the-wmt09-shared-tasks-table-3.pdf,Findings of the 2009 Workshop on Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-the-wmt09-shared-tasks.pdf,findings-of-the-wmt09-shared-tasks,Table,4,https://www.dropbox.com/request/WS20ewvmCqwfCemCkv5y,findings-of-the-wmt09-shared-tasks-table-4.pdf,Findings of the 2009 Workshop on Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-the-wmt09-shared-tasks.pdf,findings-of-the-wmt09-shared-tasks,Table,5,https://www.dropbox.com/request/WS20ewvmCqwfCemCkv5y,findings-of-the-wmt09-shared-tasks-table-5.pdf,Findings of the 2009 Workshop on Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-the-wmt09-shared-tasks.pdf,findings-of-the-wmt09-shared-tasks,Table,6,https://www.dropbox.com/request/WS20ewvmCqwfCemCkv5y,findings-of-the-wmt09-shared-tasks-table-6.pdf,Findings of the 2009 Workshop on Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-the-wmt09-shared-tasks.pdf,findings-of-the-wmt09-shared-tasks,Table,7,https://www.dropbox.com/request/WS20ewvmCqwfCemCkv5y,findings-of-the-wmt09-shared-tasks-table-7.pdf,The number of items that were judged for each task during the manual evaluation.,Inter- and intra-annotator agreement for the two types of manual evaluation,"A comparison between the best system combinations and the best individual systems. It was generally difficult to draw a statistically significant differences between the two groups, and between the combinations themselves.","Official results for the WMT09 translation task, based on the human evaluation (ranking translations relative to each other)",The system-level correlation of the automatic evaluation metrics with the human judgments for translation into English.
5,Findings of the 2009 Workshop on Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-the-wmt09-shared-tasks.pdf,findings-of-the-wmt09-shared-tasks,Table,8,https://www.dropbox.com/request/WS20ewvmCqwfCemCkv5y,findings-of-the-wmt09-shared-tasks-table-8.pdf,Findings of the 2009 Workshop on Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-the-wmt09-shared-tasks.pdf,findings-of-the-wmt09-shared-tasks,Table,9,https://www.dropbox.com/request/WS20ewvmCqwfCemCkv5y,findings-of-the-wmt09-shared-tasks-table-9.pdf,Findings of the 2009 Workshop on Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-the-wmt09-shared-tasks.pdf,findings-of-the-wmt09-shared-tasks,Table,10,https://www.dropbox.com/request/WS20ewvmCqwfCemCkv5y,findings-of-the-wmt09-shared-tasks-table-10.pdf,Findings of the 2009 Workshop on Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-the-wmt09-shared-tasks.pdf,findings-of-the-wmt09-shared-tasks,Table,11,https://www.dropbox.com/request/WS20ewvmCqwfCemCkv5y,findings-of-the-wmt09-shared-tasks-table-11.pdf,Paraphrasing with Bilingual Parallel Corpora,http://cis.upenn.edu/~ccb/publications/paraphrasing-with-bilingual-parallel-corpora.pdf,paraphrasing-with-bilingual-parallel-corpora,Figure,1,https://www.dropbox.com/request/HyKRFabZDOD6JLTGcAez,paraphrasing-with-bilingual-parallel-corpora-figure-1.pdf,The system-level correlation of the automatic evaluation metrics with the human judgements for translation out of English.,The system-level correlation for automatic metrics ranking five English-Czech systems,Sentence-level consistency of the automatic metrics with human judgments for translations into English. Italicized numbers do not beat the random-choice baseline. (This table was corrected after publication.),Sentence-level consistency of the automatic metrics with human judgments for translations out of English. Italicized numbers do not beat the random-choice baseline. (This table was corrected after publication.),Using a monolingual parallel corpus to extract paraphrases
5,Paraphrasing with Bilingual Parallel Corpora,http://cis.upenn.edu/~ccb/publications/paraphrasing-with-bilingual-parallel-corpora.pdf,paraphrasing-with-bilingual-parallel-corpora,Figure,2,https://www.dropbox.com/request/HyKRFabZDOD6JLTGcAez,paraphrasing-with-bilingual-parallel-corpora-figure-2.pdf,Paraphrasing with Bilingual Parallel Corpora,http://cis.upenn.edu/~ccb/publications/paraphrasing-with-bilingual-parallel-corpora.pdf,paraphrasing-with-bilingual-parallel-corpora,Figure,3,https://www.dropbox.com/request/HyKRFabZDOD6JLTGcAez,paraphrasing-with-bilingual-parallel-corpora-figure-3.pdf,Fast Cheap and Creative&colon; Evaluating Translation Quality Using Amazon's Mechanical Turk,http://cis.upenn.edu/~ccb/publications/mechanical-turk-for-machine-translation-evaluation.pdf,mechanical-turk-for-machine-translation-evaluation,Figure,2,https://www.dropbox.com/request/VltxwB9gFmcmXhavx1zy,mechanical-turk-for-machine-translation-evaluation-figure-2.pdf,Fast Cheap and Creative&colon; Evaluating Translation Quality Using Amazon's Mechanical Turk,http://cis.upenn.edu/~ccb/publications/mechanical-turk-for-machine-translation-evaluation.pdf,mechanical-turk-for-machine-translation-evaluation,Figure,3,https://www.dropbox.com/request/VltxwB9gFmcmXhavx1zy,mechanical-turk-for-machine-translation-evaluation-figure-3.pdf,Fast Cheap and Creative&colon; Evaluating Translation Quality Using Amazon's Mechanical Turk,http://cis.upenn.edu/~ccb/publications/mechanical-turk-for-machine-translation-evaluation.pdf,mechanical-turk-for-machine-translation-evaluation,Figure,4,https://www.dropbox.com/request/VltxwB9gFmcmXhavx1zy,mechanical-turk-for-machine-translation-evaluation-figure-4.pdf,Using a bilingual parallel corpus to extract paraphrases,Phrases highlighted for manual alignment,"The agreement of individual Turkers with the experts. The most prolific Turker performed barely above chance, indicating random clicking. This suggests that users who contribute more tend to have lower quality.",Correlation with experts’ ranking of systems. All of the different ways of combining the non-expert judgments perform at the upper bound of expert-expert correlation. All correlate more strongly than Bleu.,"Bleu scores quantifying the quality of Turkers’ translations. The chart shows the average Bleu score when one LDC translator is compared against the other 10 translators (or the other 2 translators in the case of Urdu). This gives an upper bound on the expected quality. The Turkers’ translation quality falls within a standard deviation of LDC translators for Spanish, German and Chinese. For all languages, Turkers produce significantly better translations than an online machine translation system."
5,Fast Cheap and Creative&colon; Evaluating Translation Quality Using Amazon's Mechanical Turk,http://cis.upenn.edu/~ccb/publications/mechanical-turk-for-machine-translation-evaluation.pdf,mechanical-turk-for-machine-translation-evaluation,Table,1,https://www.dropbox.com/request/VltxwB9gFmcmXhavx1zy,mechanical-turk-for-machine-translation-evaluation-table-1.pdf,Fast Cheap and Creative&colon; Evaluating Translation Quality Using Amazon's Mechanical Turk,http://cis.upenn.edu/~ccb/publications/mechanical-turk-for-machine-translation-evaluation.pdf,mechanical-turk-for-machine-translation-evaluation,Table,2,https://www.dropbox.com/request/VltxwB9gFmcmXhavx1zy,mechanical-turk-for-machine-translation-evaluation-table-2.pdf,Fast Cheap and Creative&colon; Evaluating Translation Quality Using Amazon's Mechanical Turk,http://cis.upenn.edu/~ccb/publications/mechanical-turk-for-machine-translation-evaluation.pdf,mechanical-turk-for-machine-translation-evaluation,Table,3,https://www.dropbox.com/request/VltxwB9gFmcmXhavx1zy,mechanical-turk-for-machine-translation-evaluation-table-3.pdf,Edinburgh System Description for the 2005 IWSLT Speech Translation Evaluation,http://cis.upenn.edu/~ccb/publications/iwslt05-report.pdf,iwslt05-report,Figure,1,https://www.dropbox.com/request/ukJ263jwOLgFgvJqSaMp,iwslt05-report-figure-1.pdf,Edinburgh System Description for the 2005 IWSLT Speech Translation Evaluation,http://cis.upenn.edu/~ccb/publications/iwslt05-report.pdf,iwslt05-report,Figure,2,https://www.dropbox.com/request/ukJ263jwOLgFgvJqSaMp,iwslt05-report-figure-2.pdf,Self-reported demographic information from Turkers who completed the translation HIT. The statistics on the left are for people who appeared to do the task honestly. The statistics on the right are for people who appeared to be using MT (marked as using it 20% or more in the Detect MT HIT).,HTER scores for five MT systems. The edit rate decreases as the number of editors in- creases from zero (where HTER is simply the TER score between the MT output and the reference translation) and five.,The results of evaluating the MT output using a reading comprehension test,"Phrase-based SMT: Input is segmented into phrases, each is mapped into output phrase and may be reordered","Obtaining a high precision, low recall word alignment by intersecting two GIZA++ alignments"
5,Edinburgh System Description for the 2005 IWSLT Speech Translation Evaluation,http://cis.upenn.edu/~ccb/publications/iwslt05-report.pdf,iwslt05-report,Figure,3,https://www.dropbox.com/request/ukJ263jwOLgFgvJqSaMp,iwslt05-report-figure-3.pdf,Edinburgh System Description for the 2005 IWSLT Speech Translation Evaluation,http://cis.upenn.edu/~ccb/publications/iwslt05-report.pdf,iwslt05-report,Figure,4,https://www.dropbox.com/request/ukJ263jwOLgFgvJqSaMp,iwslt05-report-figure-4.pdf,Edinburgh System Description for the 2005 IWSLT Speech Translation Evaluation,http://cis.upenn.edu/~ccb/publications/iwslt05-report.pdf,iwslt05-report,Figure,5,https://www.dropbox.com/request/ukJ263jwOLgFgvJqSaMp,iwslt05-report-figure-5.pdf,Edinburgh System Description for the 2005 IWSLT Speech Translation Evaluation,http://cis.upenn.edu/~ccb/publications/iwslt05-report.pdf,iwslt05-report,Figure,6,https://www.dropbox.com/request/ukJ263jwOLgFgvJqSaMp,iwslt05-report-figure-6.pdf,Edinburgh System Description for the 2005 IWSLT Speech Translation Evaluation,http://cis.upenn.edu/~ccb/publications/iwslt05-report.pdf,iwslt05-report,Table,1,https://www.dropbox.com/request/ukJ263jwOLgFgvJqSaMp,iwslt05-report-table-1.pdf,"Adding additional alignment points. Potential points are points in the union of the two GIZA++ alignments (grey). In the growing step, neighbouring points are added, when they connect at least one unaligned word. In a final step outlying points may be added (see Section 2.3).",Figure 4: Pseudo-code of the grow-diag-final method to symmetrise word alignments. See Section 2.3 for variations of this method.,Definition of consistent word alignments: Words of an extracted phrase pair have to be aligned to each other and nothing else,"Possible orientations of phrases: monotone (m), swap (s), or discontinuous (d)","Different word alignment methods and the effect of the phrase table: Since alignment points restrict possible phrase pairs, fewer alignment points lead to larger phrase tables."
5,Edinburgh System Description for the 2005 IWSLT Speech Translation Evaluation,http://cis.upenn.edu/~ccb/publications/iwslt05-report.pdf,iwslt05-report,Table,2,https://www.dropbox.com/request/ukJ263jwOLgFgvJqSaMp,iwslt05-report-table-2.pdf,Edinburgh System Description for the 2005 IWSLT Speech Translation Evaluation,http://cis.upenn.edu/~ccb/publications/iwslt05-report.pdf,iwslt05-report,Table,3,https://www.dropbox.com/request/ukJ263jwOLgFgvJqSaMp,iwslt05-report-table-3.pdf,Improved Statistical Machine Translation Using Paraphrases,http://cis.upenn.edu/~ccb/publications/improved-statistical-machine-translation-using-paraphrases.pdf,improved-statistical-machine-translation-using-paraphrases,Figure,5,https://www.dropbox.com/request/wWKFUTP281Fi4thTrIDA,improved-statistical-machine-translation-using-paraphrases-figure-5.pdf,Improved Statistical Machine Translation Using Paraphrases,http://cis.upenn.edu/~ccb/publications/improved-statistical-machine-translation-using-paraphrases.pdf,improved-statistical-machine-translation-using-paraphrases,Figure,6,https://www.dropbox.com/request/wWKFUTP281Fi4thTrIDA,improved-statistical-machine-translation-using-paraphrases-figure-6.pdf,Improved Statistical Machine Translation Using Paraphrases,http://cis.upenn.edu/~ccb/publications/improved-statistical-machine-translation-using-paraphrases.pdf,improved-statistical-machine-translation-using-paraphrases,Table,2,https://www.dropbox.com/request/wWKFUTP281Fi4thTrIDA,improved-statistical-machine-translation-using-paraphrases-table-2.pdf,BLEU scores for systems trained using different alignment methods,"Best lexicalised reordering methods, compared against the baseline (using only distance-based reordering penalty): Improvements for all language pairs",{},{},"Bleu scores for the various training corpora, including baseline results without paraphrasing, results for only paraphrasing unknown words, and results for paraphrasing any unseen phrase. Corpus size is measured in sentences."
5,Improved Statistical Machine Translation Using Paraphrases,http://cis.upenn.edu/~ccb/publications/improved-statistical-machine-translation-using-paraphrases.pdf,improved-statistical-machine-translation-using-paraphrases,Table,3,https://www.dropbox.com/request/wWKFUTP281Fi4thTrIDA,improved-statistical-machine-translation-using-paraphrases-table-3.pdf,Further Meta-Evaluation of Machine Translation,http://cis.upenn.edu/~ccb/publications/further-meta-evaluation-of-machine-tranlsion.pdf,further-meta-evaluation-of-machine-tranlsion,Figure,1,https://www.dropbox.com/request/q4ilRAEHrm1jw2bEswiR,further-meta-evaluation-of-machine-tranlsion-figure-1.pdf,Further Meta-Evaluation of Machine Translation,http://cis.upenn.edu/~ccb/publications/further-meta-evaluation-of-machine-tranlsion.pdf,further-meta-evaluation-of-machine-tranlsion,Figure,2,https://www.dropbox.com/request/q4ilRAEHrm1jw2bEswiR,further-meta-evaluation-of-machine-tranlsion-figure-2.pdf,Further Meta-Evaluation of Machine Translation,http://cis.upenn.edu/~ccb/publications/further-meta-evaluation-of-machine-tranlsion.pdf,further-meta-evaluation-of-machine-tranlsion,Figure,3,https://www.dropbox.com/request/q4ilRAEHrm1jw2bEswiR,further-meta-evaluation-of-machine-tranlsion-figure-3.pdf,Further Meta-Evaluation of Machine Translation,http://cis.upenn.edu/~ccb/publications/further-meta-evaluation-of-machine-tranlsion.pdf,further-meta-evaluation-of-machine-tranlsion,Table,1,https://www.dropbox.com/request/q4ilRAEHrm1jw2bEswiR,further-meta-evaluation-of-machine-tranlsion-table-1.pdf,"Bleu scores for the various training corpora, when the paraphrase feature function is not included","Properties of the training and test sets used in the shared task. The training data is drawn from the Europarl corpus and from the Project Syndicate, a web site which collects political commentary in multiple languages. For Czech and Hungarian we use other available parallel corpora. Note that the number of words is computed based on the provided tokenizer and that the number of distinct words is the based on lowercased tokens.","In constituent-based evaluation, the source sentence was parsed, and automatically aligned with the reference translation and systems’ translations",Distributions of the amount of time it took to judge single sentences for the three types of man- ual evaluation,"Difficulty of the test set parts based on the original language. For each part, we average BLEU scores from the Edinburgh systems for 12 language pairs of the shared task."
5,Further Meta-Evaluation of Machine Translation,http://cis.upenn.edu/~ccb/publications/further-meta-evaluation-of-machine-tranlsion.pdf,further-meta-evaluation-of-machine-tranlsion,Table,2,https://www.dropbox.com/request/q4ilRAEHrm1jw2bEswiR,further-meta-evaluation-of-machine-tranlsion-table-2.pdf,Further Meta-Evaluation of Machine Translation,http://cis.upenn.edu/~ccb/publications/further-meta-evaluation-of-machine-tranlsion.pdf,further-meta-evaluation-of-machine-tranlsion,Table,3,https://www.dropbox.com/request/q4ilRAEHrm1jw2bEswiR,further-meta-evaluation-of-machine-tranlsion-table-3.pdf,Further Meta-Evaluation of Machine Translation,http://cis.upenn.edu/~ccb/publications/further-meta-evaluation-of-machine-tranlsion.pdf,further-meta-evaluation-of-machine-tranlsion,Table,4,https://www.dropbox.com/request/q4ilRAEHrm1jw2bEswiR,further-meta-evaluation-of-machine-tranlsion-table-4.pdf,Further Meta-Evaluation of Machine Translation,http://cis.upenn.edu/~ccb/publications/further-meta-evaluation-of-machine-tranlsion.pdf,further-meta-evaluation-of-machine-tranlsion,Table,5,https://www.dropbox.com/request/q4ilRAEHrm1jw2bEswiR,further-meta-evaluation-of-machine-tranlsion-table-5.pdf,Further Meta-Evaluation of Machine Translation,http://cis.upenn.edu/~ccb/publications/further-meta-evaluation-of-machine-tranlsion.pdf,further-meta-evaluation-of-machine-tranlsion,Table,6,https://www.dropbox.com/request/q4ilRAEHrm1jw2bEswiR,further-meta-evaluation-of-machine-tranlsion-table-6.pdf,Participants in the shared translation task. Not all groups participated in all language pairs.,The number of items that were judged for each task during the manual evaluation. The All-English judgments were reused in the News task for individual language pairs.,Summary results for the sentence ranking judgments. The numbers report the percent of time that each system was judged to be greater than or equal to any other system. Bold indicates the highest score for that task.,Summary results for the constituent ranking judgments. The numbers report the percent of time that each system was judged to be greater than or equal to any other system. Bold indicates the highest score for that task.,Summary results for the Yes/No judgments for constituent translations judgments. The numbers report the percent of each system’s translations that were judged to be acceptable. Bold indicates the highest score for that task.
5,Further Meta-Evaluation of Machine Translation,http://cis.upenn.edu/~ccb/publications/further-meta-evaluation-of-machine-tranlsion.pdf,further-meta-evaluation-of-machine-tranlsion,Table,7,https://www.dropbox.com/request/q4ilRAEHrm1jw2bEswiR,further-meta-evaluation-of-machine-tranlsion-table-7.pdf,Further Meta-Evaluation of Machine Translation,http://cis.upenn.edu/~ccb/publications/further-meta-evaluation-of-machine-tranlsion.pdf,further-meta-evaluation-of-machine-tranlsion,Table,8,https://www.dropbox.com/request/q4ilRAEHrm1jw2bEswiR,further-meta-evaluation-of-machine-tranlsion-table-8.pdf,Further Meta-Evaluation of Machine Translation,http://cis.upenn.edu/~ccb/publications/further-meta-evaluation-of-machine-tranlsion.pdf,further-meta-evaluation-of-machine-tranlsion,Table,9,https://www.dropbox.com/request/q4ilRAEHrm1jw2bEswiR,further-meta-evaluation-of-machine-tranlsion-table-9.pdf,Further Meta-Evaluation of Machine Translation,http://cis.upenn.edu/~ccb/publications/further-meta-evaluation-of-machine-tranlsion.pdf,further-meta-evaluation-of-machine-tranlsion,Table,10,https://www.dropbox.com/request/q4ilRAEHrm1jw2bEswiR,further-meta-evaluation-of-machine-tranlsion-table-10.pdf,Further Meta-Evaluation of Machine Translation,http://cis.upenn.edu/~ccb/publications/further-meta-evaluation-of-machine-tranlsion.pdf,further-meta-evaluation-of-machine-tranlsion,Table,11,https://www.dropbox.com/request/q4ilRAEHrm1jw2bEswiR,further-meta-evaluation-of-machine-tranlsion-table-11.pdf,The average number of times that each system was judged to be better than or equal to all other systems in the sentence ranking task for the All-English condition. The subscript indicates the source language of the system.,Average system-level correlations for the automatic evaluation metrics on translations into English (This table was corrected after publication),"Average system-level correlations for the automatic evaluation metrics on translations into French, German and Spanish (This table was corrected after publication)",The percent of time that each automatic metric was consistent with human judgments for translations into English,The percent of time that each automatic metric was consistent with human judgments for translations into other languages
5,Creating Speech and Language Data With Amazon’s Mechanical Turk,http://cis.upenn.edu/~ccb/publications/creating-speech-and-language-data-with-amazon-mechanical-turk.pdf,creating-speech-and-language-data-with-amazon-mechanical-turk,Figure,1,https://www.dropbox.com/request/YHMZsAfedcCYt91j6nBo,creating-speech-and-language-data-with-amazon-mechanical-turk-figure-1.pdf,Joshua&colon; An Open Source Toolkit for Parsing-based Machine Translation,http://cis.upenn.edu/~ccb/publications/joshua-open-source-toolkit-for-statistical-machine-translation.pdf,joshua-open-source-toolkit-for-statistical-machine-translation,Table,1,https://www.dropbox.com/request/YGAzsHp9drlaVArvHShv,joshua-open-source-toolkit-for-statistical-machine-translation-table-1.pdf,Crowdsourcing Translation&colon; Professional Quality from Non-Professionals,http://cis.upenn.edu/~ccb/publications/crowdsourcing-translation.pdf,crowdsourcing-translation,Figure,1,https://www.dropbox.com/request/734EpCoDU2C8A49ghnHT,crowdsourcing-translation-figure-1.pdf,Crowdsourcing Translation&colon; Professional Quality from Non-Professionals,http://cis.upenn.edu/~ccb/publications/crowdsourcing-translation.pdf,crowdsourcing-translation,Figure,2,https://www.dropbox.com/request/734EpCoDU2C8A49ghnHT,crowdsourcing-translation-figure-2.pdf,Crowdsourcing Translation&colon; Professional Quality from Non-Professionals,http://cis.upenn.edu/~ccb/publications/crowdsourcing-translation.pdf,crowdsourcing-translation,Figure,3,https://www.dropbox.com/request/734EpCoDU2C8A49ghnHT,crowdsourcing-translation-figure-3.pdf,"Time spent, HITs completed, and amount earned from a survey of 1,000 Turkers by Ipeirotis (2010).","The uncased BLEU scores on WMT-09 French-English Task. The test set consists of 2525 segments, each with one reference translation.",A comparison of professional translations provided by the LDC to non-professional translations created on Mechanical Turk.,"We redundantly translate each source sentence by soliciting multiple translations from different Turkers. These translations are put through a subsequent editing set, where multiple edited versions are produced. We select the best translation from the set using features that predict the quality of each translation and each translator.","BLEU scores for different selection methods, measured against the reference sets. Each score is an average of four BLEU scores, each calculated against three LDC reference translations. The five right-most bars are colored in orange to indicate selection over a set that includes both original translations as well as edited versions of them."
5,Crowdsourcing Translation&colon; Professional Quality from Non-Professionals,http://cis.upenn.edu/~ccb/publications/crowdsourcing-translation.pdf,crowdsourcing-translation,Figure,4,https://www.dropbox.com/request/734EpCoDU2C8A49ghnHT,crowdsourcing-translation-figure-4.pdf,Crowdsourcing Translation&colon; Professional Quality from Non-Professionals,http://cis.upenn.edu/~ccb/publications/crowdsourcing-translation.pdf,crowdsourcing-translation,Figure,5,https://www.dropbox.com/request/734EpCoDU2C8A49ghnHT,crowdsourcing-translation-figure-5.pdf,Crowdsourcing Translation&colon; Professional Quality from Non-Professionals,http://cis.upenn.edu/~ccb/publications/crowdsourcing-translation.pdf,crowdsourcing-translation,Table,1,https://www.dropbox.com/request/734EpCoDU2C8A49ghnHT,crowdsourcing-translation-table-1.pdf,Findings of the 2010 Joint Workshop on Statistical Machine Translation and Metrics for Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-wmt10-and-metrics-matr.pdf,findings-of-wmt10-and-metrics-matr,Figure,1,https://www.dropbox.com/request/dibLQtLbNWMhiTENIaYG,findings-of-wmt10-and-metrics-matr-figure-1.pdf,Findings of the 2010 Joint Workshop on Statistical Machine Translation and Metrics for Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-wmt10-and-metrics-matr.pdf,findings-of-wmt10-and-metrics-matr,Figure,2,https://www.dropbox.com/request/dibLQtLbNWMhiTENIaYG,findings-of-wmt10-and-metrics-matr-figure-2.pdf,"BLEU scores for the five right-most setups from Figure 3, constrained over the original translations.",The effect of varying the amount of calibration data (and using only the calibration feature). The 10% point (BLEU = 37.82) and the dashed line (BLEU = 39.06) correspond to the two right-most bars of Figure 3.,"Correlation (± std. dev.) for different selection methods, compared against the reference sets.",Statistics for the training and test sets used in the translation task. The number of words and the number of distinct words is based on the provided tokenizer.,This screenshot shows what an annotator sees when beginning to edit the output of a machine translation system.
5,Findings of the 2010 Joint Workshop on Statistical Machine Translation and Metrics for Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-wmt10-and-metrics-matr.pdf,findings-of-wmt10-and-metrics-matr,Figure,3,https://www.dropbox.com/request/dibLQtLbNWMhiTENIaYG,findings-of-wmt10-and-metrics-matr-figure-3.pdf,Findings of the 2010 Joint Workshop on Statistical Machine Translation and Metrics for Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-wmt10-and-metrics-matr.pdf,findings-of-wmt10-and-metrics-matr,Figure,4,https://www.dropbox.com/request/dibLQtLbNWMhiTENIaYG,findings-of-wmt10-and-metrics-matr-figure-4.pdf,Findings of the 2010 Joint Workshop on Statistical Machine Translation and Metrics for Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-wmt10-and-metrics-matr.pdf,findings-of-wmt10-and-metrics-matr,Table,1,https://www.dropbox.com/request/dibLQtLbNWMhiTENIaYG,findings-of-wmt10-and-metrics-matr-table-1.pdf,Findings of the 2010 Joint Workshop on Statistical Machine Translation and Metrics for Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-wmt10-and-metrics-matr.pdf,findings-of-wmt10-and-metrics-matr,Table,2,https://www.dropbox.com/request/dibLQtLbNWMhiTENIaYG,findings-of-wmt10-and-metrics-matr-table-2.pdf,Findings of the 2010 Joint Workshop on Statistical Machine Translation and Metrics for Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-wmt10-and-metrics-matr.pdf,findings-of-wmt10-and-metrics-matr,Table,3,https://www.dropbox.com/request/dibLQtLbNWMhiTENIaYG,findings-of-wmt10-and-metrics-matr-table-3.pdf,The percent of time that each system’s edited output was judged to be an acceptable translation. These numbers also include judgments of the system’s output when it was marked either incomprehen- sible or acceptable and left unedited. Note that the reference translation was edited alongside the system outputs. Error bars show one positive and one negative standard deviation for the systems in that lan- guage pair.,"The effect of removing an increasing number of MTurk workers. The order in which workers are removed is by Kexp(w), the kappa agreement coefficient with expert data (excluding references).",Participants in the shared translation task. Not all groups participated in all language pairs.,Participants in the system combination task.,"The number of items that were collected for each task during the manual evaluation. An item is defined to be a rank label in the ranking task, an edited sentence in the editing task, and a yes/no judgment in the judgment task."
5,Findings of the 2010 Joint Workshop on Statistical Machine Translation and Metrics for Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-wmt10-and-metrics-matr.pdf,findings-of-wmt10-and-metrics-matr,Table,4,https://www.dropbox.com/request/dibLQtLbNWMhiTENIaYG,findings-of-wmt10-and-metrics-matr-table-4.pdf,Findings of the 2010 Joint Workshop on Statistical Machine Translation and Metrics for Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-wmt10-and-metrics-matr.pdf,findings-of-wmt10-and-metrics-matr,Table,5,https://www.dropbox.com/request/dibLQtLbNWMhiTENIaYG,findings-of-wmt10-and-metrics-matr-table-5.pdf,Findings of the 2010 Joint Workshop on Statistical Machine Translation and Metrics for Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-wmt10-and-metrics-matr.pdf,findings-of-wmt10-and-metrics-matr,Table,6,https://www.dropbox.com/request/dibLQtLbNWMhiTENIaYG,findings-of-wmt10-and-metrics-matr-table-6.pdf,Findings of the 2010 Joint Workshop on Statistical Machine Translation and Metrics for Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-wmt10-and-metrics-matr.pdf,findings-of-wmt10-and-metrics-matr,Table,7,https://www.dropbox.com/request/dibLQtLbNWMhiTENIaYG,findings-of-wmt10-and-metrics-matr-table-7.pdf,Syntactic Constraints on Paraphrases Extracted from Parallel Corpora,http://cis.upenn.edu/~ccb/publications/syntactic-constraints-on-paraphrases.pdf,syntactic-constraints-on-paraphrases,Figure,2,https://www.dropbox.com/request/HBNFh42pf4ajDkiUQGcT,syntactic-constraints-on-paraphrases-figure-2.pdf,"Inter- and intra-annotator agreement for the sentence ranking task. In this task, P(E) is 0.333.","Official results for the WMT10 translation task, based on the human evaluation (ranking translations relative to each other)","Official results for the WMT10 system combination task, based on the human evaluation (ranking translations relative to each other)","System-level Spearman’s rho correlation of the automatic evaluation metrics with the human judgments for translation into English, ordered by average absolute value.","In addition to extracting phrases that are dominated by a node in the parse tree, we also generate labels for non-syntactic constituents. Three labels are possible for create equal."
5,Syntactic Constraints on Paraphrases Extracted from Parallel Corpora,http://cis.upenn.edu/~ccb/publications/syntactic-constraints-on-paraphrases.pdf,syntactic-constraints-on-paraphrases,Table,1,https://www.dropbox.com/request/HBNFh42pf4ajDkiUQGcT,syntactic-constraints-on-paraphrases-table-1.pdf,Syntactic Constraints on Paraphrases Extracted from Parallel Corpora,http://cis.upenn.edu/~ccb/publications/syntactic-constraints-on-paraphrases.pdf,syntactic-constraints-on-paraphrases,Table,2,https://www.dropbox.com/request/HBNFh42pf4ajDkiUQGcT,syntactic-constraints-on-paraphrases-table-2.pdf,Syntactic Constraints on Paraphrases Extracted from Parallel Corpora,http://cis.upenn.edu/~ccb/publications/syntactic-constraints-on-paraphrases.pdf,syntactic-constraints-on-paraphrases,Table,3,https://www.dropbox.com/request/HBNFh42pf4ajDkiUQGcT,syntactic-constraints-on-paraphrases-table-3.pdf,Syntactic Constraints on Paraphrases Extracted from Parallel Corpora,http://cis.upenn.edu/~ccb/publications/syntactic-constraints-on-paraphrases.pdf,syntactic-constraints-on-paraphrases,Table,4,https://www.dropbox.com/request/HBNFh42pf4ajDkiUQGcT,syntactic-constraints-on-paraphrases-table-4.pdf,Syntactic Constraints on Paraphrases Extracted from Parallel Corpora,http://cis.upenn.edu/~ccb/publications/syntactic-constraints-on-paraphrases.pdf,syntactic-constraints-on-paraphrases,Table,5,https://www.dropbox.com/request/HBNFh42pf4ajDkiUQGcT,syntactic-constraints-on-paraphrases-table-5.pdf,The baseline method’s paraphrases of equal and their probabilities (excluding items with p < .01).,"The baseline’s paraphrases of create equal. Most are clearly bad, and the most probable e2 ̸= e1 is a sub- string of e1 .",Syntactically constrained paraphrases for equal when it is labeled as an adjective or adjectival phrase.,Paraphrases and syntactic labels for the non- constituent phrase create equal.,Annotators rated paraphrases along two 5-point scales.
5,Syntactic Constraints on Paraphrases Extracted from Parallel Corpora,http://cis.upenn.edu/~ccb/publications/syntactic-constraints-on-paraphrases.pdf,syntactic-constraints-on-paraphrases,Table,6,https://www.dropbox.com/request/HBNFh42pf4ajDkiUQGcT,syntactic-constraints-on-paraphrases-table-6.pdf,Improved Statistical Machine Translation Using Monolingually-Derived Paraphrases,http://cis.upenn.edu/~ccb/publications/improved-translation-with-monolingually-derived-paraphrases.pdf,improved-translation-with-monolingually-derived-paraphrases,Table,1,https://www.dropbox.com/request/VUqS0TWvvhREajFNilW7,improved-translation-with-monolingually-derived-paraphrases-table-1.pdf,Improved Statistical Machine Translation Using Monolingually-Derived Paraphrases,http://cis.upenn.edu/~ccb/publications/improved-translation-with-monolingually-derived-paraphrases.pdf,improved-translation-with-monolingually-derived-paraphrases,Table,2,https://www.dropbox.com/request/VUqS0TWvvhREajFNilW7,improved-translation-with-monolingually-derived-paraphrases-table-2.pdf,Improved Statistical Machine Translation Using Monolingually-Derived Paraphrases,http://cis.upenn.edu/~ccb/publications/improved-translation-with-monolingually-derived-paraphrases.pdf,improved-translation-with-monolingually-derived-paraphrases,Table,3,https://www.dropbox.com/request/VUqS0TWvvhREajFNilW7,improved-translation-with-monolingually-derived-paraphrases-table-3.pdf,Improved Statistical Machine Translation Using Monolingually-Derived Paraphrases,http://cis.upenn.edu/~ccb/publications/improved-translation-with-monolingually-derived-paraphrases.pdf,improved-translation-with-monolingually-derived-paraphrases,Table,4,https://www.dropbox.com/request/VUqS0TWvvhREajFNilW7,improved-translation-with-monolingually-derived-paraphrases-table-4.pdf,"The results of the manual evaluation for each of the eight conditions. Correct meaning is the percent of time that a condition was assigned a 3, 4, or 5, and correct grammar is the percent of time that it was given a 4 or 5, using the scales from Table 5.",Training set sizes (million tokens).,"E2C Results: character-based BLEU and TER scores. All models have one additional feature over baseline, except for the ""1 + 2-6"" models that have one feature for unigrams and another feature for bigrams to 6-grams. Paraphrases with score < :3 were filtered out. *** = significance test over baseline with p < 0:0001, using Koehn’s (2004) pair-wise bootstrap resampling test for BLEU with 95% confidence interval.",English paraphrases from E2C 29Kbitext systems.,"S2E Results: Lowercase BLEU and TER. Paraphrases with score < minScore were filtered out. *** = significance test over baseline with p < 0:0001, using Koehn’s (2004) pair-wise bootstrap test for BLEU with 95% confidence interval."
5,Improved Statistical Machine Translation Using Monolingually-Derived Paraphrases,http://cis.upenn.edu/~ccb/publications/improved-translation-with-monolingually-derived-paraphrases.pdf,improved-translation-with-monolingually-derived-paraphrases,Table,5,https://www.dropbox.com/request/VUqS0TWvvhREajFNilW7,improved-translation-with-monolingually-derived-paraphrases-table-5.pdf,Improved Statistical Machine Translation Using Monolingually-Derived Paraphrases,http://cis.upenn.edu/~ccb/publications/improved-translation-with-monolingually-derived-paraphrases.pdf,improved-translation-with-monolingually-derived-paraphrases,Table,6,https://www.dropbox.com/request/VUqS0TWvvhREajFNilW7,improved-translation-with-monolingually-derived-paraphrases-table-6.pdf,Statistical Machine Translation with Word- and Sentence-Aligned Parallel Corpora,https://www.cis.upenn.edu/~ccb/publications/smt-with-word-and-sentence-aligned-parallel-corpora.pdf,smt-with-word-and-sentence-aligned-parallel-corpora,Figure,1,https://www.dropbox.com/request/oJucjsBb8Gc6pGKMNkID,smt-with-word-and-sentence-aligned-parallel-corpora-figure-1.pdf,Statistical Machine Translation with Word- and Sentence-Aligned Parallel Corpora,https://www.cis.upenn.edu/~ccb/publications/smt-with-word-and-sentence-aligned-parallel-corpora.pdf,smt-with-word-and-sentence-aligned-parallel-corpora,Figure,2,https://www.dropbox.com/request/oJucjsBb8Gc6pGKMNkID,smt-with-word-and-sentence-aligned-parallel-corpora-figure-2.pdf,Statistical Machine Translation with Word- and Sentence-Aligned Parallel Corpora,https://www.cis.upenn.edu/~ccb/publications/smt-with-word-and-sentence-aligned-parallel-corpora.pdf,smt-with-word-and-sentence-aligned-parallel-corpora,Figure,3,https://www.dropbox.com/request/oJucjsBb8Gc6pGKMNkID,smt-with-word-and-sentence-aligned-parallel-corpora-figure-3.pdf,"Comparison of Spanish paraphrases: by pivoting, and by two monolingual corpora. Ordered from best to worst score.",S2E translation examples on 10k-bitext systems. Some translation differences are in bold.,"Example alignments using sentence-aligned training data (a), using word-aligned data (b), and a reference manual alignment (c)",The effect on AER of varying λ for a train- ing corpus of 16K sentence pairs with various pro- portions of word-alignments,The effect on AER of varying the ratio of word-aligned to sentence-aligned data
5,Statistical Machine Translation with Word- and Sentence-Aligned Parallel Corpora,https://www.cis.upenn.edu/~ccb/publications/smt-with-word-and-sentence-aligned-parallel-corpora.pdf,smt-with-word-and-sentence-aligned-parallel-corpora,Figure,4,https://www.dropbox.com/request/oJucjsBb8Gc6pGKMNkID,smt-with-word-and-sentence-aligned-parallel-corpora-figure-4.pdf,Statistical Machine Translation with Word- and Sentence-Aligned Parallel Corpora,https://www.cis.upenn.edu/~ccb/publications/smt-with-word-and-sentence-aligned-parallel-corpora.pdf,smt-with-word-and-sentence-aligned-parallel-corpora,Table,1,https://www.dropbox.com/request/oJucjsBb8Gc6pGKMNkID,smt-with-word-and-sentence-aligned-parallel-corpora-table-1.pdf,Statistical Machine Translation with Word- and Sentence-Aligned Parallel Corpora,https://www.cis.upenn.edu/~ccb/publications/smt-with-word-and-sentence-aligned-parallel-corpora.pdf,smt-with-word-and-sentence-aligned-parallel-corpora,Table,2,https://www.dropbox.com/request/oJucjsBb8Gc6pGKMNkID,smt-with-word-and-sentence-aligned-parallel-corpora-table-2.pdf,Statistical Machine Translation with Word- and Sentence-Aligned Parallel Corpora,https://www.cis.upenn.edu/~ccb/publications/smt-with-word-and-sentence-aligned-parallel-corpora.pdf,smt-with-word-and-sentence-aligned-parallel-corpora,Table,3,https://www.dropbox.com/request/oJucjsBb8Gc6pGKMNkID,smt-with-word-and-sentence-aligned-parallel-corpora-table-3.pdf,Statistical Machine Translation with Word- and Sentence-Aligned Parallel Corpora,https://www.cis.upenn.edu/~ccb/publications/smt-with-word-and-sentence-aligned-parallel-corpora.pdf,smt-with-word-and-sentence-aligned-parallel-corpora,Table,4,https://www.dropbox.com/request/oJucjsBb8Gc6pGKMNkID,smt-with-word-and-sentence-aligned-parallel-corpora-table-4.pdf,The effect on Bleu of varying the ratio of word-aligned to sentence-aligned data,Alignment error rates for the various IBM Models trained with sentence-aligned data,Alignment error rates for the various IBM Models trained with word-aligned data,The improved alignment error rates when using a dictionary instead of word-aligned data to constrain word translations,Improved AER leads to improved translation quality
5,Statistical Machine Translation with Word- and Sentence-Aligned Parallel Corpora,https://www.cis.upenn.edu/~ccb/publications/smt-with-word-and-sentence-aligned-parallel-corpora.pdf,smt-with-word-and-sentence-aligned-parallel-corpora,Table,5,https://www.dropbox.com/request/oJucjsBb8Gc6pGKMNkID,smt-with-word-and-sentence-aligned-parallel-corpora-table-5.pdf,Statistical Machine Translation with Word- and Sentence-Aligned Parallel Corpora,https://www.cis.upenn.edu/~ccb/publications/smt-with-word-and-sentence-aligned-parallel-corpora.pdf,smt-with-word-and-sentence-aligned-parallel-corpora,Table,6,https://www.dropbox.com/request/oJucjsBb8Gc6pGKMNkID,smt-with-word-and-sentence-aligned-parallel-corpora-table-6.pdf,Cheap Fast and Good Enough&colon; Automatic Speech Recognition with Non-Expert Transcription,http://cis.upenn.edu/~ccb/publications/automatic-speech-recognition-with-non-expert-transcription.pdf,automatic-speech-recognition-with-non-expert-transcription,Figure,1,https://www.dropbox.com/request/vWv26qPbORYlXfHBCo53,automatic-speech-recognition-with-non-expert-transcription-figure-1.pdf,Cheap Fast and Good Enough&colon; Automatic Speech Recognition with Non-Expert Transcription,http://cis.upenn.edu/~ccb/publications/automatic-speech-recognition-with-non-expert-transcription.pdf,automatic-speech-recognition-with-non-expert-transcription,Figure,2,https://www.dropbox.com/request/vWv26qPbORYlXfHBCo53,automatic-speech-recognition-with-non-expert-transcription-figure-2.pdf,Cheap Fast and Good Enough&colon; Automatic Speech Recognition with Non-Expert Transcription,http://cis.upenn.edu/~ccb/publications/automatic-speech-recognition-with-non-expert-transcription.pdf,automatic-speech-recognition-with-non-expert-transcription,Table,1,https://www.dropbox.com/request/vWv26qPbORYlXfHBCo53,automatic-speech-recognition-with-non-expert-transcription-table-1.pdf,The effect of weighting word-aligned data more heavily that its proportion in the training data (corpus size 16000 sentence pairs),Summary results for AER and translation quality experiments on Hansards data,Histogram of per-turker transcription rate for twenty hours of English CTS data. Historical estimates for high quality transcription are 50xRT. The 2004 Fisher transcription effort achieved 6xRT and the average here is 11xRT.,"WER with a varied amount of LM training data and a fixed 16hr acoustic model. MTurk transcription degrades WER by 0.8% absolute across LM size. When interpolated with 1M words of broadcast news, this degradation shrinks to 0.6%.","Quality of Non-Professional Transcription on 20 hours of English Switchboard. Even though disagreement for random selection without quality control has 23% disagreement with professional transcription, an ASR system trained on the data is only 2.5% worse than using LDC transcriptions. The upper bound for quality control (row 3) recovers only 50% of the total loss."
5,(Meta-) Evaluation of Machine Translation,http://cis.upenn.edu/~ccb/publications/meta-evaluation-of-machine-tranlsion.pdf,meta-evaluation-of-machine-tranlsion,Figure,1,https://www.dropbox.com/request/M8iFUCZdnGumqf75XpK3,meta-evaluation-of-machine-tranlsion-figure-1.pdf,(Meta-) Evaluation of Machine Translation,http://cis.upenn.edu/~ccb/publications/meta-evaluation-of-machine-tranlsion.pdf,meta-evaluation-of-machine-tranlsion,Figure,2,https://www.dropbox.com/request/M8iFUCZdnGumqf75XpK3,meta-evaluation-of-machine-tranlsion-figure-2.pdf,(Meta-) Evaluation of Machine Translation,http://cis.upenn.edu/~ccb/publications/meta-evaluation-of-machine-tranlsion.pdf,meta-evaluation-of-machine-tranlsion,Figure,3,https://www.dropbox.com/request/M8iFUCZdnGumqf75XpK3,meta-evaluation-of-machine-tranlsion-figure-3.pdf,(Meta-) Evaluation of Machine Translation,http://cis.upenn.edu/~ccb/publications/meta-evaluation-of-machine-tranlsion.pdf,meta-evaluation-of-machine-tranlsion,Figure,4,https://www.dropbox.com/request/M8iFUCZdnGumqf75XpK3,meta-evaluation-of-machine-tranlsion-figure-4.pdf,(Meta-) Evaluation of Machine Translation,http://cis.upenn.edu/~ccb/publications/meta-evaluation-of-machine-tranlsion.pdf,meta-evaluation-of-machine-tranlsion,Table,1,https://www.dropbox.com/request/M8iFUCZdnGumqf75XpK3,meta-evaluation-of-machine-tranlsion-table-1.pdf,"Properties of the training and test sets used in the shared task. The training data is drawn from the Europarl corpus and from the Project Syndicate, a web site which collects political commentary in multiple languages.","In constituent-based evaluation, the source sentence was parsed, and automatically aligned with the reference translation and systems’ translations","For each of the types of evaluation, judges were shown screens containing up to five different system translations, along with the source sentence and reference translation.",Distributions of the amount of time it took to judge single sentences for the three types of manual evaluation,Participants in the shared task. Not all groups participated in all translation directions.
5,(Meta-) Evaluation of Machine Translation,http://cis.upenn.edu/~ccb/publications/meta-evaluation-of-machine-tranlsion.pdf,meta-evaluation-of-machine-tranlsion,Table,2,https://www.dropbox.com/request/M8iFUCZdnGumqf75XpK3,meta-evaluation-of-machine-tranlsion-table-2.pdf,(Meta-) Evaluation of Machine Translation,http://cis.upenn.edu/~ccb/publications/meta-evaluation-of-machine-tranlsion.pdf,meta-evaluation-of-machine-tranlsion,Table,3,https://www.dropbox.com/request/M8iFUCZdnGumqf75XpK3,meta-evaluation-of-machine-tranlsion-table-3.pdf,(Meta-) Evaluation of Machine Translation,http://cis.upenn.edu/~ccb/publications/meta-evaluation-of-machine-tranlsion.pdf,meta-evaluation-of-machine-tranlsion,Table,4,https://www.dropbox.com/request/M8iFUCZdnGumqf75XpK3,meta-evaluation-of-machine-tranlsion-table-4.pdf,(Meta-) Evaluation of Machine Translation,http://cis.upenn.edu/~ccb/publications/meta-evaluation-of-machine-tranlsion.pdf,meta-evaluation-of-machine-tranlsion,Table,5,https://www.dropbox.com/request/M8iFUCZdnGumqf75XpK3,meta-evaluation-of-machine-tranlsion-table-5.pdf,(Meta-) Evaluation of Machine Translation,http://cis.upenn.edu/~ccb/publications/meta-evaluation-of-machine-tranlsion.pdf,meta-evaluation-of-machine-tranlsion,Table,6,https://www.dropbox.com/request/M8iFUCZdnGumqf75XpK3,meta-evaluation-of-machine-tranlsion-table-6.pdf,The number of items that were judged for each task during the manual evaluation,The proportion of time that participants’ entries were top-ranked in the human evaluation,The proportion of time that participants’ entries were top-ranked by the automatic evaluation metrics,"Kappa coefficient values representing the
inter-annotator agreement for the different types of manual evaluation",Kappa coefficient values for intra-annotator agreement for the different types of manual evaluation
5,(Meta-) Evaluation of Machine Translation,http://cis.upenn.edu/~ccb/publications/meta-evaluation-of-machine-tranlsion.pdf,meta-evaluation-of-machine-tranlsion,Table,7,https://www.dropbox.com/request/M8iFUCZdnGumqf75XpK3,meta-evaluation-of-machine-tranlsion-table-7.pdf,(Meta-) Evaluation of Machine Translation,http://cis.upenn.edu/~ccb/publications/meta-evaluation-of-machine-tranlsion.pdf,meta-evaluation-of-machine-tranlsion,Table,8,https://www.dropbox.com/request/M8iFUCZdnGumqf75XpK3,meta-evaluation-of-machine-tranlsion-table-8.pdf,(Meta-) Evaluation of Machine Translation,http://cis.upenn.edu/~ccb/publications/meta-evaluation-of-machine-tranlsion.pdf,meta-evaluation-of-machine-tranlsion,Table,9,https://www.dropbox.com/request/M8iFUCZdnGumqf75XpK3,meta-evaluation-of-machine-tranlsion-table-9.pdf,(Meta-) Evaluation of Machine Translation,http://cis.upenn.edu/~ccb/publications/meta-evaluation-of-machine-tranlsion.pdf,meta-evaluation-of-machine-tranlsion,Table,10,https://www.dropbox.com/request/M8iFUCZdnGumqf75XpK3,meta-evaluation-of-machine-tranlsion-table-10.pdf,(Meta-) Evaluation of Machine Translation,http://cis.upenn.edu/~ccb/publications/meta-evaluation-of-machine-tranlsion.pdf,meta-evaluation-of-machine-tranlsion,Table,11,https://www.dropbox.com/request/M8iFUCZdnGumqf75XpK3,meta-evaluation-of-machine-tranlsion-table-11.pdf,Average corrections for the different automatic metrics when they are used to evaluate translations into English,Average corrections for the different automatic metrics when they are used to evaluate translations into the other languages,Human evaluation for German-English submissions,Human evaluation for Spanish-English submissions,Human evaluation for French-English submissions
5,Scaling Phrase-Based Statistical Machine Translation to Larger Corpora and Longer Phrases,http://cis.upenn.edu/~ccb/publications/scaling-phrase-based-statistical-machine-translation.pdf,scaling-phrase-based-statistical-machine-translation,Figure,1,https://www.dropbox.com/request/8wHmeZq8ye8Ulzy7Y8VV,scaling-phrase-based-statistical-machine-translation-figure-1.pdf,Scaling Phrase-Based Statistical Machine Translation to Larger Corpora and Longer Phrases,http://cis.upenn.edu/~ccb/publications/scaling-phrase-based-statistical-machine-translation.pdf,scaling-phrase-based-statistical-machine-translation,Figure,2,https://www.dropbox.com/request/8wHmeZq8ye8Ulzy7Y8VV,scaling-phrase-based-statistical-machine-translation-figure-2.pdf,Scaling Phrase-Based Statistical Machine Translation to Larger Corpora and Longer Phrases,http://cis.upenn.edu/~ccb/publications/scaling-phrase-based-statistical-machine-translation.pdf,scaling-phrase-based-statistical-machine-translation,Table,1,https://www.dropbox.com/request/8wHmeZq8ye8Ulzy7Y8VV,scaling-phrase-based-statistical-machine-translation-table-1.pdf,Scaling Phrase-Based Statistical Machine Translation to Larger Corpora and Longer Phrases,http://cis.upenn.edu/~ccb/publications/scaling-phrase-based-statistical-machine-translation.pdf,scaling-phrase-based-statistical-machine-translation,Table,2,https://www.dropbox.com/request/8wHmeZq8ye8Ulzy7Y8VV,scaling-phrase-based-statistical-machine-translation-table-2.pdf,Scaling Phrase-Based Statistical Machine Translation to Larger Corpora and Longer Phrases,http://cis.upenn.edu/~ccb/publications/scaling-phrase-based-statistical-machine-translation.pdf,scaling-phrase-based-statistical-machine-translation,Table,3,https://www.dropbox.com/request/8wHmeZq8ye8Ulzy7Y8VV,scaling-phrase-based-statistical-machine-translation-table-3.pdf,"An initialized, unsorted suffix array for a very small corpus",A sorted suffix array and its corresponding suffixes,Statistics about Arabic phrases in the NIST-2004 large data track.,Estimated size of lookup tables for the NIST-2004 Arabic-English data,Lengths of phrases from the training data that occur in the NIST-2004 test set
5,Scaling Phrase-Based Statistical Machine Translation to Larger Corpora and Longer Phrases,http://cis.upenn.edu/~ccb/publications/scaling-phrase-based-statistical-machine-translation.pdf,scaling-phrase-based-statistical-machine-translation,Table,4,https://www.dropbox.com/request/8wHmeZq8ye8Ulzy7Y8VV,scaling-phrase-based-statistical-machine-translation-table-4.pdf,Scaling Phrase-Based Statistical Machine Translation to Larger Corpora and Longer Phrases,http://cis.upenn.edu/~ccb/publications/scaling-phrase-based-statistical-machine-translation.pdf,scaling-phrase-based-statistical-machine-translation,Table,5,https://www.dropbox.com/request/8wHmeZq8ye8Ulzy7Y8VV,scaling-phrase-based-statistical-machine-translation-table-5.pdf,Scaling Phrase-Based Statistical Machine Translation to Larger Corpora and Longer Phrases,http://cis.upenn.edu/~ccb/publications/scaling-phrase-based-statistical-machine-translation.pdf,scaling-phrase-based-statistical-machine-translation,Table,6,https://www.dropbox.com/request/8wHmeZq8ye8Ulzy7Y8VV,scaling-phrase-based-statistical-machine-translation-table-6.pdf,PPDB&colon; The Paraphrase Database,http://cis.upenn.edu/~ccb/publications/ppdb.pdf,ppdb,Figure,1,https://www.dropbox.com/request/t6Q1EvUQRlr1R0WhcQk5,ppdb-figure-1.pdf,Constructing Corpora for the Development and Evaluation of Paraphrase Systems,http://cis.upenn.edu/~ccb/publications/constructing-corpora-for-paraphrase-systems.pdf,constructing-corpora-for-paraphrase-systems,Figure,2,https://www.dropbox.com/request/3TQzC3pEb7vIN72K3N4H,constructing-corpora-for-paraphrase-systems-figure-2.pdf,Coverage using only repeated phrases of the specified length,Examples of O and calculation times for phrases of different frequencies,A comparison of retrieval times and translation quality when the number of translations is capped at various sample sizes,Phrasal paraphrases are extracted via bilingual pivoting.,Sample sentence pair showing the word alignments from two annotators.
5,Constructing Corpora for the Development and Evaluation of Paraphrase Systems,http://cis.upenn.edu/~ccb/publications/constructing-corpora-for-paraphrase-systems.pdf,constructing-corpora-for-paraphrase-systems,Figure,3,https://www.dropbox.com/request/3TQzC3pEb7vIN72K3N4H,constructing-corpora-for-paraphrase-systems-figure-3.pdf,Constructing Corpora for the Development and Evaluation of Paraphrase Systems,http://cis.upenn.edu/~ccb/publications/constructing-corpora-for-paraphrase-systems.pdf,constructing-corpora-for-paraphrase-systems,Figure,4,https://www.dropbox.com/request/3TQzC3pEb7vIN72K3N4H,constructing-corpora-for-paraphrase-systems-figure-4.pdf,Constructing Corpora for the Development and Evaluation of Paraphrase Systems,http://cis.upenn.edu/~ccb/publications/constructing-corpora-for-paraphrase-systems.pdf,constructing-corpora-for-paraphrase-systems,Figure,5,https://www.dropbox.com/request/3TQzC3pEb7vIN72K3N4H,constructing-corpora-for-paraphrase-systems-figure-5.pdf,Constructing Corpora for the Development and Evaluation of Paraphrase Systems,http://cis.upenn.edu/~ccb/publications/constructing-corpora-for-paraphrase-systems.pdf,constructing-corpora-for-paraphrase-systems,Table,1,https://www.dropbox.com/request/3TQzC3pEb7vIN72K3N4H,constructing-corpora-for-paraphrase-systems-table-1.pdf,Paraphrasing and Translation,http://cis.upenn.edu/~ccb/publications/callison-burch-thesis.pdf,callison-burch-thesis,Figure,2,https://www.dropbox.com/request/oTN6kiN90fKyXZyNnqRF,callison-burch-thesis-figure-2.pdf,"Validity of phrase pairs according to the phrase extraction heuristic. Only the leftmost phrase pair is valid. The others are inconsistent with the alignment or have an unaligned word on a boundary, respectively, indicated by a cross.","Agreement statistics plotted against sentence length for the three sub-corpora. Each group of
three columns correspond to πˆ, πˆ0 and Cˆ, respectively. The statistics were measured over non-identical phrase pairs using all phrase pairs: atomic and composite.",Synchronous grammar rules extracted from the MTC sub-corpus.,"Single word pairs specified by the word alignments from Figure 2, for two annotators A and B. The column entries specify the alignment type for each annotator, either sure (S) or possible (P). Dashes indicate that the word-pair was not predicted by the annotator. Italics denote lexically identical word pairs.",Barzilay and McKeown (2001) extracted paraphrases from multiple transla- tions using identical surrounding substrings
5,Paraphrasing and Translation,http://cis.upenn.edu/~ccb/publications/callison-burch-thesis.pdf,callison-burch-thesis,Figure,3,https://www.dropbox.com/request/oTN6kiN90fKyXZyNnqRF,callison-burch-thesis-figure-3.pdf,Paraphrasing and Translation,http://cis.upenn.edu/~ccb/publications/callison-burch-thesis.pdf,callison-burch-thesis,Figure,4,https://www.dropbox.com/request/oTN6kiN90fKyXZyNnqRF,callison-burch-thesis-figure-4.pdf,Paraphrasing and Translation,http://cis.upenn.edu/~ccb/publications/callison-burch-thesis.pdf,callison-burch-thesis,Figure,5,https://www.dropbox.com/request/oTN6kiN90fKyXZyNnqRF,callison-burch-thesis-figure-5.pdf,Paraphrasing and Translation,http://cis.upenn.edu/~ccb/publications/callison-burch-thesis.pdf,callison-burch-thesis,Figure,6,https://www.dropbox.com/request/oTN6kiN90fKyXZyNnqRF,callison-burch-thesis-figure-6.pdf,Paraphrasing and Translation,http://cis.upenn.edu/~ccb/publications/callison-burch-thesis.pdf,callison-burch-thesis,Figure,7,https://www.dropbox.com/request/oTN6kiN90fKyXZyNnqRF,callison-burch-thesis-figure-7.pdf,"A phrase can be aligned to many foreign phrases, which in turn can be aligned to multiple possible paraphrases",In machine translation evaluation the following scales are used by judges to assign adequacy and fluency scores to each translation,"Percent of unique unigrams, bigrams, trigrams, and 4-grams from the Europarl Spanish test sentences for which translations were learned in increasingly large training corpora",Scatterplot of the length of each translation against its number of possible permutations due to bigram mismatches for an entry in the 2005 NIST MT Eval,The decoder for the baseline system has translation options only for those words which have phrases that occur in the phrase table. In this case there are no translations for the source word votare ́.
5,Paraphrasing and Translation,http://cis.upenn.edu/~ccb/publications/callison-burch-thesis.pdf,callison-burch-thesis,Figure,8,https://www.dropbox.com/request/oTN6kiN90fKyXZyNnqRF,callison-burch-thesis-figure-8.pdf,Paraphrasing and Translation,http://cis.upenn.edu/~ccb/publications/callison-burch-thesis.pdf,callison-burch-thesis,Figure,9,https://www.dropbox.com/request/oTN6kiN90fKyXZyNnqRF,callison-burch-thesis-figure-9.pdf,Paraphrasing and Translation,http://cis.upenn.edu/~ccb/publications/callison-burch-thesis.pdf,callison-burch-thesis,Figure,10,https://www.dropbox.com/request/oTN6kiN90fKyXZyNnqRF,callison-burch-thesis-figure-10.pdf,Paraphrasing and Translation,http://cis.upenn.edu/~ccb/publications/callison-burch-thesis.pdf,callison-burch-thesis,Figure,11,https://www.dropbox.com/request/oTN6kiN90fKyXZyNnqRF,callison-burch-thesis-figure-11.pdf,Paraphrasing and Translation,http://cis.upenn.edu/~ccb/publications/callison-burch-thesis.pdf,callison-burch-thesis,Table,1,https://www.dropbox.com/request/oTN6kiN90fKyXZyNnqRF,callison-burch-thesis-table-1.pdf,{},{},{},{},{}
5,Paraphrasing and Translation,http://cis.upenn.edu/~ccb/publications/callison-burch-thesis.pdf,callison-burch-thesis,Table,2,https://www.dropbox.com/request/oTN6kiN90fKyXZyNnqRF,callison-burch-thesis-table-2.pdf,Paraphrasing and Translation,http://cis.upenn.edu/~ccb/publications/callison-burch-thesis.pdf,callison-burch-thesis,Table,3,https://www.dropbox.com/request/oTN6kiN90fKyXZyNnqRF,callison-burch-thesis-table-3.pdf,Paraphrasing and Translation,http://cis.upenn.edu/~ccb/publications/callison-burch-thesis.pdf,callison-burch-thesis,Table,4,https://www.dropbox.com/request/oTN6kiN90fKyXZyNnqRF,callison-burch-thesis-table-4.pdf,Paraphrasing and Translation,http://cis.upenn.edu/~ccb/publications/callison-burch-thesis.pdf,callison-burch-thesis,Table,5,https://www.dropbox.com/request/oTN6kiN90fKyXZyNnqRF,callison-burch-thesis-table-5.pdf,Paraphrasing and Translation,http://cis.upenn.edu/~ccb/publications/callison-burch-thesis.pdf,callison-burch-thesis,Table,6,https://www.dropbox.com/request/oTN6kiN90fKyXZyNnqRF,callison-burch-thesis-table-6.pdf,{},{},{},{},{}
5,Paraphrasing and Translation,http://cis.upenn.edu/~ccb/publications/callison-burch-thesis.pdf,callison-burch-thesis,Table,7,https://www.dropbox.com/request/oTN6kiN90fKyXZyNnqRF,callison-burch-thesis-table-7.pdf,Paraphrasing and Translation,http://cis.upenn.edu/~ccb/publications/callison-burch-thesis.pdf,callison-burch-thesis,Table,8,https://www.dropbox.com/request/oTN6kiN90fKyXZyNnqRF,callison-burch-thesis-table-8.pdf,Paraphrasing and Translation,http://cis.upenn.edu/~ccb/publications/callison-burch-thesis.pdf,callison-burch-thesis,Table,9,https://www.dropbox.com/request/oTN6kiN90fKyXZyNnqRF,callison-burch-thesis-table-9.pdf,Paraphrasing and Translation,http://cis.upenn.edu/~ccb/publications/callison-burch-thesis.pdf,callison-burch-thesis,Table,10,https://www.dropbox.com/request/oTN6kiN90fKyXZyNnqRF,callison-burch-thesis-table-10.pdf,Paraphrasing and Translation,http://cis.upenn.edu/~ccb/publications/callison-burch-thesis.pdf,callison-burch-thesis,Table,11,https://www.dropbox.com/request/oTN6kiN90fKyXZyNnqRF,callison-burch-thesis-table-11.pdf,{},{},{},{},{}
5,Constraining the Phrase-Based Joint Probability Statistical Translation Model,http://cis.upenn.edu/~ccb/publications/constraining-the-phrase-based-joint-probability-model.pdf,constraining-the-phrase-based-joint-probability-model,Table,1,https://www.dropbox.com/request/zZbwJog157fjayCIGPMq,constraining-the-phrase-based-joint-probability-model-table-1.pdf,Constraining the Phrase-Based Joint Probability Statistical Translation Model,http://cis.upenn.edu/~ccb/publications/constraining-the-phrase-based-joint-probability-model.pdf,constraining-the-phrase-based-joint-probability-model,Table,2,https://www.dropbox.com/request/zZbwJog157fjayCIGPMq,constraining-the-phrase-based-joint-probability-model-table-2.pdf,Constraining the Phrase-Based Joint Probability Statistical Translation Model,http://cis.upenn.edu/~ccb/publications/constraining-the-phrase-based-joint-probability-model.pdf,constraining-the-phrase-based-joint-probability-model,Table,3,https://www.dropbox.com/request/zZbwJog157fjayCIGPMq,constraining-the-phrase-based-joint-probability-model-table-3.pdf,Machine Translation of Arabic Dialects,http://cis.upenn.edu/~ccb/publications/machine-translation-of-arabic-dialects.pdf,machine-translation-of-arabic-dialects,Figure,1,https://www.dropbox.com/request/lMqNJubIIUmIZci0ftRz,machine-translation-of-arabic-dialects-figure-1.pdf,Machine Translation of Arabic Dialects,http://cis.upenn.edu/~ccb/publications/machine-translation-of-arabic-dialects.pdf,machine-translation-of-arabic-dialects,Figure,2,https://www.dropbox.com/request/lMqNJubIIUmIZci0ftRz,machine-translation-of-arabic-dialects-figure-2.pdf,The number of possible phrasal alignments for sentence pairs calculated using Stirling numbers of the second kind.,"Bleu scores for the Joint Model with IBM constraints and prior counts, corpus size indicates number of sentence pairs",Translation table size in millions of phrase pairs,"One possible breakdown of spoken Arabic into dialect groups: Maghrebi, Egyptian, Levantine, Gulf and Iraqi. Habash (2010) gives a breakdown along mostly the same lines. We used this map as an illustration for annotators in our dialect classification task (Section 3.1), with Arabic names for the dialects instead of English",Examples of improvement in MT output when training on our Dialectal Arabic-English parallel corpus instead of an MSA-English parallel corpus.
5,Machine Translation of Arabic Dialects,http://cis.upenn.edu/~ccb/publications/machine-translation-of-arabic-dialects.pdf,machine-translation-of-arabic-dialects,Figure,3,https://www.dropbox.com/request/lMqNJubIIUmIZci0ftRz,machine-translation-of-arabic-dialects-figure-3.pdf,Machine Translation of Arabic Dialects,http://cis.upenn.edu/~ccb/publications/machine-translation-of-arabic-dialects.pdf,machine-translation-of-arabic-dialects,Figure,4,https://www.dropbox.com/request/lMqNJubIIUmIZci0ftRz,machine-translation-of-arabic-dialects-figure-4.pdf,Machine Translation of Arabic Dialects,http://cis.upenn.edu/~ccb/publications/machine-translation-of-arabic-dialects.pdf,machine-translation-of-arabic-dialects,Table,1,https://www.dropbox.com/request/lMqNJubIIUmIZci0ftRz,machine-translation-of-arabic-dialects-table-1.pdf,Machine Translation of Arabic Dialects,http://cis.upenn.edu/~ccb/publications/machine-translation-of-arabic-dialects.pdf,machine-translation-of-arabic-dialects,Table,2,https://www.dropbox.com/request/lMqNJubIIUmIZci0ftRz,machine-translation-of-arabic-dialects-table-2.pdf,Machine Translation of Arabic Dialects,http://cis.upenn.edu/~ccb/publications/machine-translation-of-arabic-dialects.pdf,machine-translation-of-arabic-dialects,Table,3,https://www.dropbox.com/request/lMqNJubIIUmIZci0ftRz,machine-translation-of-arabic-dialects-table-3.pdf,"Examples of ambiguous words that are translated incorrectly by the MSA-English system, but correctly by the Dialectal Arabic-English system.","Learning curves showing the effects of increasing the size of dialectal training data, when combined with the 150M-word MSA parallel corpus, and when used alone. Adding the MSA training data is only useful when the dialectal data is scarce (200k words).",The total costs for the three MTurk subtasks involved with the creation of our Dialectal Arabic-English parallel corpus.,Statistics about the training/tuning/test datasets used in our experiments. The token counts are calculated before MADA segmentation.,"Comparison of the effect of morphological segmentation when translating MSA web text and Dialectal Arabic web text. The morphological segmentation uniformly improves translation quality, but the improvements are more dramatic for MSA than for Dialectal Arabic when comparing similarly-sized training corpora."
5,Machine Translation of Arabic Dialects,http://cis.upenn.edu/~ccb/publications/machine-translation-of-arabic-dialects.pdf,machine-translation-of-arabic-dialects,Table,4,https://www.dropbox.com/request/lMqNJubIIUmIZci0ftRz,machine-translation-of-arabic-dialects-table-4.pdf,Machine Translation of Arabic Dialects,http://cis.upenn.edu/~ccb/publications/machine-translation-of-arabic-dialects.pdf,machine-translation-of-arabic-dialects,Table,5,https://www.dropbox.com/request/lMqNJubIIUmIZci0ftRz,machine-translation-of-arabic-dialects-table-5.pdf,Machine Translation of Arabic Dialects,http://cis.upenn.edu/~ccb/publications/machine-translation-of-arabic-dialects.pdf,machine-translation-of-arabic-dialects,Table,6,https://www.dropbox.com/request/lMqNJubIIUmIZci0ftRz,machine-translation-of-arabic-dialects-table-6.pdf,Machine Translation of Arabic Dialects,http://cis.upenn.edu/~ccb/publications/machine-translation-of-arabic-dialects.pdf,machine-translation-of-arabic-dialects,Table,7,https://www.dropbox.com/request/lMqNJubIIUmIZci0ftRz,machine-translation-of-arabic-dialects-table-7.pdf,Improving Translation Lexicon Induction from Monolingual Corpora via Dependency Contexts and Part-of-Speech Equivalences,http://cis.upenn.edu/~ccb/publications/improving-translation-lexicon-induction.pdf,improving-translation-lexicon-induction,Figure,1,https://www.dropbox.com/request/f8fMLfzazCWDk0pNAmdx,improving-translation-lexicon-induction-figure-1.pdf,"A comparison of translation quality of Egyptian, Levantine, and MSA web text, using various training corpora. The highest BLEU scores are achieved using the full set of dialectal data (which combines Levantine and Egyptian), since the Egyptian alone is sparse. For Levantine, adding Egyptian has no effect. In both cases, adding MSA to the dialectal data results in marginally worse translations.",The most frequent OOV’s (with counts ≥ 10) of our test sets against the MSA training data.,"Results on a truly independent test set, consisting of data harvested from Egyptian Facebook pages that are entirely distinct from the our dialectal training set. The improvements over the MSA baseline are still considerable: +2.9 BLEU points when no Facebook data is available for tuning and +2.7 with a Facebook tuning set.","A comparison of the effectiveness of performing Levantine-to-MSA mapping before translating into English, versus translating directly from Levantine into English. The mapping from Levantine to MSA was done manually, so it is an optimistic estimate of what might be done automatically. Although initially helpful to the MSA baseline system, the usefulness of pivoting through MSA drops as more dialectal data is added, eventually hurting performance.","Illustration of (Rapp, 1999) model for translating spanish word “crecimiento (growth)” via dependency context vectors extracted from respective monolingual corpora as explained in Section 3.1.2"
5,Improving Translation Lexicon Induction from Monolingual Corpora via Dependency Contexts and Part-of-Speech Equivalences,http://cis.upenn.edu/~ccb/publications/improving-translation-lexicon-induction.pdf,improving-translation-lexicon-induction,Figure,2,https://www.dropbox.com/request/f8fMLfzazCWDk0pNAmdx,improving-translation-lexicon-induction-figure-2.pdf,Improving Translation Lexicon Induction from Monolingual Corpora via Dependency Contexts and Part-of-Speech Equivalences,http://cis.upenn.edu/~ccb/publications/improving-translation-lexicon-induction.pdf,improving-translation-lexicon-induction,Figure,3,https://www.dropbox.com/request/f8fMLfzazCWDk0pNAmdx,improving-translation-lexicon-induction-figure-3.pdf,Improving Translation Lexicon Induction from Monolingual Corpora via Dependency Contexts and Part-of-Speech Equivalences,http://cis.upenn.edu/~ccb/publications/improving-translation-lexicon-induction.pdf,improving-translation-lexicon-induction,Figure,4,https://www.dropbox.com/request/f8fMLfzazCWDk0pNAmdx,improving-translation-lexicon-induction-figure-4.pdf,Improving Translation Lexicon Induction from Monolingual Corpora via Dependency Contexts and Part-of-Speech Equivalences,http://cis.upenn.edu/~ccb/publications/improving-translation-lexicon-induction.pdf,improving-translation-lexicon-induction,Figure,5,https://www.dropbox.com/request/f8fMLfzazCWDk0pNAmdx,improving-translation-lexicon-induction-figure-5.pdf,Improving Translation Lexicon Induction from Monolingual Corpora via Dependency Contexts and Part-of-Speech Equivalences,http://cis.upenn.edu/~ccb/publications/improving-translation-lexicon-induction.pdf,improving-translation-lexicon-induction,Figure,6,https://www.dropbox.com/request/f8fMLfzazCWDk0pNAmdx,improving-translation-lexicon-induction-figure-6.pdf,Illustration of using dependency trees to model richer contexts for projection,Precision/Recall curve showing superior performance of dependency context model as compared to adjacent context at different recall points. Precision is the fraction of tested Spanish words with Top 1 translation correct and Recall is fraction of the 1000 Spanish words tested upon.,Illustration of using part-of-speech tag mapping to restrict candidate space of translations,"Illustration of mapping Spanish part-of-speech tagset to English tagset. The tagsets vary greatly in notation and the morphological/syntactic constituents represented and need to be mapped first, using the algorithm described in Section 6.1.",Precision/Recall curve showing superior performance of using part-of-speech equivalences for translating all word-types. Precision is the fraction of tested Spanish words with Top 1 translation correct and Recall is fraction of the 1000 Spanish words tested upon.
5,Improving Translation Lexicon Induction from Monolingual Corpora via Dependency Contexts and Part-of-Speech Equivalences,http://cis.upenn.edu/~ccb/publications/improving-translation-lexicon-induction.pdf,improving-translation-lexicon-induction,Table,1,https://www.dropbox.com/request/f8fMLfzazCWDk0pNAmdx,improving-translation-lexicon-induction-table-1.pdf,Improving Translation Lexicon Induction from Monolingual Corpora via Dependency Contexts and Part-of-Speech Equivalences,http://cis.upenn.edu/~ccb/publications/improving-translation-lexicon-induction.pdf,improving-translation-lexicon-induction,Table,2,https://www.dropbox.com/request/f8fMLfzazCWDk0pNAmdx,improving-translation-lexicon-induction-table-2.pdf,Improving Translation Lexicon Induction from Monolingual Corpora via Dependency Contexts and Part-of-Speech Equivalences,http://cis.upenn.edu/~ccb/publications/improving-translation-lexicon-induction.pdf,improving-translation-lexicon-induction,Table,3,https://www.dropbox.com/request/f8fMLfzazCWDk0pNAmdx,improving-translation-lexicon-induction-table-3.pdf,Improving Translation Lexicon Induction from Monolingual Corpora via Dependency Contexts and Part-of-Speech Equivalences,http://cis.upenn.edu/~ccb/publications/improving-translation-lexicon-induction.pdf,improving-translation-lexicon-induction,Table,4,https://www.dropbox.com/request/f8fMLfzazCWDk0pNAmdx,improving-translation-lexicon-induction-table-4.pdf,Co-Training for Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/co-training-for-smt.pdf,co-training-for-smt,Figure,1,https://www.dropbox.com/request/anuCu09cu9ZyfapRaoLH,co-training-for-smt-figure-1.pdf,Contrasting context words derived from the adjacent vs dependency models for the above example.,Top 10 translation candidates for the spanish word “camino (way)” for the best adjacent context model (Adjbow) and best dependency context model (Depposn). The bold English terms show the acceptable translations.,Performance of various context-based models learned from monolingual corpora and phrase-table learned from parallel corpora on Noun translation.,"List of 20 most confident mappings using the dependency context based model for noun translation. Note that although the first mapping is the correct one, it was not present in the lexicon used for evaluation and hence is marked as incorrect.",Translation accuracy plotted against training corpus size.
5,Co-Training for Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/co-training-for-smt.pdf,co-training-for-smt,Figure,2,https://www.dropbox.com/request/anuCu09cu9ZyfapRaoLH,co-training-for-smt-figure-2.pdf,Co-Training for Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/co-training-for-smt.pdf,co-training-for-smt,Figure,3,https://www.dropbox.com/request/anuCu09cu9ZyfapRaoLH,co-training-for-smt-figure-3.pdf,Co-Training for Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/co-training-for-smt.pdf,co-training-for-smt,Figure,4,https://www.dropbox.com/request/anuCu09cu9ZyfapRaoLH,co-training-for-smt-figure-4.pdf,Co-Training for Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/co-training-for-smt.pdf,co-training-for-smt,Figure,5,https://www.dropbox.com/request/anuCu09cu9ZyfapRaoLH,co-training-for-smt-figure-5.pdf,Learning Sentential Paraphrases from Bilingual Parallel Corpora for Text-to-Text Generation,http://cis.upenn.edu/~ccb/publications/learning-sentential-paraphrases-from-bilingual-parallel-corpora.pdf,learning-sentential-paraphrases-from-bilingual-parallel-corpora,Table,2,https://www.dropbox.com/request/NXdIZpZjW82lv4TePquv,learning-sentential-paraphrases-from-bilingual-parallel-corpora-table-2.pdf,"Co-training using German, French, and Spanish sources as views on English translations",The co-training algorithm for machine translation,“Coaching” of German to English by a French to English translation model,“Coaching” of German to English by multiple translation models,Number and distribution of rules in our para- phrase grammar. Note the significant number of identity paraphrases and rules with complex nonterminal labels.
5,Learning Sentential Paraphrases from Bilingual Parallel Corpora for Text-to-Text Generation,http://cis.upenn.edu/~ccb/publications/learning-sentential-paraphrases-from-bilingual-parallel-corpora.pdf,learning-sentential-paraphrases-from-bilingual-parallel-corpora,Table,3,https://www.dropbox.com/request/NXdIZpZjW82lv4TePquv,learning-sentential-paraphrases-from-bilingual-parallel-corpora-table-3.pdf,Learning Sentential Paraphrases from Bilingual Parallel Corpora for Text-to-Text Generation,http://cis.upenn.edu/~ccb/publications/learning-sentential-paraphrases-from-bilingual-parallel-corpora.pdf,learning-sentential-paraphrases-from-bilingual-parallel-corpora,Table,4,https://www.dropbox.com/request/NXdIZpZjW82lv4TePquv,learning-sentential-paraphrases-from-bilingual-parallel-corpora-table-4.pdf,Learning Sentential Paraphrases from Bilingual Parallel Corpora for Text-to-Text Generation,http://cis.upenn.edu/~ccb/publications/learning-sentential-paraphrases-from-bilingual-parallel-corpora.pdf,learning-sentential-paraphrases-from-bilingual-parallel-corpora,Table,5,https://www.dropbox.com/request/NXdIZpZjW82lv4TePquv,learning-sentential-paraphrases-from-bilingual-parallel-corpora-table-5.pdf,Using Mechanical Turk to Build Machine Translation Evaluation Sets,http://cis.upenn.edu/~ccb/publications/using-mechanical-turk-to-build-machine-translation-evaluation-sets.pdf,using-mechanical-turk-to-build-machine-translation-evaluation-sets,Table,1,https://www.dropbox.com/request/Mo7w7XN4oLMg6gow0gQM,using-mechanical-turk-to-build-machine-translation-evaluation-sets-table-1.pdf,Using Mechanical Turk to Build Machine Translation Evaluation Sets,http://cis.upenn.edu/~ccb/publications/using-mechanical-turk-to-build-machine-translation-evaluation-sets.pdf,using-mechanical-turk-to-build-machine-translation-evaluation-sets,Table,2,https://www.dropbox.com/request/Mo7w7XN4oLMg6gow0gQM,using-mechanical-turk-to-build-machine-translation-evaluation-sets-table-2.pdf,"Results of the human evaluation on longer compressions: pairwise compression rates (CR), meaning and grammaticality scores. Bold indicates a statistically significance difference at p < 0.05.","Human evaluation for shorter compressions and for variations of our paraphrase system. +Feat. includes the compression features from Section 6.1, +Aug. includes optional deletion rules from Section 6.4.",Example compressions produced by the two systems in Table 3 for three input sentences from our test data.,"This table shows three MT systems evaluated on five different test sets. For each system-test set pair, two numbers are displayed. The top number is the BLEU score for that system when using that test set. For example, ISI-Syntax tested on the NIST-2009 test set has a BLEU score of 33.10. The bottom number is the percentage of baseline system performance that is achieved. ISI-Syntax (the highest-performing system on NIST2009 to our knowledge) is used as the baseline. Thus, it will always have 100% as the percentage performance for all of the test sets. To illustrate computing the percentage performance for the other systems, consider for JHU-Syntax tested on NIST2009, that its BLEU score of 32.77 divided by the BLEU score of the baseline system is 32.77/33.10  99.00%","This table shows three MT systems evaluated using the official NIST2009 test set and the two test sets we constructed (MTurk-NoEditing and MTurk-Edited). For each system-test set pair, two numbers are displayed. The top number is the BLEU score for that system when using that test set. For example, ISI-Syntax tested on the NIST-2009 test set has a BLEU score of 33.10. The bottom number is the percentage of baseline system performance that is achieved. ISI-Syntax (the highest-performing system on NIST2009 to our knowledge) is used as the baseline."
5,ParaMetric&colon; An Automatic Evaluation Metric for Paraphrasing,http://cis.upenn.edu/~ccb/publications/parametric.pdf,parametric,Figure,1,https://www.dropbox.com/request/9bMDMjMmdza9lzUbBPs9,parametric-figure-1.pdf,ParaMetric&colon; An Automatic Evaluation Metric for Paraphrasing,http://cis.upenn.edu/~ccb/publications/parametric.pdf,parametric,Figure,2,https://www.dropbox.com/request/9bMDMjMmdza9lzUbBPs9,parametric-figure-2.pdf,ParaMetric&colon; An Automatic Evaluation Metric for Paraphrasing,http://cis.upenn.edu/~ccb/publications/parametric.pdf,parametric,Figure,3,https://www.dropbox.com/request/9bMDMjMmdza9lzUbBPs9,parametric-figure-3.pdf,ParaMetric&colon; An Automatic Evaluation Metric for Paraphrasing,http://cis.upenn.edu/~ccb/publications/parametric.pdf,parametric,Table,1,https://www.dropbox.com/request/9bMDMjMmdza9lzUbBPs9,parametric-table-1.pdf,The Arabic Online Commentary Dataset&colon; An Annotated Dataset of Informal Arabic with High Dialectal Content,http://cis.upenn.edu/~ccb/publications/arabic-dialect-corpus.pdf,arabic-dialect-corpus,Table,1,https://www.dropbox.com/request/v3nq1tDIp8o1Jne7l3eY,arabic-dialect-corpus-table-1.pdf,Pairs of English sentences were aligned by hand. Black squares indicate paraphrase corre- spondences.,Pang et al. (2003) created word graphs by merging parse trees. Paths with the same start and end nodes are treated as paraphrases.,Bannard and Callison-Burch (2005) extracted paraphrases by equating English phrases that share a common translation.,Non-identical words and phrases which are identified as being in correspondence by the alignments in Figure 1.,"A summary of the different components of the AOC dataset. Overall, 1.4M comments were harvested from 86.1K articles, corresponding to 52.1M words."
5,The Arabic Online Commentary Dataset&colon; An Annotated Dataset of Informal Arabic with High Dialectal Content,http://cis.upenn.edu/~ccb/publications/arabic-dialect-corpus.pdf,arabic-dialect-corpus,Table,2,https://www.dropbox.com/request/v3nq1tDIp8o1Jne7l3eY,arabic-dialect-corpus-table-2.pdf,The Arabic Online Commentary Dataset&colon; An Annotated Dataset of Informal Arabic with High Dialectal Content,http://cis.upenn.edu/~ccb/publications/arabic-dialect-corpus.pdf,arabic-dialect-corpus,Table,3,https://www.dropbox.com/request/v3nq1tDIp8o1Jne7l3eY,arabic-dialect-corpus-table-3.pdf,Arabic Dialect Identification,http://cis.upenn.edu/~ccb/publications/arabic-dialect-id.pdf,arabic-dialect-id,Figure,1,https://www.dropbox.com/request/CHr8JGFw4VEjNOeiJIgm,arabic-dialect-id-figure-1.pdf,Arabic Dialect Identification,http://cis.upenn.edu/~ccb/publications/arabic-dialect-id.pdf,arabic-dialect-id,Figure,2,https://www.dropbox.com/request/CHr8JGFw4VEjNOeiJIgm,arabic-dialect-id-figure-2.pdf,Arabic Dialect Identification,http://cis.upenn.edu/~ccb/publications/arabic-dialect-id.pdf,arabic-dialect-id,Figure,3,https://www.dropbox.com/request/CHr8JGFw4VEjNOeiJIgm,arabic-dialect-id-figure-3.pdf,A breakdown of sentences for which ≥ 2 annotators agreed on whether dialectal content exists or not.,"Accuracy, dialect precision, and dialect recall (10-fold cross validation) for various classification tasks.","One possible breakdown of spoken Arabic into dialect groups: Maghrebi, Egyptian, Levantine, Gulf, and Iraqi. Habash (2010) and Versteegh (2001) give a breakdown along mostly the same lines. Note that this is a relatively coarse breakdown, and further division of the dialect groups is possible, especially in large regions such as the Maghreb.","Two roughly equivalent Arabic sentences, one in MSA and one in Levantine Arabic, translated by the same MT system (Google Translate) into English. An acceptable translation would be When will we see this group of criminals undergo trial (or tried)?. The MSA variant is handled well, while the dialectal variant is mostly transliterated.","Two roughly equivalent Arabic sentences, one in MSA and one in Egyptian Arabic, translated by the same MT system (Google Translate) into English. An acceptable translation would be What is this that is happening? What is this that I’m seeing?. As in Figure 2, the dialectal variant is handles quite poorly."
5,Arabic Dialect Identification,http://cis.upenn.edu/~ccb/publications/arabic-dialect-id.pdf,arabic-dialect-id,Figure,4,https://www.dropbox.com/request/CHr8JGFw4VEjNOeiJIgm,arabic-dialect-id-figure-4.pdf,Arabic Dialect Identification,http://cis.upenn.edu/~ccb/publications/arabic-dialect-id.pdf,arabic-dialect-id,Figure,5,https://www.dropbox.com/request/CHr8JGFw4VEjNOeiJIgm,arabic-dialect-id-figure-5.pdf,Arabic Dialect Identification,http://cis.upenn.edu/~ccb/publications/arabic-dialect-id.pdf,arabic-dialect-id,Figure,6,https://www.dropbox.com/request/CHr8JGFw4VEjNOeiJIgm,arabic-dialect-id-figure-6.pdf,Arabic Dialect Identification,http://cis.upenn.edu/~ccb/publications/arabic-dialect-id.pdf,arabic-dialect-id,Figure,7,https://www.dropbox.com/request/CHr8JGFw4VEjNOeiJIgm,arabic-dialect-id-figure-7.pdf,Arabic Dialect Identification,http://cis.upenn.edu/~ccb/publications/arabic-dialect-id.pdf,arabic-dialect-id,Figure,8,https://www.dropbox.com/request/CHr8JGFw4VEjNOeiJIgm,arabic-dialect-id-figure-8.pdf,"The output of a Spanish-to-English system when given a Portuguese sentence as input, compared to the output of a Portuguese-to-English system, which performs well. The behavior is very similar to that in Figures 2 and 3, namely the failure to translate out-of-vocabulary words when there is a language mismatch.","Three sentences that were identified by our annotators as dialectical, even thought they do not contain individually dialectal words. A word-based OOV-detection approach would fail to classify these sentences as being dialectal, since all these words could appear in an MSA corpus. One might argue that a distinction should be drawn between informal uses of MSA versus dialectical sentences, but annotators consistently classify these sentences as dialect.","The dialectal sentences of Figure 5, with MSA equivalents.","The interface for the dialect identification task. This example, and the full interface, can be viewed at the URL http://bit.ly/eUtiO3.","The distribution of labels provided by the workers for the dialect identification task, over all three news sources (a) and over each individual news source (b–d). Al-Ghad is published in Jordan, Al-Riyadh in Saudi Arabia, and Al-Youm Al-Sabe’ in Egypt. Their local readerships are reflected in the higher proportion of corresponding dialects. Note that this is not a breakdown on the sentence level, and does not reflect any kind of majority voting. For example, most of the LEV labels on sentences from the Saudi newspaper are trumped by GLF labels when taking a majority vote, making the proportion of LEV-majority sentences smaller than what might be deduced by looking at the label distribution in (c)."
5,Arabic Dialect Identification,http://cis.upenn.edu/~ccb/publications/arabic-dialect-id.pdf,arabic-dialect-id,Figure,9,https://www.dropbox.com/request/CHr8JGFw4VEjNOeiJIgm,arabic-dialect-id-figure-9.pdf,Arabic Dialect Identification,http://cis.upenn.edu/~ccb/publications/arabic-dialect-id.pdf,arabic-dialect-id,Figure,10,https://www.dropbox.com/request/CHr8JGFw4VEjNOeiJIgm,arabic-dialect-id-figure-10.pdf,Arabic Dialect Identification,http://cis.upenn.edu/~ccb/publications/arabic-dialect-id.pdf,arabic-dialect-id,Figure,11,https://www.dropbox.com/request/CHr8JGFw4VEjNOeiJIgm,arabic-dialect-id-figure-11.pdf,Arabic Dialect Identification,http://cis.upenn.edu/~ccb/publications/arabic-dialect-id.pdf,arabic-dialect-id,Table,1,https://www.dropbox.com/request/CHr8JGFw4VEjNOeiJIgm,arabic-dialect-id-table-1.pdf,Arabic Dialect Identification,http://cis.upenn.edu/~ccb/publications/arabic-dialect-id.pdf,arabic-dialect-id,Table,2,https://www.dropbox.com/request/CHr8JGFw4VEjNOeiJIgm,arabic-dialect-id-table-2.pdf,"A bubble chart showing workers’ MSA and dialect recall. Each data point (or ‘bubble’) in the graph represents one annotator, with the bubble size corresponding to the number of Assignments completed by that annotator.","Learning curves for the general MSA vs. dialect task, with all three news sources pooled together. Learning curves for the individual news sources can be found in Figure 11. The 83% line has no significance, and is provided to ease comparison with Figure 11.","Learning curves for the MSA vs. dialect task, for each of the three news sources. The 83% line has no significance, and is provided to ease comparison across the three components, and with Figure 10.","A few examples illustrating similarities and differences across MSA and three Arabic dialects: Levantine, Gulf, and Egyptian. Even when a word is spelled the same across two or more varieties, the pronunciation might differ due to differences in short vowels (which are not spelled out). Also, due to the lack of orthography standardization, and variance in pronunciation even within a single dialect, some dialectal words could have more than one spelling (e.g. Egyptian “I drink” could be bAšrb, Levantine “He drinks” could be byšrb). (We use the Habash-Soudi-Buckwalter transliteration scheme to represent Arabic orthography, which maps each Arabic letter to a single, distinct character. We provide a table with the character mapping in Appendix A.)","A summary of the different components of the AOC dataset. Overall, 1.4M comments were harvested from 86.1K articles, corresponding to 52.1M words."
5,Arabic Dialect Identification,http://cis.upenn.edu/~ccb/publications/arabic-dialect-id.pdf,arabic-dialect-id,Table,3,https://www.dropbox.com/request/CHr8JGFw4VEjNOeiJIgm,arabic-dialect-id-table-3.pdf,Arabic Dialect Identification,http://cis.upenn.edu/~ccb/publications/arabic-dialect-id.pdf,arabic-dialect-id,Table,4,https://www.dropbox.com/request/CHr8JGFw4VEjNOeiJIgm,arabic-dialect-id-table-4.pdf,Arabic Dialect Identification,http://cis.upenn.edu/~ccb/publications/arabic-dialect-id.pdf,arabic-dialect-id,Table,5,https://www.dropbox.com/request/CHr8JGFw4VEjNOeiJIgm,arabic-dialect-id-table-5.pdf,Arabic Dialect Identification,http://cis.upenn.edu/~ccb/publications/arabic-dialect-id.pdf,arabic-dialect-id,Table,6,https://www.dropbox.com/request/CHr8JGFw4VEjNOeiJIgm,arabic-dialect-id-table-6.pdf,Arabic Dialect Identification,http://cis.upenn.edu/~ccb/publications/arabic-dialect-id.pdf,arabic-dialect-id,Table,7,https://www.dropbox.com/request/CHr8JGFw4VEjNOeiJIgm,arabic-dialect-id-table-7.pdf,"Some statistics over the labels provided by three spammers. Compared to the typical worker (right-most column), all workers perform terribly on the MSA control items, and also usually fail to recognize dialectal content in commentary sentences. Other red flags, such as geographic location and ‘identifying’ unrepresented dialects, are further proof of the spammy behavior.","The specific-dialect label distribution (given that a dialect label was provided), shown for each speaker group.","Two annotators with a General label bias, one who uses the label liberally, and one who is more conservative. Note that in both cases, there is a noticeably smaller percentage of General labels in the Egyptian newspaper than in the Jordanian and Saudi newspapers.","Accuracy rates (%) on several 2-way classification tasks (MSA vs. dialect) for various models. Models in the top part of the table do not utilize the dialect-annotated data, while models in the bottom part do. (For the latter kind of models, the accuracy rates reported are based on a training set size of 90% of the available data.)","Confusion matrix in the 4-way classification setup. Rows correspond to actual labels, and columns correspond to predicted labels. For instance, 6.7% of MSA sentences were given a GLF label (first row, third column). Note that entries within a single row sum to 100%."
5,Arabic Dialect Identification,http://cis.upenn.edu/~ccb/publications/arabic-dialect-id.pdf,arabic-dialect-id,Table,8,https://www.dropbox.com/request/CHr8JGFw4VEjNOeiJIgm,arabic-dialect-id-table-8.pdf,Constructing Parallel Corpora for Six Indian Languages via Crowdsourcing,http://cis.upenn.edu/~ccb/publications/constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing.pdf,constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing,Figure,1,https://www.dropbox.com/request/jL5BrqIyX8byv03UaOEL,constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing-figure-1.pdf,Constructing Parallel Corpora for Six Indian Languages via Crowdsourcing,http://cis.upenn.edu/~ccb/publications/constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing.pdf,constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing,Figure,2,https://www.dropbox.com/request/jL5BrqIyX8byv03UaOEL,constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing-figure-2.pdf,Constructing Parallel Corpora for Six Indian Languages via Crowdsourcing,http://cis.upenn.edu/~ccb/publications/constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing.pdf,constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing,Figure,3,https://www.dropbox.com/request/jL5BrqIyX8byv03UaOEL,constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing-figure-3.pdf,Constructing Parallel Corpora for Six Indian Languages via Crowdsourcing,http://cis.upenn.edu/~ccb/publications/constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing.pdf,constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing,Table,8,https://www.dropbox.com/request/jL5BrqIyX8byv03UaOEL,constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing-table-8.pdf,"Predicted label breakdown for the crawled data, over the four varieties of Arabic. All varieties were given equal priors.",An example of SOV word ordering in Tamil. Translation: The senator prepared her remarks.,"An example of the morphology of the Bengali word হাত্সিলাম, meaning [I] was walking. CONT denotes the continuous aspect, while PAST denotes past tense.","The total volume of translations (measured in English words) as a function of elapsed days. For Malayalam, we collected half a million words of translations in just under a week.","Misspellings of japanese (947) in the training portion of the Urdu-English data, along with their counts."
5,Constructing Parallel Corpora for Six Indian Languages via Crowdsourcing,http://cis.upenn.edu/~ccb/publications/constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing.pdf,constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing,Table,9,https://www.dropbox.com/request/jL5BrqIyX8byv03UaOEL,constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing-table-9.pdf,A Compact Data Structure for Searchable Translation Memories,http://cis.upenn.edu/~ccb/publications/compact-data-structure-for-searchable-translation-memories.pdf,compact-data-structure-for-searchable-translation-memories,Figure,1,https://www.dropbox.com/request/Eb0mfSvCQrPyNEwFjP7V,compact-data-structure-for-searchable-translation-memories-figure-1.pdf,A Compact Data Structure for Searchable Translation Memories,http://cis.upenn.edu/~ccb/publications/compact-data-structure-for-searchable-translation-memories.pdf,compact-data-structure-for-searchable-translation-memories,Figure,2,https://www.dropbox.com/request/Eb0mfSvCQrPyNEwFjP7V,compact-data-structure-for-searchable-translation-memories-figure-2.pdf,A Compact Data Structure for Searchable Translation Memories,http://cis.upenn.edu/~ccb/publications/compact-data-structure-for-searchable-translation-memories.pdf,compact-data-structure-for-searchable-translation-memories,Figure,3,https://www.dropbox.com/request/Eb0mfSvCQrPyNEwFjP7V,compact-data-structure-for-searchable-translation-memories-figure-3.pdf,A Compact Data Structure for Searchable Translation Memories,http://cis.upenn.edu/~ccb/publications/compact-data-structure-for-searchable-translation-memories.pdf,compact-data-structure-for-searchable-translation-memories,Figure,4,https://www.dropbox.com/request/Eb0mfSvCQrPyNEwFjP7V,compact-data-structure-for-searchable-translation-memories-figure-4.pdf,"Hiero translation results using Berkeley align- ments instead of GIZA++ heuristics. The gain columns denotes improvements relative to the Hiero systems in Ta- ble 5. In many cases (bold gains), the BLEU scores are at or above even the SAMT models from that table.",Search results for the English phrase “west bank”,A word-level alignment for a sentence pair that occurs in our training data,"An initialized, unsorted suffix array for a very small corpus",A sorted suffix array and its corresponding suffixes
5,A Compact Data Structure for Searchable Translation Memories,http://cis.upenn.edu/~ccb/publications/compact-data-structure-for-searchable-translation-memories.pdf,compact-data-structure-for-searchable-translation-memories,Figure,5,https://www.dropbox.com/request/Eb0mfSvCQrPyNEwFjP7V,compact-data-structure-for-searchable-translation-memories-figure-5.pdf,Dirt Cheap Web-Scale Parallel Text from the Common Crawl,http://cis.upenn.edu/~ccb/publications/bitexts-from-common-crawl.pdf,bitexts-from-common-crawl,Table,1,https://www.dropbox.com/request/VqcWltaQJ8pv34oWRQ0y,bitexts-from-common-crawl-table-1.pdf,Dirt Cheap Web-Scale Parallel Text from the Common Crawl,http://cis.upenn.edu/~ccb/publications/bitexts-from-common-crawl.pdf,bitexts-from-common-crawl,Table,2,https://www.dropbox.com/request/VqcWltaQJ8pv34oWRQ0y,bitexts-from-common-crawl-table-2.pdf,Dirt Cheap Web-Scale Parallel Text from the Common Crawl,http://cis.upenn.edu/~ccb/publications/bitexts-from-common-crawl.pdf,bitexts-from-common-crawl,Table,3,https://www.dropbox.com/request/VqcWltaQJ8pv34oWRQ0y,bitexts-from-common-crawl-table-3.pdf,Dirt Cheap Web-Scale Parallel Text from the Common Crawl,http://cis.upenn.edu/~ccb/publications/bitexts-from-common-crawl.pdf,bitexts-from-common-crawl,Table,9,https://www.dropbox.com/request/VqcWltaQJ8pv34oWRQ0y,bitexts-from-common-crawl-table-9.pdf,A word-level alignment for the sentence in the suffix array,"The amount of parallel data mined from CommonCrawl for each language paired with English. Source tokens are counts of the foreign language tokens, and target tokens are counts of the English language tokens.","Manual evaluation of precision (by sen- tence pair) on the extracted parallel data for Span- ish, French, and German (paired with English).",Automatic evaluation of precision through language identification for several lan- guages paired with English.,"BLEU scores for French-English and English-French before and after adding the mined parallel data to systems trained on data from WMT data including the French-English Giga- word (Callison-Burch et al., 2011)."
5,Dirt Cheap Web-Scale Parallel Text from the Common Crawl,http://cis.upenn.edu/~ccb/publications/bitexts-from-common-crawl.pdf,bitexts-from-common-crawl,Table,10,https://www.dropbox.com/request/VqcWltaQJ8pv34oWRQ0y,bitexts-from-common-crawl-table-10.pdf,Dirt Cheap Web-Scale Parallel Text from the Common Crawl,http://cis.upenn.edu/~ccb/publications/bitexts-from-common-crawl.pdf,bitexts-from-common-crawl,Table,11,https://www.dropbox.com/request/VqcWltaQJ8pv34oWRQ0y,bitexts-from-common-crawl-table-11.pdf,Joshua 4.0&colon; Packing PRO and Paraphrases,http://cis.upenn.edu/~ccb/publications/joshua-4.0.pdf,joshua-4.0,Figure,1,https://www.dropbox.com/request/3926sO1QHoe6AnmOrkGc,joshua-4.0-figure-1.pdf,Joshua 4.0&colon; Packing PRO and Paraphrases,http://cis.upenn.edu/~ccb/publications/joshua-4.0.pdf,joshua-4.0,Figure,2,https://www.dropbox.com/request/3926sO1QHoe6AnmOrkGc,joshua-4.0-figure-2.pdf,Joshua 4.0&colon; Packing PRO and Paraphrases,http://cis.upenn.edu/~ccb/publications/joshua-4.0.pdf,joshua-4.0,Figure,3,https://www.dropbox.com/request/3926sO1QHoe6AnmOrkGc,joshua-4.0-figure-3.pdf,The size (in English tokens) of the training corpora used in the SMT experiments from Tables 8 and 9 for each language pair.,n-gram coverage percentages (up to 4-grams) of the source side of our test sets given our different parallel training corpora computed at the type level.,"An illustration of our packed grammar data structures. The source sides of the grammar rules are stored in a packed trie. Each node may contain n children and the symbols linking to them, and m entries for rules that share the same source side. Each rule entry links to a node in the target-side trie, where the full target string can be retrieved by walking up the trie until the root is reached. The rule entries also contain a data block id, which identifies feature data attached to the rule. The features are encoded according to a type/quantization specification and stored as variable-length blocks of data in a byte buffer.","A visualization of the loading and decoding speed on the WMT12 French-English development set contrasting the packed grammar representation with the standard format. Grammar loading for the packed grammar representation is substantially faster than that for the baseline setup. Even with a slightly slower decoding speed (note the difference in the slopes) the packed grammar finishes in less than half the time, compared to the standard format.",Experimental results on the development and test sets. The x-axis is the number of iterations (up to 30) and the y-axis is the BLEU score. The three curves in each figure correspond to three classifiers. Upper row: results trained using only dense features (10 features); Lower row: results trained using dense+sparse features (1026 features). Left column: development set (MT03); Middle column: test set (MT04); Right column: test set (MT05).
5,Joshua 4.0&colon; Packing PRO and Paraphrases,http://cis.upenn.edu/~ccb/publications/joshua-4.0.pdf,joshua-4.0,Table,1,https://www.dropbox.com/request/3926sO1QHoe6AnmOrkGc,joshua-4.0-table-1.pdf,Joshua 4.0&colon; Packing PRO and Paraphrases,http://cis.upenn.edu/~ccb/publications/joshua-4.0.pdf,joshua-4.0,Table,2,https://www.dropbox.com/request/3926sO1QHoe6AnmOrkGc,joshua-4.0-table-2.pdf,Joshua 4.0&colon; Packing PRO and Paraphrases,http://cis.upenn.edu/~ccb/publications/joshua-4.0.pdf,joshua-4.0,Table,3,https://www.dropbox.com/request/3926sO1QHoe6AnmOrkGc,joshua-4.0-table-3.pdf,Joshua 4.0&colon; Packing PRO and Paraphrases,http://cis.upenn.edu/~ccb/publications/joshua-4.0.pdf,joshua-4.0,Table,4,https://www.dropbox.com/request/3926sO1QHoe6AnmOrkGc,joshua-4.0-table-4.pdf,Joshua 4.0&colon; Packing PRO and Paraphrases,http://cis.upenn.edu/~ccb/publications/joshua-4.0.pdf,joshua-4.0,Table,5,https://www.dropbox.com/request/3926sO1QHoe6AnmOrkGc,joshua-4.0-table-5.pdf,Decoding-time memory use for the packed grammar versus the standard grammar format. Even without lossy quantization the packed grammar representation yields significant savings in memory consumption. Adding 8-bit quantization for the real- valued features in the grammar reduces even large syntactic grammars to a manageable size.,Comparison between the results given by Z-MERT and J-PRO (trained with 10 features).,Extraction times and grammar sizes for Hiero grammars using the Europarl and News Commentary training data for each listed language pair.,Extraction times and grammar sizes for the SAMT grammars using the Europarl and News Commentary training data for each listed language pair.,Large paraphrase grammars extracted from EuroParl data using Thrax. The sentence and word counts refer to the English side of the bitexts used.
5,Feasibility of Human-in-the-loop Minimum Error Rate Training,http://cis.upenn.edu/~ccb/publications/HMERT.pdf,HMERT,Figure,1,https://www.dropbox.com/request/YlbURMFD9QbYWJIMTKvD,HMERT-figure-1.pdf,Feasibility of Human-in-the-loop Minimum Error Rate Training,http://cis.upenn.edu/~ccb/publications/HMERT.pdf,HMERT,Figure,2,https://www.dropbox.com/request/YlbURMFD9QbYWJIMTKvD,HMERT-figure-2.pdf,Feasibility of Human-in-the-loop Minimum Error Rate Training,http://cis.upenn.edu/~ccb/publications/HMERT.pdf,HMERT,Figure,3,https://www.dropbox.com/request/YlbURMFD9QbYWJIMTKvD,HMERT-figure-3.pdf,Feasibility of Human-in-the-loop Minimum Error Rate Training,http://cis.upenn.edu/~ccb/publications/HMERT.pdf,HMERT,Table,1,https://www.dropbox.com/request/YlbURMFD9QbYWJIMTKvD,HMERT-table-1.pdf,Feasibility of Human-in-the-loop Minimum Error Rate Training,http://cis.upenn.edu/~ccb/publications/HMERT.pdf,HMERT,Table,2,https://www.dropbox.com/request/YlbURMFD9QbYWJIMTKvD,HMERT-table-2.pdf,"Och’s method applied to a set of two foreign sentences. This figure is essentially a visualization of equation (1). We show here sufficient statistics for TER for simplicity, since there are only 2 of them, but the metric optimized in MERT is usually BLEU.","The source parse tree (top) and the candidate derivation tree (bottom). Nodes in the parse tree with a thick border correspond to the frontier node set with maxLen = 4. The human annotator only sees the portion surrounded by the dashed rectangle, including the highlighting (though excluding the word alignment links).",Label percolation under different maxLen values. The bottom two curves are the breakdown of the difference between the middle two. Accuracy is measured against majority votes.,"Ranking comparison results. The left half corresponds to the experiment (open to all workers) where the English reference was shown, whereas the right half corresponds to the experiment (open only to workers living in Germany) where the English reference was not shown.","Ranking comparison results, grouped by sentence. This table corresponds to the left half of Table 1. 3 judgments were collected for each comparison, with the “aggregate” for a comparison calculated from these 3 judgments. For instance, an aggregate of “RYPT +3” means all 3 judgments favored RYPT’s choice, and “RYPT +1” means one more judgment favored RYPT than did BLEU."
5,Large-Scale Cost-Focused Active Learning for Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/cost-focused-active-learning-for-statistical-machine-translation.pdf,cost-focused-active-learning-for-statistical-machine-translation,Figure,1,https://www.dropbox.com/request/6kRXBZv9SUMFwsBX6B6R,cost-focused-active-learning-for-statistical-machine-translation-figure-1.pdf,Large-Scale Cost-Focused Active Learning for Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/cost-focused-active-learning-for-statistical-machine-translation.pdf,cost-focused-active-learning-for-statistical-machine-translation,Figure,2,https://www.dropbox.com/request/6kRXBZv9SUMFwsBX6B6R,cost-focused-active-learning-for-statistical-machine-translation-figure-2.pdf,Large-Scale Cost-Focused Active Learning for Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/cost-focused-active-learning-for-statistical-machine-translation.pdf,cost-focused-active-learning-for-statistical-machine-translation,Figure,3,https://www.dropbox.com/request/6kRXBZv9SUMFwsBX6B6R,cost-focused-active-learning-for-statistical-machine-translation-figure-3.pdf,Large-Scale Cost-Focused Active Learning for Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/cost-focused-active-learning-for-statistical-machine-translation.pdf,cost-focused-active-learning-for-statistical-machine-translation,Figure,4,https://www.dropbox.com/request/6kRXBZv9SUMFwsBX6B6R,cost-focused-active-learning-for-statistical-machine-translation-figure-4.pdf,Large-Scale Cost-Focused Active Learning for Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/cost-focused-active-learning-for-statistical-machine-translation.pdf,cost-focused-active-learning-for-statistical-machine-translation,Figure,5,https://www.dropbox.com/request/6kRXBZv9SUMFwsBX6B6R,cost-focused-active-learning-for-statistical-machine-translation-figure-5.pdf,"Syntax-based and Hierarchical Phrase-Based MT systems’ learning curves on the LDC Urdu-English language pack. The x-axis measures the number of sentence pairs in the training data. The y-axis measures BLEU score. Note the diminishing returns as more data is added. Also note how relatively early on in the process previous studies were terminated. In contrast, the focus of our main experiments doesn’t even be- gin until much higher performance has already been achieved with a period of diminishing returns firmly established.",The VG sentence selection algorithm,Random vs VG selection. The x-axis measures the number of sentence pairs in the training data. The y-axis measures BLEU score.,Random vs VG selection. The x-axis measures the number of foreign words in the training data. The y-axis measures BLEU score.,Random vs Shortest vs Longest selection. The x-axis measures the number of sentence pairs in the training data. The y-axis measures BLEU score.
5,Large-Scale Cost-Focused Active Learning for Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/cost-focused-active-learning-for-statistical-machine-translation.pdf,cost-focused-active-learning-for-statistical-machine-translation,Figure,6,https://www.dropbox.com/request/6kRXBZv9SUMFwsBX6B6R,cost-focused-active-learning-for-statistical-machine-translation-figure-6.pdf,Large-Scale Cost-Focused Active Learning for Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/cost-focused-active-learning-for-statistical-machine-translation.pdf,cost-focused-active-learning-for-statistical-machine-translation,Figure,7,https://www.dropbox.com/request/6kRXBZv9SUMFwsBX6B6R,cost-focused-active-learning-for-statistical-machine-translation-figure-7.pdf,Large-Scale Cost-Focused Active Learning for Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/cost-focused-active-learning-for-statistical-machine-translation.pdf,cost-focused-active-learning-for-statistical-machine-translation,Figure,8,https://www.dropbox.com/request/6kRXBZv9SUMFwsBX6B6R,cost-focused-active-learning-for-statistical-machine-translation-figure-8.pdf,Large-Scale Cost-Focused Active Learning for Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/cost-focused-active-learning-for-statistical-machine-translation.pdf,cost-focused-active-learning-for-statistical-machine-translation,Figure,9,https://www.dropbox.com/request/6kRXBZv9SUMFwsBX6B6R,cost-focused-active-learning-for-statistical-machine-translation-figure-9.pdf,Large-Scale Cost-Focused Active Learning for Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/cost-focused-active-learning-for-statistical-machine-translation.pdf,cost-focused-active-learning-for-statistical-machine-translation,Figure,10,https://www.dropbox.com/request/6kRXBZv9SUMFwsBX6B6R,cost-focused-active-learning-for-statistical-machine-translation-figure-10.pdf,Random vs Shortest vs Longest selection. The x-axis measures the number of foreign words in the training data. The y-axis measures BLEU score.,VG vs MostNew vs ModerateNew selection. The x-axis measures the number of sentence pairs in the training data. The y-axis measures BLEU score.,Screenshot of the interface we used for soliciting translations for triggers.,HNG vs Random collection of new data via MTurk. y-axis measures BLEU. x-axis measures annotation time in seconds.,Distribution of translation speeds (in seconds per word) for HNG postings versus complete sentence postings. The y-axis measures relative frequency. The x-axis measures translation speed in seconds per word (so farther to the left is faster).
5,Large-Scale Cost-Focused Active Learning for Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/cost-focused-active-learning-for-statistical-machine-translation.pdf,cost-focused-active-learning-for-statistical-machine-translation,Figure,11,https://www.dropbox.com/request/6kRXBZv9SUMFwsBX6B6R,cost-focused-active-learning-for-statistical-machine-translation-figure-11.pdf,Paraphrase Substitution for Recognizing Textual Entailment,http://cis.upenn.edu/~ccb/publications/paraphrase-substitution-for-recognizing-textual-entailment.pdf,paraphrase-substitution-for-recognizing-textual-entailment,Figure,1,https://www.dropbox.com/request/uVEHmEMMUgS5dKHywxWA,paraphrase-substitution-for-recognizing-textual-entailment-figure-1.pdf,Paraphrase Substitution for Recognizing Textual Entailment,http://cis.upenn.edu/~ccb/publications/paraphrase-substitution-for-recognizing-textual-entailment.pdf,paraphrase-substitution-for-recognizing-textual-entailment,Figure,2,https://www.dropbox.com/request/uVEHmEMMUgS5dKHywxWA,paraphrase-substitution-for-recognizing-textual-entailment-figure-2.pdf,Paraphrase Substitution for Recognizing Textual Entailment,http://cis.upenn.edu/~ccb/publications/paraphrase-substitution-for-recognizing-textual-entailment.pdf,paraphrase-substitution-for-recognizing-textual-entailment,Figure,3,https://www.dropbox.com/request/uVEHmEMMUgS5dKHywxWA,paraphrase-substitution-for-recognizing-textual-entailment-figure-3.pdf,Paraphrase Substitution for Recognizing Textual Entailment,http://cis.upenn.edu/~ccb/publications/paraphrase-substitution-for-recognizing-textual-entailment.pdf,paraphrase-substitution-for-recognizing-textual-entailment,Table,1,https://www.dropbox.com/request/uVEHmEMMUgS5dKHywxWA,paraphrase-substitution-for-recognizing-textual-entailment-table-1.pdf,Bucking the trend: performance of HNG-selected additional data from BBC web crawl data annotated via Amazon Mechanical Turk. y-axis measures BLEU. x-axis measures number of words annotated.,A bilingual parallel corpus can be used to extract paraphrases,"Left, average scores of various metrics for German, English, Spanish, French, Italian, Dutch and Portugese; right, average scores, including dependency tree alignment, for English, Spanish and Dutch",{},Examples paraphrases and probabilities for the phrase dead bodies
5,Paraphrase Substitution for Recognizing Textual Entailment,http://cis.upenn.edu/~ccb/publications/paraphrase-substitution-for-recognizing-textual-entailment.pdf,paraphrase-substitution-for-recognizing-textual-entailment,Table,2,https://www.dropbox.com/request/uVEHmEMMUgS5dKHywxWA,paraphrase-substitution-for-recognizing-textual-entailment-table-2.pdf,Paraphrase Substitution for Recognizing Textual Entailment,http://cis.upenn.edu/~ccb/publications/paraphrase-substitution-for-recognizing-textual-entailment.pdf,paraphrase-substitution-for-recognizing-textual-entailment,Table,3,https://www.dropbox.com/request/uVEHmEMMUgS5dKHywxWA,paraphrase-substitution-for-recognizing-textual-entailment-table-3.pdf,Reranking Bilingually Extracted Paraphrases Using Monolingual Distributional Similarity,http://cis.upenn.edu/~ccb/publications/reranking-bilingually-extracted-paraphrases-using-monolingual-distributional-similarity.pdf,reranking-bilingually-extracted-paraphrases-using-monolingual-distributional-similarity,Figure,1,https://www.dropbox.com/request/Tmy4m71jFzcaJ28cFzdi,reranking-bilingually-extracted-paraphrases-using-monolingual-distributional-similarity-figure-1.pdf,Reranking Bilingually Extracted Paraphrases Using Monolingual Distributional Similarity,http://cis.upenn.edu/~ccb/publications/reranking-bilingually-extracted-paraphrases-using-monolingual-distributional-similarity.pdf,reranking-bilingually-extracted-paraphrases-using-monolingual-distributional-similarity,Figure,2,https://www.dropbox.com/request/Tmy4m71jFzcaJ28cFzdi,reranking-bilingually-extracted-paraphrases-using-monolingual-distributional-similarity-figure-2.pdf,Reranking Bilingually Extracted Paraphrases Using Monolingual Distributional Similarity,http://cis.upenn.edu/~ccb/publications/reranking-bilingually-extracted-paraphrases-using-monolingual-distributional-similarity.pdf,reranking-bilingually-extracted-paraphrases-using-monolingual-distributional-similarity,Figure,3,https://www.dropbox.com/request/Tmy4m71jFzcaJ28cFzdi,reranking-bilingually-extracted-paraphrases-using-monolingual-distributional-similarity-figure-3.pdf,{},{},Using a bilingual parallel corpus to extract paraphrases.,"Averaged scores in the top K paraphrase candidates as a function of K for different reranking metrics. All methods performs similarly in meaning preservation, but SyntBiP-MonoDS outperforms other scoring methods in grammaticality, as shown in the bottom graph.",{}
5,Reranking Bilingually Extracted Paraphrases Using Monolingual Distributional Similarity,http://cis.upenn.edu/~ccb/publications/reranking-bilingually-extracted-paraphrases-using-monolingual-distributional-similarity.pdf,reranking-bilingually-extracted-paraphrases-using-monolingual-distributional-similarity,Figure,4,https://www.dropbox.com/request/Tmy4m71jFzcaJ28cFzdi,reranking-bilingually-extracted-paraphrases-using-monolingual-distributional-similarity-figure-4.pdf,Paraphrase Fragment Extraction from Monolingual Comparable Corpora,http://cis.upenn.edu/~ccb/publications/paraphrase-fragment-extraction-from-monolingual-comparable-corpora.pdf,paraphrase-fragment-extraction-from-monolingual-comparable-corpora,Table,3,https://www.dropbox.com/request/LtgwnOpmt1J1zJqQx30F,paraphrase-fragment-extraction-from-monolingual-comparable-corpora-table-3.pdf,Paraphrase Fragment Extraction from Monolingual Comparable Corpora,http://cis.upenn.edu/~ccb/publications/paraphrase-fragment-extraction-from-monolingual-comparable-corpora.pdf,paraphrase-fragment-extraction-from-monolingual-comparable-corpora,Table,4,https://www.dropbox.com/request/LtgwnOpmt1J1zJqQx30F,paraphrase-fragment-extraction-from-monolingual-comparable-corpora-table-4.pdf,Paraphrase Fragment Extraction from Monolingual Comparable Corpora,http://cis.upenn.edu/~ccb/publications/paraphrase-fragment-extraction-from-monolingual-comparable-corpora.pdf,paraphrase-fragment-extraction-from-monolingual-comparable-corpora,Table,5,https://www.dropbox.com/request/LtgwnOpmt1J1zJqQx30F,paraphrase-fragment-extraction-from-monolingual-comparable-corpora-table-5.pdf,Use of Modality and Negation in Semantically-Informed Syntactic MT,http://cis.upenn.edu/~ccb/publications/modality-and-negation-in-semantically-informed-syntactic-mt.pdf,modality-and-negation-in-semantically-informed-syntactic-mt,Figure,1,https://www.dropbox.com/request/dK8xCckJV139gDJWcPOH,modality-and-negation-in-semantically-informed-syntactic-mt-figure-1.pdf,{},The size of our corpus. We only used ca. 10% of the GIGAWORD corpus in the experiments and the size of the collection at each stage are shown in the table.,The (partial) distribution of N-grams (N=1-4) in different paraphrase collections.,Some examples of the extracted paraphrase fragment pairs.,Modality/Negation Tagging Examples
5,Use of Modality and Negation in Semantically-Informed Syntactic MT,http://cis.upenn.edu/~ccb/publications/modality-and-negation-in-semantically-informed-syntactic-mt.pdf,modality-and-negation-in-semantically-informed-syntactic-mt,Figure,2,https://www.dropbox.com/request/dK8xCckJV139gDJWcPOH,modality-and-negation-in-semantically-informed-syntactic-mt-figure-2.pdf,Use of Modality and Negation in Semantically-Informed Syntactic MT,http://cis.upenn.edu/~ccb/publications/modality-and-negation-in-semantically-informed-syntactic-mt.pdf,modality-and-negation-in-semantically-informed-syntactic-mt,Figure,8,https://www.dropbox.com/request/dK8xCckJV139gDJWcPOH,modality-and-negation-in-semantically-informed-syntactic-mt-figure-8.pdf,Use of Modality and Negation in Semantically-Informed Syntactic MT,http://cis.upenn.edu/~ccb/publications/modality-and-negation-in-semantically-informed-syntactic-mt.pdf,modality-and-negation-in-semantically-informed-syntactic-mt,Figure,9,https://www.dropbox.com/request/dK8xCckJV139gDJWcPOH,modality-and-negation-in-semantically-informed-syntactic-mt-figure-9.pdf,Use of Modality and Negation in Semantically-Informed Syntactic MT,http://cis.upenn.edu/~ccb/publications/modality-and-negation-in-semantically-informed-syntactic-mt.pdf,modality-and-negation-in-semantically-informed-syntactic-mt,Figure,10,https://www.dropbox.com/request/dK8xCckJV139gDJWcPOH,modality-and-negation-in-semantically-informed-syntactic-mt-figure-10.pdf,Use of Modality and Negation in Semantically-Informed Syntactic MT,http://cis.upenn.edu/~ccb/publications/modality-and-negation-in-semantically-informed-syntactic-mt.pdf,modality-and-negation-in-semantically-informed-syntactic-mt,Figure,11,https://www.dropbox.com/request/dK8xCckJV139gDJWcPOH,modality-and-negation-in-semantically-informed-syntactic-mt-figure-11.pdf,"An example of Urdu-English translation. Shown are an Urdu source document, a reference translation produced by a professional human translator, and machine translation output from a phrase-based model (Moses) without linguistic information, which is representative of state-of-the-art MT quality before the SIMT effort.",Example of Embedded Target Head found inside VP must be found,Example of Modality Composed with Negation: TrigAble and TrigNegation combine to form NOTAble,"A sentence on the English side of the bilingual parallel training corpus is parsed with a syntactic parser, and also tagged with our modality tagger. The tags are then grafted onto the syntactic parse tree to form new categories like VP-TargNOTAble and VP-TargRequire. Grafting happens prior to extracting translation rules, which happens normally except for the use of the augmented trees.","Example translation rules with tags for modality, negation, and entities combined with syntactic categories."
5,Use of Modality and Negation in Semantically-Informed Syntactic MT,http://cis.upenn.edu/~ccb/publications/modality-and-negation-in-semantically-informed-syntactic-mt.pdf,modality-and-negation-in-semantically-informed-syntactic-mt,Table,1,https://www.dropbox.com/request/dK8xCckJV139gDJWcPOH,modality-and-negation-in-semantically-informed-syntactic-mt-table-1.pdf,Use of Modality and Negation in Semantically-Informed Syntactic MT,http://cis.upenn.edu/~ccb/publications/modality-and-negation-in-semantically-informed-syntactic-mt.pdf,modality-and-negation-in-semantically-informed-syntactic-mt,Table,2,https://www.dropbox.com/request/dK8xCckJV139gDJWcPOH,modality-and-negation-in-semantically-informed-syntactic-mt-table-2.pdf,Evaluating sentence compression&colon; Pitfalls and suggested remedies,http://cis.upenn.edu/~ccb/publications/evaluating-sentence-compression-pitfalls-and-suggested-remedies.pdf,evaluating-sentence-compression-pitfalls-and-suggested-remedies,Figure,1,https://www.dropbox.com/request/CxWHTkjhsfBD6UBZtouS,evaluating-sentence-compression-pitfalls-and-suggested-remedies-figure-1.pdf,Evaluating sentence compression&colon; Pitfalls and suggested remedies,http://cis.upenn.edu/~ccb/publications/evaluating-sentence-compression-pitfalls-and-suggested-remedies.pdf,evaluating-sentence-compression-pitfalls-and-suggested-remedies,Table,1,https://www.dropbox.com/request/CxWHTkjhsfBD6UBZtouS,evaluating-sentence-compression-pitfalls-and-suggested-remedies-table-1.pdf,Evaluating sentence compression&colon; Pitfalls and suggested remedies,http://cis.upenn.edu/~ccb/publications/evaluating-sentence-compression-pitfalls-and-suggested-remedies.pdf,evaluating-sentence-compression-pitfalls-and-suggested-remedies,Table,2,https://www.dropbox.com/request/CxWHTkjhsfBD6UBZtouS,evaluating-sentence-compression-pitfalls-and-suggested-remedies-table-2.pdf,"The size of the various data sets used for the experiments in this paper including the training, development (dev), incremental test set (devtest) and blind test set (test). The dev/devtest was a split of the NIST08 Urdu-English test set, and the blind test set was NIST09.","Modality Tags with their Negated Versions. Note that Require and Permit are in a dual relation, and thus RequireNegation is represented as NOTPermit and PermitNegation is represented as NOTRequire.",Compression rate strongly correlates with human judgments of meaning and grammaticality. Gold represents gold-standard compression and Deletion the results of a leading deletion model. Gold.1 grammar judgments were made alongside the original sentence and Gold.2 were made in isolation.,Three acceptable compressions of a sentence created by different annotators (the first is the original).,"Mean quality ratings of two competing models once the compression rates have been standardized, and as reported in the original work (denoted ∗). There is no significant improvement, but the numerically better model changes."
5,Improved Statistical Translation Through Editing,http://cis.upenn.edu/~ccb/publications/improved-smt-through-editing.pdf,improved-smt-through-editing,Figure,1,https://www.dropbox.com/request/8id0uEY4LKeC1G7psxtG,improved-smt-through-editing-figure-1.pdf,Improved Statistical Translation Through Editing,http://cis.upenn.edu/~ccb/publications/improved-smt-through-editing.pdf,improved-smt-through-editing,Figure,2,https://www.dropbox.com/request/8id0uEY4LKeC1G7psxtG,improved-smt-through-editing-figure-2.pdf,Improved Statistical Translation Through Editing,http://cis.upenn.edu/~ccb/publications/improved-smt-through-editing.pdf,improved-smt-through-editing,Figure,3,https://www.dropbox.com/request/8id0uEY4LKeC1G7psxtG,improved-smt-through-editing-figure-3.pdf,Improved Statistical Translation Through Editing,http://cis.upenn.edu/~ccb/publications/improved-smt-through-editing.pdf,improved-smt-through-editing,Figure,4,https://www.dropbox.com/request/8id0uEY4LKeC1G7psxtG,improved-smt-through-editing-figure-4.pdf,Improved Statistical Translation Through Editing,http://cis.upenn.edu/~ccb/publications/improved-smt-through-editing.pdf,improved-smt-through-editing,Figure,5,https://www.dropbox.com/request/8id0uEY4LKeC1G7psxtG,improved-smt-through-editing-figure-5.pdf,A word-level alignment for a sentence pair that occurs in our training data,Extracting incrementally larger phrases from a word alignment,An example of the phrases that were used to translate two sentences,Word alignment produced by the alignment server for an edited translation,Word alignment produced by the alignment server for an edited translation
5,Improved Statistical Translation Through Editing,http://cis.upenn.edu/~ccb/publications/improved-smt-through-editing.pdf,improved-smt-through-editing,Figure,6,https://www.dropbox.com/request/8id0uEY4LKeC1G7psxtG,improved-smt-through-editing-figure-6.pdf,Joshua 3.0&colon; Syntax-based Machine Translation with the Thrax Grammar Extractor,http://cis.upenn.edu/~ccb/publications/joshua-3.0.pdf,joshua-3.0,Figure,1,https://www.dropbox.com/request/xsrnvy8MPk7nSeykN3Z6,joshua-3.0-figure-1.pdf,Joshua 3.0&colon; Syntax-based Machine Translation with the Thrax Grammar Extractor,http://cis.upenn.edu/~ccb/publications/joshua-3.0.pdf,joshua-3.0,Figure,2,https://www.dropbox.com/request/xsrnvy8MPk7nSeykN3Z6,joshua-3.0-figure-2.pdf,Joshua 3.0&colon; Syntax-based Machine Translation with the Thrax Grammar Extractor,http://cis.upenn.edu/~ccb/publications/joshua-3.0.pdf,joshua-3.0,Table,1,https://www.dropbox.com/request/xsrnvy8MPk7nSeykN3Z6,joshua-3.0-table-1.pdf,Joshua 3.0&colon; Syntax-based Machine Translation with the Thrax Grammar Extractor,http://cis.upenn.edu/~ccb/publications/joshua-3.0.pdf,joshua-3.0,Table,2,https://www.dropbox.com/request/xsrnvy8MPk7nSeykN3Z6,joshua-3.0-table-2.pdf,A software tool which allows an advanced user to correct misaligned sentence pairs from the training data,An aligned sentence pair.,An SAMT derivation. The shaded terminal symbols are the lexicalized part of a rule with terminals and non-terminals. The unshaded terminals are directly dominated by a nonterminal symbol.,A subset of the Hiero and SAMT rules extracted from the sentence pair of Figure 1.,Training data size after subsampling.
5,Joshua 3.0&colon; Syntax-based Machine Translation with the Thrax Grammar Extractor,http://cis.upenn.edu/~ccb/publications/joshua-3.0.pdf,joshua-3.0,Table,3,https://www.dropbox.com/request/xsrnvy8MPk7nSeykN3Z6,joshua-3.0-table-3.pdf,Answer Extraction as Sequence Tagging with Tree Edit Distance,http://cis.upenn.edu/~ccb/publications/answer-extraction-as-sequence-tagging.pdf,answer-extraction-as-sequence-tagging,Figure,1,https://www.dropbox.com/request/LHQv7fa12JRDEzPn2dpL,answer-extraction-as-sequence-tagging-figure-1.pdf,Answer Extraction as Sequence Tagging with Tree Edit Distance,http://cis.upenn.edu/~ccb/publications/answer-extraction-as-sequence-tagging.pdf,answer-extraction-as-sequence-tagging,Figure,2,https://www.dropbox.com/request/LHQv7fa12JRDEzPn2dpL,answer-extraction-as-sequence-tagging-figure-2.pdf,Answer Extraction as Sequence Tagging with Tree Edit Distance,http://cis.upenn.edu/~ccb/publications/answer-extraction-as-sequence-tagging.pdf,answer-extraction-as-sequence-tagging,Figure,3,https://www.dropbox.com/request/LHQv7fa12JRDEzPn2dpL,answer-extraction-as-sequence-tagging-figure-3.pdf,Answer Extraction as Sequence Tagging with Tree Edit Distance,http://cis.upenn.edu/~ccb/publications/answer-extraction-as-sequence-tagging.pdf,answer-extraction-as-sequence-tagging,Figure,4,https://www.dropbox.com/request/LHQv7fa12JRDEzPn2dpL,answer-extraction-as-sequence-tagging-figure-4.pdf,Single-reference BLEU-4 scores.,"Edits transforming a source sentence (left) to a question (right). Each node consists of: lemma, POS tag and dependency relation, with root nodes and punctuation not shown. Shown includes deletion (⇥ and strikethrough on the left), alignment (arrows) and insertion (shaded area). Order of operations is not displayed. The standard TED model does not capture the alignment between tennis and sport (see Section 2.2).",An example of linear-chain CRF for an- swer sequence tagging.,"A sample sequence tagging output that fails to predict an answer. From line 2 on, the first column is the reference output and the second column is the model output with the marginal probability for predicated labels. Note that World War II has much lower probabilities as an O than others.","Impact of adding features based on chunking and question-type (CHUNKING) and tree edits (TED), e.g., EDIT and ALIGN."
5,Answer Extraction as Sequence Tagging with Tree Edit Distance,http://cis.upenn.edu/~ccb/publications/answer-extraction-as-sequence-tagging.pdf,answer-extraction-as-sequence-tagging,Figure,5,https://www.dropbox.com/request/LHQv7fa12JRDEzPn2dpL,answer-extraction-as-sequence-tagging-figure-5.pdf,Answer Extraction as Sequence Tagging with Tree Edit Distance,http://cis.upenn.edu/~ccb/publications/answer-extraction-as-sequence-tagging.pdf,answer-extraction-as-sequence-tagging,Figure,6,https://www.dropbox.com/request/LHQv7fa12JRDEzPn2dpL,answer-extraction-as-sequence-tagging-figure-6.pdf,Answer Extraction as Sequence Tagging with Tree Edit Distance,http://cis.upenn.edu/~ccb/publications/answer-extraction-as-sequence-tagging.pdf,answer-extraction-as-sequence-tagging,Table,3,https://www.dropbox.com/request/LHQv7fa12JRDEzPn2dpL,answer-extraction-as-sequence-tagging-table-3.pdf,Cheap Facts and Counter-Facts,http://cis.upenn.edu/~ccb/publications/cheap-facts-and-counter-facts.pdf,cheap-facts-and-counter-facts,Table,1,https://www.dropbox.com/request/aKQ87azpmfh9XPxAWXFj,cheap-facts-and-counter-facts-table-1.pdf,Cheap Facts and Counter-Facts,http://cis.upenn.edu/~ccb/publications/cheap-facts-and-counter-facts.pdf,cheap-facts-and-counter-facts,Table,2,https://www.dropbox.com/request/aKQ87azpmfh9XPxAWXFj,cheap-facts-and-counter-facts-table-2.pdf,{},{},Results on the QA Sentence Ranking task.,The statistics of the (valid) data we collect. The Total column presents the number of extracted NEs and generated hypotheses and the Average column shows the average numbers per text respectively.,"The comparison between the generated (counter-)facts and the original hypotheses from the RTE dataset. The Ave. Length column represents the average number of words in each hypothesis; The Ave. BoW shows the average bag-of-words similarity compared with the text. The three columns on the right are all about the position of the NE appearing in the sentence, how likely it is at the head, middle, or tail of the sentence."
5,Cheap Facts and Counter-Facts,http://cis.upenn.edu/~ccb/publications/cheap-facts-and-counter-facts.pdf,cheap-facts-and-counter-facts,Table,3,https://www.dropbox.com/request/aKQ87azpmfh9XPxAWXFj,cheap-facts-and-counter-facts-table-3.pdf,Cheap Facts and Counter-Facts,http://cis.upenn.edu/~ccb/publications/cheap-facts-and-counter-facts.pdf,cheap-facts-and-counter-facts,Table,4,https://www.dropbox.com/request/aKQ87azpmfh9XPxAWXFj,cheap-facts-and-counter-facts-table-4.pdf,Cheap Facts and Counter-Facts,http://cis.upenn.edu/~ccb/publications/cheap-facts-and-counter-facts.pdf,cheap-facts-and-counter-facts,Table,5,https://www.dropbox.com/request/aKQ87azpmfh9XPxAWXFj,cheap-facts-and-counter-facts-table-5.pdf,Incremental Syntactic Language Models for Phrase-based Translation,http://cis.upenn.edu/~ccb/publications/incremental-syntactic-language-models-for-phrase-based-translation.pdf,incremental-syntactic-language-models-for-phrase-based-translation,Figure,1,https://www.dropbox.com/request/Oqtuiwy9F3QJzQwo2ijY,incremental-syntactic-language-models-for-phrase-based-translation-figure-1.pdf,Incremental Syntactic Language Models for Phrase-based Translation,http://cis.upenn.edu/~ccb/publications/incremental-syntactic-language-models-for-phrase-based-translation.pdf,incremental-syntactic-language-models-for-phrase-based-translation,Figure,6,https://www.dropbox.com/request/Oqtuiwy9F3QJzQwo2ijY,incremental-syntactic-language-models-for-phrase-based-translation-figure-6.pdf,"The comparison of the generated (counter-)facts with the original hypotheses. The Valid column shows the percentage of the valid (counter-)facts; and other columns present the distribution of harder, easier cases than the original hypotheses or with the same difficulty.","Examples of facts and counter-facts, compared with the original texts and hypotheses. We ask the Turkers to write several (counter-)facts about the highlighted NEs, and only part of the results are shown here.","The results of baseline RTE systems on the data we collected, compared with the original RTE-5 dataset. The Counter-/Facts column shows the number of T-H pairs; and the other scores in percentage are accuracy of the systems.","Partial decoding lattice for standard phrase-based decoding stack algorithm translating the German sentence Der Pra ̈sident trifft am Freitag den Vorstand. Each node h in decoding stack t represents the application of a translation option, and includes the source sentence coverage vector, target language n-gram state, and syntactic language model state τ ̃th. Hypothesis combination is also shown, indicating where lattice paths with identical n-gram histories converge. We use the English translation The president meets the board on Friday as a running example throughout all Figures.","A hypothesis in the phrase-based decoding lattice from Figure 1 is expanded using translation option the board of source phrase den Vorstand. Syntactic language model state τ ̃31 contains random variables s1..3; likewise τ ̃51 contains s1..3. The intervening random variables r1..3, s1..3, and r1..3 are calculated by transition function δ (Eq. 6, as defined by §4.1), but are not stored. Observed random variables (e3..e5) are shown for clarity, but are not explicitly stored in any syntactic language model state."
5,Incremental Syntactic Language Models for Phrase-based Translation,http://cis.upenn.edu/~ccb/publications/incremental-syntactic-language-models-for-phrase-based-translation.pdf,incremental-syntactic-language-models-for-phrase-based-translation,Figure,7,https://www.dropbox.com/request/Oqtuiwy9F3QJzQwo2ijY,incremental-syntactic-language-models-for-phrase-based-translation-figure-7.pdf,Incremental Syntactic Language Models for Phrase-based Translation,http://cis.upenn.edu/~ccb/publications/incremental-syntactic-language-models-for-phrase-based-translation.pdf,incremental-syntactic-language-models-for-phrase-based-translation,Figure,8,https://www.dropbox.com/request/Oqtuiwy9F3QJzQwo2ijY,incremental-syntactic-language-models-for-phrase-based-translation-figure-8.pdf,Incremental Syntactic Language Models for Phrase-based Translation,http://cis.upenn.edu/~ccb/publications/incremental-syntactic-language-models-for-phrase-based-translation.pdf,incremental-syntactic-language-models-for-phrase-based-translation,Figure,9,https://www.dropbox.com/request/Oqtuiwy9F3QJzQwo2ijY,incremental-syntactic-language-models-for-phrase-based-translation-figure-9.pdf,Linear B System Description for the 2005 NIST MT Evaluation Exercise,http://cis.upenn.edu/~ccb/publications/linear-b-system-description-for-nist-mt-eval-2005.pdf,linear-b-system-description-for-nist-mt-eval-2005,Figure,1,https://www.dropbox.com/request/FJt8NzXGn2CaV9gyi0M2,linear-b-system-description-for-nist-mt-eval-2005-figure-1.pdf,Semantically Informed Machine Translation (SIMT),http://cis.upenn.edu/~ccb/publications/scale-2009-report.pdf,scale-2009-report,Figure,3,https://www.dropbox.com/request/8vETuiezk5lHkdlOXJwo,scale-2009-report-figure-3.pdf,Average per-word perplexity values. HHMM was run with beam size of 2000. Bold indicates best single-model results for LMs trained on WSJ sections 2-21. Best overall in italics.,Mean per-sentence decoding time (in seconds) for dev set using Moses with and without syntactic language model. HHMM parser beam sizes are indicated for the syntactic LM.,"Results for Ur-En devtest (only sentences with 1-20 words) with HHMM beam size of 2000 and Moses settings of distortion limit 10, stack size 200, and ttable limit 20.",In the simple editing condition subjects simply edited the output of a fully-automatic statistical machine translation system,{}
5,Semantically Informed Machine Translation (SIMT),http://cis.upenn.edu/~ccb/publications/scale-2009-report.pdf,scale-2009-report,Figure,4,https://www.dropbox.com/request/8vETuiezk5lHkdlOXJwo,scale-2009-report-figure-4.pdf,Semantically Informed Machine Translation (SIMT),http://cis.upenn.edu/~ccb/publications/scale-2009-report.pdf,scale-2009-report,Figure,5,https://www.dropbox.com/request/8vETuiezk5lHkdlOXJwo,scale-2009-report-figure-5.pdf,Semantically Informed Machine Translation (SIMT),http://cis.upenn.edu/~ccb/publications/scale-2009-report.pdf,scale-2009-report,Figure,6,https://www.dropbox.com/request/8vETuiezk5lHkdlOXJwo,scale-2009-report-figure-6.pdf,Semantically Informed Machine Translation (SIMT),http://cis.upenn.edu/~ccb/publications/scale-2009-report.pdf,scale-2009-report,Figure,7,https://www.dropbox.com/request/8vETuiezk5lHkdlOXJwo,scale-2009-report-figure-7.pdf,Semantically Informed Machine Translation (SIMT),http://cis.upenn.edu/~ccb/publications/scale-2009-report.pdf,scale-2009-report,Figure,8,https://www.dropbox.com/request/8vETuiezk5lHkdlOXJwo,scale-2009-report-figure-8.pdf,{},{},{},{},{}
5,Semantically Informed Machine Translation (SIMT),http://cis.upenn.edu/~ccb/publications/scale-2009-report.pdf,scale-2009-report,Figure,9,https://www.dropbox.com/request/8vETuiezk5lHkdlOXJwo,scale-2009-report-figure-9.pdf,Semantically Informed Machine Translation (SIMT),http://cis.upenn.edu/~ccb/publications/scale-2009-report.pdf,scale-2009-report,Figure,10,https://www.dropbox.com/request/8vETuiezk5lHkdlOXJwo,scale-2009-report-figure-10.pdf,Semantically Informed Machine Translation (SIMT),http://cis.upenn.edu/~ccb/publications/scale-2009-report.pdf,scale-2009-report,Figure,11,https://www.dropbox.com/request/8vETuiezk5lHkdlOXJwo,scale-2009-report-figure-11.pdf,Semantically Informed Machine Translation (SIMT),http://cis.upenn.edu/~ccb/publications/scale-2009-report.pdf,scale-2009-report,Table,1,https://www.dropbox.com/request/8vETuiezk5lHkdlOXJwo,scale-2009-report-table-1.pdf,Semantically Informed Machine Translation (SIMT),http://cis.upenn.edu/~ccb/publications/scale-2009-report.pdf,scale-2009-report,Table,2,https://www.dropbox.com/request/8vETuiezk5lHkdlOXJwo,scale-2009-report-table-2.pdf,{},"PhysicallocationrelationforIn the West Bank, a passenger was wounded.",{},{},{}
5,Semantically Informed Machine Translation (SIMT),http://cis.upenn.edu/~ccb/publications/scale-2009-report.pdf,scale-2009-report,Table,3,https://www.dropbox.com/request/8vETuiezk5lHkdlOXJwo,scale-2009-report-table-3.pdf,Semantically Informed Machine Translation (SIMT),http://cis.upenn.edu/~ccb/publications/scale-2009-report.pdf,scale-2009-report,Table,4,https://www.dropbox.com/request/8vETuiezk5lHkdlOXJwo,scale-2009-report-table-4.pdf,Semantically Informed Machine Translation (SIMT),http://cis.upenn.edu/~ccb/publications/scale-2009-report.pdf,scale-2009-report,Table,5,https://www.dropbox.com/request/8vETuiezk5lHkdlOXJwo,scale-2009-report-table-5.pdf,Semantically Informed Machine Translation (SIMT),http://cis.upenn.edu/~ccb/publications/scale-2009-report.pdf,scale-2009-report,Table,6,https://www.dropbox.com/request/8vETuiezk5lHkdlOXJwo,scale-2009-report-table-6.pdf,Semantically Informed Machine Translation (SIMT),http://cis.upenn.edu/~ccb/publications/scale-2009-report.pdf,scale-2009-report,Table,7,https://www.dropbox.com/request/8vETuiezk5lHkdlOXJwo,scale-2009-report-table-7.pdf,Example clusters derived from the Brown bigram mutual information clustering.,The BBC English and Urdu data.,{},Coverage of new Urdu phrases in the (relatively small) test set.,Examples of English-to-English transfer rules learned via pivoting.
5,Semantically Informed Machine Translation (SIMT),http://cis.upenn.edu/~ccb/publications/scale-2009-report.pdf,scale-2009-report,Table,8,https://www.dropbox.com/request/8vETuiezk5lHkdlOXJwo,scale-2009-report-table-8.pdf,Semantically Informed Machine Translation (SIMT),http://cis.upenn.edu/~ccb/publications/scale-2009-report.pdf,scale-2009-report,Table,9,https://www.dropbox.com/request/8vETuiezk5lHkdlOXJwo,scale-2009-report-table-9.pdf,Semantically Informed Machine Translation (SIMT),http://cis.upenn.edu/~ccb/publications/scale-2009-report.pdf,scale-2009-report,Table,10,https://www.dropbox.com/request/8vETuiezk5lHkdlOXJwo,scale-2009-report-table-10.pdf,Semantically Informed Machine Translation (SIMT),http://cis.upenn.edu/~ccb/publications/scale-2009-report.pdf,scale-2009-report,Table,11,https://www.dropbox.com/request/8vETuiezk5lHkdlOXJwo,scale-2009-report-table-11.pdf,SemEval-2015 Task 1&colon; Paraphrase and Semantic Similarity in Twitter,http://cis.upenn.edu/~ccb/publications/paraphrase-and-semantic-similarity-in-twitter.pdf,paraphrase-and-semantic-similarity-in-twitter,Table,2,https://www.dropbox.com/request/1AyE3987yJaLnc3cEba6,paraphrase-and-semantic-similarity-in-twitter-table-2.pdf,{},{},{},{},Statistics of PIT-2015 Twitter Paraphrase Corpus. Debatable cases are those received a medium-score from annotators. The percentage of paraphrases is lower in the test set because it was constructed without topic selection.
5,SemEval-2015 Task 1&colon; Paraphrase and Semantic Similarity in Twitter,http://cis.upenn.edu/~ccb/publications/paraphrase-and-semantic-similarity-in-twitter.pdf,paraphrase-and-semantic-similarity-in-twitter,Table,3,https://www.dropbox.com/request/1AyE3987yJaLnc3cEba6,paraphrase-and-semantic-similarity-in-twitter-table-3.pdf,Transliterating From All Languages,http://cis.upenn.edu/~ccb/publications/transliterating-from-all-languages.pdf,transliterating-from-all-languages,Figure,1,https://www.dropbox.com/request/1ePGpz51aRZTAZoKxuJB,transliterating-from-all-languages-figure-1.pdf,Transliterating From All Languages,http://cis.upenn.edu/~ccb/publications/transliterating-from-all-languages.pdf,transliterating-from-all-languages,Figure,2,https://www.dropbox.com/request/1ePGpz51aRZTAZoKxuJB,transliterating-from-all-languages-figure-2.pdf,Transliterating From All Languages,http://cis.upenn.edu/~ccb/publications/transliterating-from-all-languages.pdf,transliterating-from-all-languages,Figure,3,https://www.dropbox.com/request/1ePGpz51aRZTAZoKxuJB,transliterating-from-all-languages-figure-3.pdf,Transliterating From All Languages,http://cis.upenn.edu/~ccb/publications/transliterating-from-all-languages.pdf,transliterating-from-all-languages,Table,1,https://www.dropbox.com/request/1ePGpz51aRZTAZoKxuJB,transliterating-from-all-languages-table-1.pdf,"Evaluation results. The first column presents the rank of each team in the two tasks based on each team’s best system. The superscripts are the ranks of systems, ordered by F1 for Paraphrase Identification (PI) task and Pearson for Semantic Similarity (SS) task. ⇧ indicates unsupervised or semi-supervised system. In total, 19 teams participated in the PI task, of which 14 teams also participated in the SS task. Note that although the two sub-tasks share the same test set of 972 sentence pairs, the PI task ignores 134 debatable cases (received a medium-score from expert annotator) and uses only 838 pairs (663 paraphrases and 175 non-paraphrases) in evaluation, while SS task uses all 972 pairs. This causes that the F1-score in the PI task can be higher than the maximum F1-score in the SS task. Also note that the F1-scores of the baselines in the PI task are higher than reported in the Table 2 of (Xu et al., 2014), because the later reported maximum F1-scores on the PI task, ignoring the debatable cases.","Number of training pairs vs. system performance as measured by average normalized edit distance from the reference. The normalized edit distance is the minimum number of insertions, deletions, and substitutions that must be made to transform one string into the other, normalized by the length of the reference string, and multiplied by 100.","Learning curves resulting from holding out some training pairs from the models. The normalized edit distance is the minimum number of insertions, deletions, and substitutions that must be made to transform one string into the other, normalized by the length of the reference string, and multiplied by 100.",The percent of perfect transliterations found in the n-best output vs. n in n-best.,"Examples of Russian to English and Greek to English transliteration rules learned by Joshua along with the following associated log probabilities: a character sequence mapping probability, a character substitution probability, and a character-based language model probability."
5,Transliterating From All Languages,http://cis.upenn.edu/~ccb/publications/transliterating-from-all-languages.pdf,transliterating-from-all-languages,Table,2,https://www.dropbox.com/request/1ePGpz51aRZTAZoKxuJB,transliterating-from-all-languages-table-2.pdf,Transliterating From All Languages,http://cis.upenn.edu/~ccb/publications/transliterating-from-all-languages.pdf,transliterating-from-all-languages,Table,3,https://www.dropbox.com/request/1ePGpz51aRZTAZoKxuJB,transliterating-from-all-languages-table-3.pdf,Transliterating From All Languages,http://cis.upenn.edu/~ccb/publications/transliterating-from-all-languages.pdf,transliterating-from-all-languages,Table,4,https://www.dropbox.com/request/1ePGpz51aRZTAZoKxuJB,transliterating-from-all-languages-table-4.pdf,Transliterating From All Languages,http://cis.upenn.edu/~ccb/publications/transliterating-from-all-languages.pdf,transliterating-from-all-languages,Table,5,https://www.dropbox.com/request/1ePGpz51aRZTAZoKxuJB,transliterating-from-all-languages-table-5.pdf,Transliterating From All Languages,http://cis.upenn.edu/~ccb/publications/transliterating-from-all-languages.pdf,transliterating-from-all-languages,Table,6,https://www.dropbox.com/request/1ePGpz51aRZTAZoKxuJB,transliterating-from-all-languages-table-6.pdf,"The 100 languages with the largest number of name pairs with English. The counts are for Wikipedia pages describing people that have a inter-language link with English, and whose title is not identical to the English page title.","Languages of interest and the number of harvested person names. There are many more English names than there are for other languages and, correspondingly, its overlap with other languages is relatively large. Consequently, the amount of training data for transliterating between English and other languages is greater than between any other pair of languages.",Examples of multi-word Russian-English name pairs that require word alignments and filtering.,A comparison of our performance against the systems submitted to the Russian and Hindi transliteration shared tasks at the 2009 Named Entities Workshop.,"Examples of candidate transliterations and their corresponding reference transliterations, and the edit distances and normalized edit distances between them. The normalized edit distance is the minimum number of insertions, deletions, and substitutions that must be made to transform one string into the other, normalized by the length of the reference string, and multiplied by 100."
5,Transliterating From All Languages,http://cis.upenn.edu/~ccb/publications/transliterating-from-all-languages.pdf,transliterating-from-all-languages,Table,7,https://www.dropbox.com/request/1ePGpz51aRZTAZoKxuJB,transliterating-from-all-languages-table-7.pdf,Transliterating From All Languages,http://cis.upenn.edu/~ccb/publications/transliterating-from-all-languages.pdf,transliterating-from-all-languages,Table,8,https://www.dropbox.com/request/1ePGpz51aRZTAZoKxuJB,transliterating-from-all-languages-table-8.pdf,Paraphrastic Sentence Compression with a Character-based Metric&colon; Tightening without Deletion,http://cis.upenn.edu/~ccb/publications/paraphrastic-sentence-compression.pdf,paraphrastic-sentence-compression,Figure,1,https://www.dropbox.com/request/K3jLazZrYIr1IXN5ThXT,paraphrastic-sentence-compression-figure-1.pdf,Paraphrastic Sentence Compression with a Character-based Metric&colon; Tightening without Deletion,http://cis.upenn.edu/~ccb/publications/paraphrastic-sentence-compression.pdf,paraphrastic-sentence-compression,Table,1,https://www.dropbox.com/request/K3jLazZrYIr1IXN5ThXT,paraphrastic-sentence-compression-table-1.pdf,Paraphrastic Sentence Compression with a Character-based Metric&colon; Tightening without Deletion,http://cis.upenn.edu/~ccb/publications/paraphrastic-sentence-compression.pdf,paraphrastic-sentence-compression,Table,2,https://www.dropbox.com/request/K3jLazZrYIr1IXN5ThXT,paraphrastic-sentence-compression-table-2.pdf,"The 10 most common errors. The reference is on the left, and hypothesis is on the right. E  indicates that the letter E is dropped from the hypothesis, and  I indicates I is inserted.","Examples of Russian to English transliteration output. The system produced the reference transliteration in the first three examples, and it produced a correct alternative English transliteration in the second three examples. It incorrectly transliterated the final three.",Using a bilingual parallel corpus to extract paraphrases.,Candidate paraphrases for study in detail with corresponding approximate cosine similarity (Monolingual) and translation model (Bilingual) scores.,Mean ratings of compressions using just deletion or substitution at different paraphrase thresholds (Cos). Deletion performed better in all settings.
5,Paraphrastic Sentence Compression with a Character-based Metric&colon; Tightening without Deletion,http://cis.upenn.edu/~ccb/publications/paraphrastic-sentence-compression.pdf,paraphrastic-sentence-compression,Table,3,https://www.dropbox.com/request/K3jLazZrYIr1IXN5ThXT,paraphrastic-sentence-compression-table-3.pdf,Monolingual Distributional Similarity for Text-to-Text Generation,http://cis.upenn.edu/~ccb/publications/monolingual-distributional-similarity-for-text-to-text-generation.pdf,monolingual-distributional-similarity-for-text-to-text-generation,Figure,1,https://www.dropbox.com/request/jBlcOE6FKK37jo1bGXi5,monolingual-distributional-similarity-for-text-to-text-generation-figure-1.pdf,Monolingual Distributional Similarity for Text-to-Text Generation,http://cis.upenn.edu/~ccb/publications/monolingual-distributional-similarity-for-text-to-text-generation.pdf,monolingual-distributional-similarity-for-text-to-text-generation,Figure,2,https://www.dropbox.com/request/jBlcOE6FKK37jo1bGXi5,monolingual-distributional-similarity-for-text-to-text-generation-figure-2.pdf,Monolingual Distributional Similarity for Text-to-Text Generation,http://cis.upenn.edu/~ccb/publications/monolingual-distributional-similarity-for-text-to-text-generation.pdf,monolingual-distributional-similarity-for-text-to-text-generation,Figure,3,https://www.dropbox.com/request/jBlcOE6FKK37jo1bGXi5,monolingual-distributional-similarity-for-text-to-text-generation-figure-3.pdf,Monolingual Distributional Similarity for Text-to-Text Generation,http://cis.upenn.edu/~ccb/publications/monolingual-distributional-similarity-for-text-to-text-generation.pdf,monolingual-distributional-similarity-for-text-to-text-generation,Figure,4,https://www.dropbox.com/request/jBlcOE6FKK37jo1bGXi5,monolingual-distributional-similarity-for-text-to-text-generation-figure-4.pdf,"Mean ratings of compressions generated by a substitution oracle, deletion only, deletion on the oracle compression, and the gold standard. Being able to choose the best paraphrases would enable our substitution model to outperform the deletion model.",Pivot-based paraphrase extraction for contiguous phrases. Two phrases translating to the same phrase in the foreign language are assumed to be paraphrases of one another.,"Extraction of syntactic paraphrases via the pivoting approach: We aggregate over different sur- face realizations, matching the lexicalized portions of the rule and generalizing over the nonterminals.","An example of a synchronous paraphrastic derivation, here a sentence compression. Shaded words are deleted in the indicated rule applications.",Scoring a rule by extracting and scoring contiguous phrases consistent with the alignment. The overall score of the rule is determined by averaging across all pairs of contiguous subphrases
5,Monolingual Distributional Similarity for Text-to-Text Generation,http://cis.upenn.edu/~ccb/publications/monolingual-distributional-similarity-for-text-to-text-generation.pdf,monolingual-distributional-similarity-for-text-to-text-generation,Figure,5,https://www.dropbox.com/request/jBlcOE6FKK37jo1bGXi5,monolingual-distributional-similarity-for-text-to-text-generation-figure-5.pdf,Monolingual Distributional Similarity for Text-to-Text Generation,http://cis.upenn.edu/~ccb/publications/monolingual-distributional-similarity-for-text-to-text-generation.pdf,monolingual-distributional-similarity-for-text-to-text-generation,Figure,6,https://www.dropbox.com/request/jBlcOE6FKK37jo1bGXi5,monolingual-distributional-similarity-for-text-to-text-generation-figure-6.pdf,Monolingual Distributional Similarity for Text-to-Text Generation,http://cis.upenn.edu/~ccb/publications/monolingual-distributional-similarity-for-text-to-text-generation.pdf,monolingual-distributional-similarity-for-text-to-text-generation,Figure,7,https://www.dropbox.com/request/jBlcOE6FKK37jo1bGXi5,monolingual-distributional-similarity-for-text-to-text-generation-figure-7.pdf,Monolingual Distributional Similarity for Text-to-Text Generation,http://cis.upenn.edu/~ccb/publications/monolingual-distributional-similarity-for-text-to-text-generation.pdf,monolingual-distributional-similarity-for-text-to-text-generation,Table,1,https://www.dropbox.com/request/jBlcOE6FKK37jo1bGXi5,monolingual-distributional-similarity-for-text-to-text-generation-table-1.pdf,Monolingual Distributional Similarity for Text-to-Text Generation,http://cis.upenn.edu/~ccb/publications/monolingual-distributional-similarity-for-text-to-text-generation.pdf,monolingual-distributional-similarity-for-text-to-text-generation,Table,2,https://www.dropbox.com/request/jBlcOE6FKK37jo1bGXi5,monolingual-distributional-similarity-for-text-to-text-generation-table-2.pdf,"An example of the n-gram feature extraction on an n-gram corpus. Here, “the long-term” is seen preceded by “revise” (43 times) and followed by “plans” (97 times). The corresponding left- and right-side features are added to the phrase signature with the counts of the n-grams that gave rise to them.","An example of the syntactic feature-set. The phrase “the long-term” is annotated with position-aware lexical and part-of-speech n-gram features (e.g. “on to” on the left, and “investment” and “NN” to its right), labeled dependency links (e.g. amod − investment) and features derived from the phrase’s CCG label NP /NN .","A pairwise breakdown of the human judgments comparing the systems. Dark grey regions show the number of times the two systems were tied, and light grey shows how many times one system was judged to be better than the other.","Results of the human evaluation on longer compressions: pairwise compression rates (CR), meaning and grammaticality scores. Bold indicates a statistically significance difference at p < 0.05.",Example compressions produced by our systems and the baselines Table 1 for three input sentences from our test data.
5,Predicting Human-Targeted Translation Edit Rate via Untrained Human Annotators,http://cis.upenn.edu/~ccb/publications/predicting-HTER-from-untrained-annotators.pdf,predicting-HTER-from-untrained-annotators,Figure,1,https://www.dropbox.com/request/l5H5z9WXDxKjVFDucHt0,predicting-HTER-from-untrained-annotators-figure-1.pdf,Predicting Human-Targeted Translation Edit Rate via Untrained Human Annotators,http://cis.upenn.edu/~ccb/publications/predicting-HTER-from-untrained-annotators.pdf,predicting-HTER-from-untrained-annotators,Table,1,https://www.dropbox.com/request/l5H5z9WXDxKjVFDucHt0,predicting-HTER-from-untrained-annotators-table-1.pdf,A Lightweight and High Performance Monolingual Word Aligner,http://cis.upenn.edu/~ccb/publications/monolingual-word-aligner.pdf,monolingual-word-aligner,Table,1,https://www.dropbox.com/request/pY7pDo9kVL43bnOWhIdO,monolingual-word-aligner-table-1.pdf,A Lightweight and High Performance Monolingual Word Aligner,http://cis.upenn.edu/~ccb/publications/monolingual-word-aligner.pdf,monolingual-word-aligner,Table,2,https://www.dropbox.com/request/pY7pDo9kVL43bnOWhIdO,monolingual-word-aligner-table-2.pdf,A Lightweight and High Performance Monolingual Word Aligner,http://cis.upenn.edu/~ccb/publications/monolingual-word-aligner.pdf,monolingual-word-aligner,Table,3,https://www.dropbox.com/request/pY7pDo9kVL43bnOWhIdO,monolingual-word-aligner-table-3.pdf,"Rank correlation between predicted ranking and HTER ranking for different prediction schemes, across the four genres, and across various sizes of the worker verification set.",The 4 genres of the dataset.,"Results on the 800 pairs of test data. E% stands for exact (perfect) match rate. Systems marked with ⇤ are reported by MacCartney et al. (2008), with / by Thadani and McKeown (2011).","Alignment runtime in seconds per sentence pair on two corpora: RTE2 (Cohn et al., 2008) and FUSION (McKeown et al., 2010). The MANLI-* results are from Thadani and McKeown (2011), on a Xeon 2.0GHz with 6MB Cache. The runtime for this work takes the longest timing from S2T and T2S, on a Xeon 2.2GHz with 4MB cache (the closest we can find to match their hardware). Horizontally in a real- world application where sentences have similar length, this work is roughly 20x faster (0.096 vs. 2.45). Vertically, the decoding time for our work increases less dramatically when sentence length increases (0.025!0.096 vs. 0.08!2.45).",Performance without POS and/or Word-Net features.
5,Extracting Lexically Divergent Paraphrases from Twitter,http://cis.upenn.edu/~ccb/publications/extracting-paraphrases-from-twitter.pdf,extracting-paraphrases-from-twitter,Figure,1,https://www.dropbox.com/request/I6UkCNxBIn0jIP13S3aK,extracting-paraphrases-from-twitter-figure-1.pdf,Extracting Lexically Divergent Paraphrases from Twitter,http://cis.upenn.edu/~ccb/publications/extracting-paraphrases-from-twitter.pdf,extracting-paraphrases-from-twitter,Figure,2,https://www.dropbox.com/request/I6UkCNxBIn0jIP13S3aK,extracting-paraphrases-from-twitter-figure-2.pdf,Extracting Lexically Divergent Paraphrases from Twitter,http://cis.upenn.edu/~ccb/publications/extracting-paraphrases-from-twitter.pdf,extracting-paraphrases-from-twitter,Figure,3,https://www.dropbox.com/request/I6UkCNxBIn0jIP13S3aK,extracting-paraphrases-from-twitter-figure-3.pdf,Extracting Lexically Divergent Paraphrases from Twitter,http://cis.upenn.edu/~ccb/publications/extracting-paraphrases-from-twitter.pdf,extracting-paraphrases-from-twitter,Figure,4,https://www.dropbox.com/request/I6UkCNxBIn0jIP13S3aK,extracting-paraphrases-from-twitter-figure-4.pdf,Extracting Lexically Divergent Paraphrases from Twitter,http://cis.upenn.edu/~ccb/publications/extracting-paraphrases-from-twitter.pdf,extracting-paraphrases-from-twitter,Table,3,https://www.dropbox.com/request/I6UkCNxBIn0jIP13S3aK,extracting-paraphrases-from-twitter-table-3.pdf,"(a) a plate representation of the MULTIP model (b) an example instantiation of MULTIP for the pair of sentences “Manti bout to be the next Junior Seau” and “Teo is the little new Junior Seau”, in which a new American football player Manti Te’o was being compared to a famous former player Junior Seau.
Only 4 out of the total 6 × 5 word pairs, z1 - z30, are shown here.",MULTIP Learning Algorithm,"Precision and recall curves. Our MULTIP model alone achieves competitive performance with the LEXLATENT system that combines latent space model and feature-based supervised classifier. The two approaches have complementary strengths, and achieves significant improvement when combined together (MULTIP-PE).","A heat-map showing overlap between expert and crowdsourcing annotation. The intensity along the diagonal indicates good reliability of crowdsourcing workers for this particular task; and the shift above the diagonal reflects the difference between the two annotation schemas. For crowd- sourcing (turk), the numbers indicate how many annotators out of 5 picked the sentence pair as para- phrases; 0,1 are considered non-paraphrases; 3,4,5 are paraphrases. For expert annotation, all 0,1,2 are non-paraphrases; 4,5 are paraphrases. Medium- scored cases are discarded in training and testing in our experiments.",Feature ablation by removing each individual feature group from the full set.
5,Extracting Lexically Divergent Paraphrases from Twitter,http://cis.upenn.edu/~ccb/publications/extracting-paraphrases-from-twitter.pdf,extracting-paraphrases-from-twitter,Table,4,https://www.dropbox.com/request/I6UkCNxBIn0jIP13S3aK,extracting-paraphrases-from-twitter-table-4.pdf,Hierarchical Phrase-Based Grammar Extraction in Joshua&colon; Suffix Arrays and Prefix Trees,http://cis.upenn.edu/~ccb/publications/hiero-grammar-extraction-with-suffix-arrays.pdf,hiero-grammar-extraction-with-suffix-arrays,Figure,1,https://www.dropbox.com/request/bRg8D1SnRnDxjMG6R5FN,hiero-grammar-extraction-with-suffix-arrays-figure-1.pdf,Hierarchical Phrase-Based Grammar Extraction in Joshua&colon; Suffix Arrays and Prefix Trees,http://cis.upenn.edu/~ccb/publications/hiero-grammar-extraction-with-suffix-arrays.pdf,hiero-grammar-extraction-with-suffix-arrays,Figure,2,https://www.dropbox.com/request/bRg8D1SnRnDxjMG6R5FN,hiero-grammar-extraction-with-suffix-arrays-figure-2.pdf,Joshua 5.0&colon; Sparser better faster server,http://cis.upenn.edu/~ccb/publications/joshua-5.0.pdf,joshua-5.0,Figure,1,https://www.dropbox.com/request/LkgceRLCE2gewNBvdF0u,joshua-5.0-figure-1.pdf,Supervised Bilingual Lexicon Induction with Multiple Monolingual Signals,http://cis.upenn.edu/~ccb/publications/supervised-bilingual-lexicon-induction.pdf,supervised-bilingual-lexicon-induction,Figure,2,https://www.dropbox.com/request/oJs9sbAa6uUYmj49qlYg,supervised-bilingual-lexicon-induction-figure-2.pdf,Example system outputs; rank is the position in the list of all candidate paraphrase pairs in the test set ordered by model score. MULTIP discovers lexically divergent paraphrases while LEXLATENT prefers more overall sentence similarity. Underline marks the word pair(s) with highest estimated probability as paraphrastic anchor(s) for each sentence pair.,"During suffix array creation, the contents of a corpus array are sorted using the element comparison function Compare_Elements","Query intersection algorithm implemented in Joshua. This algorithm is adapted from a corrected version (Lopez, p.c.) of query intersection (Lopez, 2008).",End-to-end runtime as a function of the number of threads. Each data point is the minimum of at least fifteen different runs.,Performance on the development set goes up as features are greedily added to the feature space. Mean performance is slightly higher using this subset of six features (second to last bar) than using all features (last bar).
5,Supervised Bilingual Lexicon Induction with Multiple Monolingual Signals,http://cis.upenn.edu/~ccb/publications/supervised-bilingual-lexicon-induction.pdf,supervised-bilingual-lexicon-induction,Figure,3,https://www.dropbox.com/request/oJs9sbAa6uUYmj49qlYg,supervised-bilingual-lexicon-induction-figure-3.pdf,Supervised Bilingual Lexicon Induction with Multiple Monolingual Signals,http://cis.upenn.edu/~ccb/publications/supervised-bilingual-lexicon-induction.pdf,supervised-bilingual-lexicon-induction,Figure,4,https://www.dropbox.com/request/oJs9sbAa6uUYmj49qlYg,supervised-bilingual-lexicon-induction-figure-4.pdf,Supervised Bilingual Lexicon Induction with Multiple Monolingual Signals,http://cis.upenn.edu/~ccb/publications/supervised-bilingual-lexicon-induction.pdf,supervised-bilingual-lexicon-induction,Table,1,https://www.dropbox.com/request/oJs9sbAa6uUYmj49qlYg,supervised-bilingual-lexicon-induction-table-1.pdf,Supervised Bilingual Lexicon Induction with Multiple Monolingual Signals,http://cis.upenn.edu/~ccb/publications/supervised-bilingual-lexicon-induction.pdf,supervised-bilingual-lexicon-induction,Table,2,https://www.dropbox.com/request/oJs9sbAa6uUYmj49qlYg,supervised-bilingual-lexicon-induction-table-2.pdf,A Multi-Dialect Multi-Genre Corpus of Informal Written Arabic,http://cis.upenn.edu/~ccb/publications/arabic-dialect-corpus-2.pdf,arabic-dialect-corpus-2,Figure,1,https://www.dropbox.com/request/En36r1s7SUs7QSFYQyZb,arabic-dialect-corpus-2-figure-1.pdf,"Learning curves over number of positive training instances, up to 1250. For some languages, 1250 positive training instances are not available. In all cases, evaluation is on the development data and the number of negative training instances is three times the number of positive. For all languages, performance is fairly stable after about 300 positive training instances.",Millions of monolingual word tokens vs. Lexicon Induction Top-10 Accuracy,Millions of monolingual web crawl and Wikipedia word tokens,Top-10 Accuracy on test set. Performance increases for all languages moving from the baseline (MRR) to discriminative training (Supv).,Arabic Map
5,A Multi-Dialect Multi-Genre Corpus of Informal Written Arabic,http://cis.upenn.edu/~ccb/publications/arabic-dialect-corpus-2.pdf,arabic-dialect-corpus-2,Figure,2,https://www.dropbox.com/request/En36r1s7SUs7QSFYQyZb,arabic-dialect-corpus-2-figure-2.pdf,A Multi-Dialect Multi-Genre Corpus of Informal Written Arabic,http://cis.upenn.edu/~ccb/publications/arabic-dialect-corpus-2.pdf,arabic-dialect-corpus-2,Figure,3,https://www.dropbox.com/request/En36r1s7SUs7QSFYQyZb,arabic-dialect-corpus-2-figure-3.pdf,A Multi-Dialect Multi-Genre Corpus of Informal Written Arabic,http://cis.upenn.edu/~ccb/publications/arabic-dialect-corpus-2.pdf,arabic-dialect-corpus-2,Figure,4,https://www.dropbox.com/request/En36r1s7SUs7QSFYQyZb,arabic-dialect-corpus-2-figure-4.pdf,A Multi-Dialect Multi-Genre Corpus of Informal Written Arabic,http://cis.upenn.edu/~ccb/publications/arabic-dialect-corpus-2.pdf,arabic-dialect-corpus-2,Figure,5,https://www.dropbox.com/request/En36r1s7SUs7QSFYQyZb,arabic-dialect-corpus-2-figure-5.pdf,A Multi-Dialect Multi-Genre Corpus of Informal Written Arabic,http://cis.upenn.edu/~ccb/publications/arabic-dialect-corpus-2.pdf,arabic-dialect-corpus-2,Figure,7,https://www.dropbox.com/request/En36r1s7SUs7QSFYQyZb,arabic-dialect-corpus-2-figure-7.pdf,Screenshot of HIT,Newspaper commentary annotated on MTurk as having high dialectal content,Tweets annotated on MTurk as having high dialectal content,Workers’ Performance on Twitter HIT,Sample Commentary
5,Affinity Measures based on the Graph Laplacian,http://cis.upenn.edu/~ccb/publications/graph-laplacian-affinity-measures.pdf,graph-laplacian-affinity-measures,Table,3,https://www.dropbox.com/request/Ig63UD4vPgYpLgPBfzK6,graph-laplacian-affinity-measures-table-3.pdf,Affinity Measures based on the Graph Laplacian,http://cis.upenn.edu/~ccb/publications/graph-laplacian-affinity-measures.pdf,graph-laplacian-affinity-measures,Table,4,https://www.dropbox.com/request/Ig63UD4vPgYpLgPBfzK6,graph-laplacian-affinity-measures-table-4.pdf,Affinity Measures based on the Graph Laplacian,http://cis.upenn.edu/~ccb/publications/graph-laplacian-affinity-measures.pdf,graph-laplacian-affinity-measures,Table,5,https://www.dropbox.com/request/Ig63UD4vPgYpLgPBfzK6,graph-laplacian-affinity-measures-table-5.pdf,PARMA&colon; A Predicate Argument Aligner,http://cis.upenn.edu/~ccb/publications/parma.pdf,parma,Figure,1,https://www.dropbox.com/request/pNbJJs2paCgw2Xq3HQZu,parma-figure-1.pdf,PARMA&colon; A Predicate Argument Aligner,http://cis.upenn.edu/~ccb/publications/parma.pdf,parma,Figure,2,https://www.dropbox.com/request/pNbJJs2paCgw2Xq3HQZu,parma-figure-2.pdf,Similarity via pagerank (® = 0.1).,Similarity via inverse Laplacian.,Denoising graph Laplacian via SVD,"Example of gold-standard alignment pairs from Roth and Frank’s data set and our data set created from the LDC’s Multiple Translation Corpora. The RF data set exhibits high lexical overlap, where most of the alignments are between identical words like police-police and said-said. The LDC MTC was constructed to increase lexical diversity, balcony and tent-camp may relax this assumption.","We plotted the PARMA’s performance on each of the document pairs. Red squares show the F1 for individual document pairs drawn from Roth and Frank’s data set, and black circles show F1 for our Multiple Translation Corpora test set. The x-axis represents the cosine similarity between the document pairs. On the RF data set, performance is correlated with lexical similarity. On our more lexically diverse set, this is not the case. This could be due to the fact that some of the documents in the RF sets are minor re-writes of the same newswire story, making them easy to align."
5,PARMA&colon; A Predicate Argument Aligner,http://cis.upenn.edu/~ccb/publications/parma.pdf,parma,Table,1,https://www.dropbox.com/request/pNbJJs2paCgw2Xq3HQZu,parma-table-1.pdf,The Language Demographics of  Amazon Mechanical Turk,http://cis.upenn.edu/~ccb/publications/language-demographics-of-mechanical-turk.pdf,language-demographics-of-mechanical-turk,Figure,1,https://www.dropbox.com/request/wOntFfxuCaoP5UfQ2HXp,language-demographics-of-mechanical-turk-figure-1.pdf,The Language Demographics of  Amazon Mechanical Turk,http://cis.upenn.edu/~ccb/publications/language-demographics-of-mechanical-turk.pdf,language-demographics-of-mechanical-turk,Figure,2,https://www.dropbox.com/request/wOntFfxuCaoP5UfQ2HXp,language-demographics-of-mechanical-turk-figure-2.pdf,The Language Demographics of  Amazon Mechanical Turk,http://cis.upenn.edu/~ccb/publications/language-demographics-of-mechanical-turk.pdf,language-demographics-of-mechanical-turk,Figure,3,https://www.dropbox.com/request/wOntFfxuCaoP5UfQ2HXp,language-demographics-of-mechanical-turk-figure-3.pdf,The Language Demographics of  Amazon Mechanical Turk,http://cis.upenn.edu/~ccb/publications/language-demographics-of-mechanical-turk.pdf,language-demographics-of-mechanical-turk,Figure,4,https://www.dropbox.com/request/wOntFfxuCaoP5UfQ2HXp,language-demographics-of-mechanical-turk-figure-4.pdf,"PARMA outperforms the baseline lemma matching system on the three test sets, drawn from the Extended Event Coreference Bank, Roth and Frank’s data, and our set created from the Multiple Translation Corpora. PARMA achieves a higher F1 and recall score than Roth and Frank’s reported result.","The number of workers per country. This map was generated based on geolocating the IP address of 4,983 workers in our study. Omitted are 60 workers who were located in more than one country during the study, and 238 workers who could not be geolocated. The size of the circles represents the number of workers from each country. The two largest are India (1,998 workers) and the United States (866). To calibrate the sizes: the Philippines has 142 workers, Egypt has 25, Russia has 10, and Sri Lanka has 4.",Days to complete the translation HITs for 40 of the languages. Tick marks represent the completion of individual assignments.,"An example of the Turkers’ translations of a Hindi sentence. The translations are unedited and contain fixable spelling, capitalization and grammatical errors.","Translation quality for languages with at least 50 Turkers. The dark blue bars indicate the proportion of translations which exactly matched gold standard translations, and light blue indicate translations which were judged to be correct synonyms. Error bars show the 95% confidence intervals for each language."
5,The Language Demographics of  Amazon Mechanical Turk,http://cis.upenn.edu/~ccb/publications/language-demographics-of-mechanical-turk.pdf,language-demographics-of-mechanical-turk,Figure,5,https://www.dropbox.com/request/wOntFfxuCaoP5UfQ2HXp,language-demographics-of-mechanical-turk-figure-5.pdf,The Language Demographics of  Amazon Mechanical Turk,http://cis.upenn.edu/~ccb/publications/language-demographics-of-mechanical-turk.pdf,language-demographics-of-mechanical-turk,Figure,6,https://www.dropbox.com/request/wOntFfxuCaoP5UfQ2HXp,language-demographics-of-mechanical-turk-figure-6.pdf,The Language Demographics of  Amazon Mechanical Turk,http://cis.upenn.edu/~ccb/publications/language-demographics-of-mechanical-turk.pdf,language-demographics-of-mechanical-turk,Table,1,https://www.dropbox.com/request/wOntFfxuCaoP5UfQ2HXp,language-demographics-of-mechanical-turk-table-1.pdf,The Language Demographics of  Amazon Mechanical Turk,http://cis.upenn.edu/~ccb/publications/language-demographics-of-mechanical-turk.pdf,language-demographics-of-mechanical-turk,Table,2,https://www.dropbox.com/request/wOntFfxuCaoP5UfQ2HXp,language-demographics-of-mechanical-turk-table-2.pdf,The Language Demographics of  Amazon Mechanical Turk,http://cis.upenn.edu/~ccb/publications/language-demographics-of-mechanical-turk.pdf,language-demographics-of-mechanical-turk,Table,3,https://www.dropbox.com/request/wOntFfxuCaoP5UfQ2HXp,language-demographics-of-mechanical-turk-table-3.pdf,"(a) Individual workers’ overlap with Google Translate. We removed the 500 workers with the highest overlap (shaded region on the left) from our analyses, as it is reasonable to assume these workers are cheating by submitting translations from Google. Workers with no overlap (shaded region on the right) are also likely to be cheating, e.g. by submitting random text.
(b) Cumulative distribution of overlap with Google translate for workers and translations. We see that eliminating all workers with >70% overlap with google translate still preserves 90% of translations and >90% of workers.",The total volume of translations (measured in English words) as a function of elapsed days.,"Self-reported native language of 3,216 bilingual Turkers. Not shown are 49 languages with 20 speakers. We omit 1,801 Turkers who did not report their native language, 243 who reported 2 native languages, and 83 with","A list of the languages that were used in our study, grouped by the number of Wikipedia articles in the language. Each language’s code is given in parentheses. These language codes are used in other figures throughout this paper.","Translation quality when partitioning the translations into two groups, one containing translations submitted by Turkers whose location is within regions that plausibly speak the foreign language, and the other containing translations from Turkers outside those regions. In general, in-region Turkers provide higher quality translations. (**) indicates differences significant at p=0.05, (*) at p=0.10."
5,The Language Demographics of  Amazon Mechanical Turk,http://cis.upenn.edu/~ccb/publications/language-demographics-of-mechanical-turk.pdf,language-demographics-of-mechanical-turk,Table,4,https://www.dropbox.com/request/wOntFfxuCaoP5UfQ2HXp,language-demographics-of-mechanical-turk-table-4.pdf,The Language Demographics of  Amazon Mechanical Turk,http://cis.upenn.edu/~ccb/publications/language-demographics-of-mechanical-turk.pdf,language-demographics-of-mechanical-turk,Table,5,https://www.dropbox.com/request/wOntFfxuCaoP5UfQ2HXp,language-demographics-of-mechanical-turk-table-5.pdf,The Language Demographics of  Amazon Mechanical Turk,http://cis.upenn.edu/~ccb/publications/language-demographics-of-mechanical-turk.pdf,language-demographics-of-mechanical-turk,Table,6,https://www.dropbox.com/request/wOntFfxuCaoP5UfQ2HXp,language-demographics-of-mechanical-turk-table-6.pdf,Combining Bilingual and Comparable Corpora for Low Resource Machine Translation,http://cis.upenn.edu/~ccb/publications/combining-bilingual-and-comparable-corpora.pdf,combining-bilingual-and-comparable-corpora,Figure,1,https://www.dropbox.com/request/3yLbkiRUsF3WG7rzBN3V,combining-bilingual-and-comparable-corpora-figure-1.pdf,Combining Bilingual and Comparable Corpora for Low Resource Machine Translation,http://cis.upenn.edu/~ccb/publications/combining-bilingual-and-comparable-corpora.pdf,combining-bilingual-and-comparable-corpora,Figure,2,https://www.dropbox.com/request/3yLbkiRUsF3WG7rzBN3V,combining-bilingual-and-comparable-corpora-figure-2.pdf,Size of parallel corpora and bilingual dictionaries collected for each language.,"BLEU scores for translating into English using bilingual parallel corpora by themselves, and with the addition of single-word dictionaries. Scores are calculated using four reference translations and represent the mean of three MERT runs.","The green box shows the best languages to target on MTurk. These languages have many workers who generate high quality results quickly. We defined many workers as 50 or more active in-region workers, high quality as","Examples of OOV Bengali words, our top-3 ranked induced translations, and their correct translations.","Comparison of learning curves over lines of parallel training data for four SMT systems: our baseline phrase-based model (baseline), model that supplements the baseline with translations of OOV words induced using our supervised bilingual lexicon induction framework (+Trans), model that supplements the baseline with additional phrase table features estimated over comparable corpora (+Feats), and a system that supplements the baseline with both OOV translations and additional features (+Trans & Feats)."
5,Combining Bilingual and Comparable Corpora for Low Resource Machine Translation,http://cis.upenn.edu/~ccb/publications/combining-bilingual-and-comparable-corpora.pdf,combining-bilingual-and-comparable-corpora,Figure,3,https://www.dropbox.com/request/3yLbkiRUsF3WG7rzBN3V,combining-bilingual-and-comparable-corpora-figure-3.pdf,Combining Bilingual and Comparable Corpora for Low Resource Machine Translation,http://cis.upenn.edu/~ccb/publications/combining-bilingual-and-comparable-corpora.pdf,combining-bilingual-and-comparable-corpora,Table,1,https://www.dropbox.com/request/3yLbkiRUsF3WG7rzBN3V,combining-bilingual-and-comparable-corpora-table-1.pdf,Combining Bilingual and Comparable Corpora for Low Resource Machine Translation,http://cis.upenn.edu/~ccb/publications/combining-bilingual-and-comparable-corpora.pdf,combining-bilingual-and-comparable-corpora,Table,2,https://www.dropbox.com/request/3yLbkiRUsF3WG7rzBN3V,combining-bilingual-and-comparable-corpora-table-2.pdf,Combining Bilingual and Comparable Corpora for Low Resource Machine Translation,http://cis.upenn.edu/~ccb/publications/combining-bilingual-and-comparable-corpora.pdf,combining-bilingual-and-comparable-corpora,Table,3,https://www.dropbox.com/request/3yLbkiRUsF3WG7rzBN3V,combining-bilingual-and-comparable-corpora-table-3.pdf,Combining Bilingual and Comparable Corpora for Low Resource Machine Translation,http://cis.upenn.edu/~ccb/publications/combining-bilingual-and-comparable-corpora.pdf,combining-bilingual-and-comparable-corpora,Table,4,https://www.dropbox.com/request/3yLbkiRUsF3WG7rzBN3V,combining-bilingual-and-comparable-corpora-table-4.pdf,English to Urdu translation results using varying amounts of comparable corpora to estimate features and induce translations.,"Information about datasets released by Post et al. (2012): thousands of words in the source language parallel sentences and dictionaries, and percent of development set word types (unique word tokens) and word tokens that are OOV (do not appear in either section of the training data).","Millions of words of time-stamped web crawls and Wikipedia text, by language.",Percent of word types in a held out portion of the training data which are translated correctly by our bilingual lexicon induction technique. Evaluation is over the top-1 and top-10 outputs in the ranked lists for each source word.,"BLEU performance gains that target coverage (+OOV Trans.) and accuracy (+Features), and both (+Feats & OOV). OOV oracle uses OOV translations from automatic word alignments. Hiero and SAMT results are reported in Post et al. (2012)."
5,Combining Bilingual and Comparable Corpora for Low Resource Machine Translation,http://cis.upenn.edu/~ccb/publications/combining-bilingual-and-comparable-corpora.pdf,combining-bilingual-and-comparable-corpora,Table,5,https://www.dropbox.com/request/3yLbkiRUsF3WG7rzBN3V,combining-bilingual-and-comparable-corpora-table-5.pdf,Integrating Output from Specialized Modules in Machine Translation&colon; Transliteration in Joshua,http://cis.upenn.edu/~ccb/publications/integrating-output-from-specialized-modules-in-machine-translation.pdf,integrating-output-from-specialized-modules-in-machine-translation,Table,1,https://www.dropbox.com/request/dleEyclpm4RvPBwhCQff,integrating-output-from-specialized-modules-in-machine-translation-table-1.pdf,Integrating Output from Specialized Modules in Machine Translation&colon; Transliteration in Joshua,http://cis.upenn.edu/~ccb/publications/integrating-output-from-specialized-modules-in-machine-translation.pdf,integrating-output-from-specialized-modules-in-machine-translation,Table,2,https://www.dropbox.com/request/dleEyclpm4RvPBwhCQff,integrating-output-from-specialized-modules-in-machine-translation-table-2.pdf,Integrating Output from Specialized Modules in Machine Translation&colon; Transliteration in Joshua,http://cis.upenn.edu/~ccb/publications/integrating-output-from-specialized-modules-in-machine-translation.pdf,integrating-output-from-specialized-modules-in-machine-translation,Table,3,https://www.dropbox.com/request/dleEyclpm4RvPBwhCQff,integrating-output-from-specialized-modules-in-machine-translation-table-3.pdf,Using Categorial Grammar to Label Translation Rules,http://cis.upenn.edu/~ccb/publications/using-categorial-grammar-to-label-translation-rules.pdf,using-categorial-grammar-to-label-translation-rules,Figure,1,https://www.dropbox.com/request/Bgdaar9UC0LtGXG32EHR,using-categorial-grammar-to-label-translation-rules-figure-1.pdf,"Varying minimum training data frequency of source words for which new translations are induced and included in the phrase-based model. In all cases, the top-1 induced translation is added to the phrase table and features estimated over comparable corpora are included (i.e. +Feats & Trans model).",Impact of transliteration on BLEU in submissions to NIST MT09 evaluation.,Examples of improvements from transliteration.,Impact of transliteration. Note that the location name “Swabia” was incorrectly transliterated to “Cuba.” This example indicates the future room for improvement.,"An example CCG derivation for the sentence “They own properties in various cities and villages” using the lexicon from Table 1. Φ indicates a conjunction operation; > and < are forward and backward function application, respectively."
5,Using Categorial Grammar to Label Translation Rules,http://cis.upenn.edu/~ccb/publications/using-categorial-grammar-to-label-translation-rules.pdf,using-categorial-grammar-to-label-translation-rules,Table,2,https://www.dropbox.com/request/Bgdaar9UC0LtGXG32EHR,using-categorial-grammar-to-label-translation-rules-table-2.pdf,Using Categorial Grammar to Label Translation Rules,http://cis.upenn.edu/~ccb/publications/using-categorial-grammar-to-label-translation-rules.pdf,using-categorial-grammar-to-label-translation-rules,Table,3,https://www.dropbox.com/request/Bgdaar9UC0LtGXG32EHR,using-categorial-grammar-to-label-translation-rules-table-3.pdf,Semantically-Informed Syntactic Machine Translation&colon; A Tree-Grafting Approach,http://cis.upenn.edu/~ccb/publications/semantically-informed-syntactic-machine-translation.pdf,semantically-informed-syntactic-machine-translation,Figure,1,https://www.dropbox.com/request/xRmVR88B1h7GUwIA5Jhw,semantically-informed-syntactic-machine-translation-figure-1.pdf,Semantically-Informed Syntactic Machine Translation&colon; A Tree-Grafting Approach,http://cis.upenn.edu/~ccb/publications/semantically-informed-syntactic-machine-translation.pdf,semantically-informed-syntactic-machine-translation,Figure,2,https://www.dropbox.com/request/xRmVR88B1h7GUwIA5Jhw,semantically-informed-syntactic-machine-translation-figure-2.pdf,Semantically-Informed Syntactic Machine Translation&colon; A Tree-Grafting Approach,http://cis.upenn.edu/~ccb/publications/semantically-informed-syntactic-machine-translation.pdf,semantically-informed-syntactic-machine-translation,Figure,3,https://www.dropbox.com/request/xRmVR88B1h7GUwIA5Jhw,semantically-informed-syntactic-machine-translation-figure-3.pdf,Number of translation rules and non- terminal labels in an Urdu–English grammar under various models.,Results of translation experiments on Urdu–English. Higher BLEU scores are better. BLEU’s brevity penalty is reported in parentheses.,"An example of Urdu-English translation. Shown are an Urdu source document, a reference translation produced by a professional human translator, and machine translation output from a phrase-based model (Moses) without linguistic information, which is representative of state-of-the-art MT quality before the SIMT effort.","The evolution of a semantically informed approach to our synchronous context free grammars (SCFGs). At the start of summer the decoder used translation rules with a single generic non-terminal symbol, later syntactic categories were used, and by the end of the summer the translation rules included semantic elements such as named entities and modalities.","Workflow for producing semantically-grafted parse trees. The English side of the parallel corpus is automatically parsed, and also tagged with modality and named-entity markers. These tags are then grafted onto the syntactic parse trees. The relation finder was designed for additional tagging but was not implemented in the current work. (Future work will test relations as another component of meaning that may contribute toward im- proved MT ouput.)"
5,Semantically-Informed Syntactic Machine Translation&colon; A Tree-Grafting Approach,http://cis.upenn.edu/~ccb/publications/semantically-informed-syntactic-machine-translation.pdf,semantically-informed-syntactic-machine-translation,Figure,4,https://www.dropbox.com/request/xRmVR88B1h7GUwIA5Jhw,semantically-informed-syntactic-machine-translation-figure-4.pdf,Semantically-Informed Syntactic Machine Translation&colon; A Tree-Grafting Approach,http://cis.upenn.edu/~ccb/publications/semantically-informed-syntactic-machine-translation.pdf,semantically-informed-syntactic-machine-translation,Figure,5,https://www.dropbox.com/request/xRmVR88B1h7GUwIA5Jhw,semantically-informed-syntactic-machine-translation-figure-5.pdf,Semantically-Informed Syntactic Machine Translation&colon; A Tree-Grafting Approach,http://cis.upenn.edu/~ccb/publications/semantically-informed-syntactic-machine-translation.pdf,semantically-informed-syntactic-machine-translation,Figure,6,https://www.dropbox.com/request/xRmVR88B1h7GUwIA5Jhw,semantically-informed-syntactic-machine-translation-figure-6.pdf,Semantically-Informed Syntactic Machine Translation&colon; A Tree-Grafting Approach,http://cis.upenn.edu/~ccb/publications/semantically-informed-syntactic-machine-translation.pdf,semantically-informed-syntactic-machine-translation,Figure,7,https://www.dropbox.com/request/xRmVR88B1h7GUwIA5Jhw,semantically-informed-syntactic-machine-translation-figure-7.pdf,Semantically-Informed Syntactic Machine Translation&colon; A Tree-Grafting Approach,http://cis.upenn.edu/~ccb/publications/semantically-informed-syntactic-machine-translation.pdf,semantically-informed-syntactic-machine-translation,Table,1,https://www.dropbox.com/request/xRmVR88B1h7GUwIA5Jhw,semantically-informed-syntactic-machine-translation-table-1.pdf,"A sentence on the English side of the bilingual parallel training corpus is parsed with a syntactic parser, and also tagged with a named entity tagger. The tags are then grafted onto the syntactic parse tree to form new categories like NP-GPE and NP-weapon. Grafting happens prior to extracting translation rules, which happens normally except for the use of the augmented trees.",Example translation rules with named entity tags and modalities combined with syntactic categories.,"Results for a range of experiments conducted during the SIMT effort. Results show scores for base- line systems, which here include a phrase-based model (Moses) and a hierarchical phrase-based model (Hiero), neither of which make use of syntactic information. These also show the substantial improvements when syn- tax is introduced, along with different numbers of feature functions (FFs), and further improvements from semantic elements. The scores are lowercased Bleu calculated on the held-out devtest set.","An example of the improvements to Urdu-English translation before and after the SIMT effort. Output is from the baseline Hiero model, which does not use linguistic information, and from the final model, which incorporates syntactic and semantic information.","The size of the various data sets used for the experiments in this paper including the training, development (dev), incremental test set (devtest) and blind test set (test). The dev/devtest was a split of the NIST08 Urdu-English test set, and the blind test set was NIST09."
5,Toward Statistical Machine Translation without Parallel Corpora,http://cis.upenn.edu/~ccb/publications/toward-statistical-machine-translation-without-parallel-corpora.pdf,toward-statistical-machine-translation-without-parallel-corpora,Figure,5,https://www.dropbox.com/request/5QeuKHR0w2Od1QNFXzRv,toward-statistical-machine-translation-without-parallel-corpora-figure-5.pdf,Toward Statistical Machine Translation without Parallel Corpora,http://cis.upenn.edu/~ccb/publications/toward-statistical-machine-translation-without-parallel-corpora.pdf,toward-statistical-machine-translation-without-parallel-corpora,Figure,6,https://www.dropbox.com/request/5QeuKHR0w2Od1QNFXzRv,toward-statistical-machine-translation-without-parallel-corpora-figure-6.pdf,Toward Statistical Machine Translation without Parallel Corpora,http://cis.upenn.edu/~ccb/publications/toward-statistical-machine-translation-without-parallel-corpora.pdf,toward-statistical-machine-translation-without-parallel-corpora,Figure,7,https://www.dropbox.com/request/5QeuKHR0w2Od1QNFXzRv,toward-statistical-machine-translation-without-parallel-corpora-figure-7.pdf,Toward Statistical Machine Translation without Parallel Corpora,http://cis.upenn.edu/~ccb/publications/toward-statistical-machine-translation-without-parallel-corpora.pdf,toward-statistical-machine-translation-without-parallel-corpora,Figure,8,https://www.dropbox.com/request/5QeuKHR0w2Od1QNFXzRv,toward-statistical-machine-translation-without-parallel-corpora-figure-8.pdf,Toward Statistical Machine Translation without Parallel Corpora,http://cis.upenn.edu/~ccb/publications/toward-statistical-machine-translation-without-parallel-corpora.pdf,toward-statistical-machine-translation-without-parallel-corpora,Table,1,https://www.dropbox.com/request/5QeuKHR0w2Od1QNFXzRv,toward-statistical-machine-translation-without-parallel-corpora-table-1.pdf,Algorithm for estimating reordering probabilities from monolingual data.,"Collecting phrase orientation statistics for a English-German phrase pair (“profile”, “Profils”) from non-parallel sentences (the German sentence translates as “Creating a Facebook profile is easy”).","Much of the loss in BLEU score when bilingually estimated features are removed from a Spanish-English translation system (experiments 1-4) can be recovered when they are replaced with monolingual equivalents estimated from monolingual Europarl data (experiments 5-10). The labels indicate how the different types of parameters are estimated, the first part is for phrase-table features, the second is for reordering probabilities.",Performance of monolingual features derived from truly monolingual corpora. Over 82% of the BLEU score loss can be recovered.,Statistics about the monolingual training data and the phrase table that was used in all of the experiments.
5,Evaluating Question Answering Systems Using FAQ Answer Injection,http://cis.upenn.edu/~ccb/publications/evaluating-question-answering-systems-using-faq-answer-injection.pdf,evaluating-question-answering-systems-using-faq-answer-injection,Figure,1,https://www.dropbox.com/request/FkvLHsgbKV3GnByjYpm4,evaluating-question-answering-systems-using-faq-answer-injection-figure-1.pdf,Evaluating Question Answering Systems Using FAQ Answer Injection,http://cis.upenn.edu/~ccb/publications/evaluating-question-answering-systems-using-faq-answer-injection.pdf,evaluating-question-answering-systems-using-faq-answer-injection,Figure,2,https://www.dropbox.com/request/FkvLHsgbKV3GnByjYpm4,evaluating-question-answering-systems-using-faq-answer-injection-figure-2.pdf,WikiTopics&colon; What is Popular on Wikipedia and Why,http://cis.upenn.edu/~ccb/publications/wikitopics-what-is-popular-on-wikipedia-and-why.pdf,wikitopics-what-is-popular-on-wikipedia-and-why,Figure,1,https://www.dropbox.com/request/DmSE834FGg4Ig3feQWQr,wikitopics-what-is-popular-on-wikipedia-and-why-figure-1.pdf,WikiTopics&colon; What is Popular on Wikipedia and Why,http://cis.upenn.edu/~ccb/publications/wikitopics-what-is-popular-on-wikipedia-and-why.pdf,wikitopics-what-is-popular-on-wikipedia-and-why,Figure,2,https://www.dropbox.com/request/DmSE834FGg4Ig3feQWQr,wikitopics-what-is-popular-on-wikipedia-and-why-figure-2.pdf,WikiTopics&colon; What is Popular on Wikipedia and Why,http://cis.upenn.edu/~ccb/publications/wikitopics-what-is-popular-on-wikipedia-and-why.pdf,wikitopics-what-is-popular-on-wikipedia-and-why,Figure,3,https://www.dropbox.com/request/DmSE834FGg4Ig3feQWQr,wikitopics-what-is-popular-on-wikipedia-and-why-figure-3.pdf,FAQ Answer Injection.,Detection of Components Responsible for Recall Drop.,"Automatically selected articles for Jan 27, 2009.",Process diagram: (a) Topic selection: select interesting articles based on increase in pageviews. (b) Clustering: cluster the articles according to relevant events using topic models or Wikipedia’s hyperlink structure. (c) Textualization: select the sentence that best summarizes the relevant event,"Pageviews for all the hand-curated articles related to the inauguration of Barack Obama. Pageviews spike on the same day as the event took place–January 20, 2009."
5,WikiTopics&colon; What is Popular on Wikipedia and Why,http://cis.upenn.edu/~ccb/publications/wikitopics-what-is-popular-on-wikipedia-and-why.pdf,wikitopics-what-is-popular-on-wikipedia-and-why,Table,2,https://www.dropbox.com/request/DmSE834FGg4Ig3feQWQr,wikitopics-what-is-popular-on-wikipedia-and-why-table-2.pdf,The Multilingual Paraphrase Database,http://cis.upenn.edu/~ccb/publications/ppdb-multilingual.pdf,ppdb-multilingual,Figure,1,https://www.dropbox.com/request/BE0DXlny4XfxdCw3MiI7,ppdb-multilingual-figure-1.pdf,The Multilingual Paraphrase Database,http://cis.upenn.edu/~ccb/publications/ppdb-multilingual.pdf,ppdb-multilingual,Figure,2,https://www.dropbox.com/request/BE0DXlny4XfxdCw3MiI7,ppdb-multilingual-figure-2.pdf,The Multilingual Paraphrase Database,http://cis.upenn.edu/~ccb/publications/ppdb-multilingual.pdf,ppdb-multilingual,Figure,3,https://www.dropbox.com/request/BE0DXlny4XfxdCw3MiI7,ppdb-multilingual-figure-3.pdf,The Multilingual Paraphrase Database,http://cis.upenn.edu/~ccb/publications/ppdb-multilingual.pdf,ppdb-multilingual,Table,1,https://www.dropbox.com/request/BE0DXlny4XfxdCw3MiI7,ppdb-multilingual-table-1.pdf,"Textualization: evaluation results of sentence selection schemes. Self fallback scheme first tries to select the best sentence as the Self scheme, and if it fails to select one it falls back to the Recent scheme.",German paraphrases are extracted by pivoting over a shared English translation.,"In addition to extracting lexical and phrasal paraphrases, we also extract syntactic paraphrases. These have nonterminal symbols that act as slots that can be filled by other paraphrases that match that syntactic type. The syntactic labels are drawn from parse trees of the English sen- tences in our bitexts.","An overview of paraphrase collection size per language, measured in millions of paraphrase pairs.","An example paraphrase rule for German. The four fields are the left hand size nonterminal, the phrase, the paraphrase and the features associated with the rule."
5,The Multilingual Paraphrase Database,http://cis.upenn.edu/~ccb/publications/ppdb-multilingual.pdf,ppdb-multilingual,Table,2,https://www.dropbox.com/request/BE0DXlny4XfxdCw3MiI7,ppdb-multilingual-table-2.pdf,"Upping the Ante for ""Best of Breed"" Machine Translation Providers",http://cis.upenn.edu/~ccb/publications/upping-the-ante.pdf,upping-the-ante,Table,1,https://www.dropbox.com/request/Q7xHh0jUBxbYzo7F9Bd9,upping-the-ante-table-1.pdf,"Upping the Ante for ""Best of Breed"" Machine Translation Providers",http://cis.upenn.edu/~ccb/publications/upping-the-ante.pdf,upping-the-ante,Table,2,https://www.dropbox.com/request/Q7xHh0jUBxbYzo7F9Bd9,upping-the-ante-table-2.pdf,"Upping the Ante for ""Best of Breed"" Machine Translation Providers",http://cis.upenn.edu/~ccb/publications/upping-the-ante.pdf,upping-the-ante,Table,3,https://www.dropbox.com/request/Q7xHh0jUBxbYzo7F9Bd9,upping-the-ante-table-3.pdf,"Upping the Ante for ""Best of Breed"" Machine Translation Providers",http://cis.upenn.edu/~ccb/publications/upping-the-ante.pdf,upping-the-ante,Table,4,https://www.dropbox.com/request/Q7xHh0jUBxbYzo7F9Bd9,upping-the-ante-table-4.pdf,The sizes of the bilingual training data used to extract each language-specific version of PPDB.,Japanese to English Chat Sentences,French to English Web Page Sentences,French to English Chat Sentences,English to French Sentences (Chat and Web Page)
5,Learning to translate with products of novices&colon; a suite of open-ended challenge problems for teaching MT,http://cis.upenn.edu/~ccb/publications/teaching-machine-translation.pdf,teaching-machine-translation,Figure,1,https://www.dropbox.com/request/CC0qu5tterYRoBpUPVOT,teaching-machine-translation-figure-1.pdf,Learning to translate with products of novices&colon; a suite of open-ended challenge problems for teaching MT,http://cis.upenn.edu/~ccb/publications/teaching-machine-translation.pdf,teaching-machine-translation,Figure,2,https://www.dropbox.com/request/CC0qu5tterYRoBpUPVOT,teaching-machine-translation-figure-2.pdf,Learning to translate with products of novices&colon; a suite of open-ended challenge problems for teaching MT,http://cis.upenn.edu/~ccb/publications/teaching-machine-translation.pdf,teaching-machine-translation,Figure,3,https://www.dropbox.com/request/CC0qu5tterYRoBpUPVOT,teaching-machine-translation-figure-3.pdf,Learning to translate with products of novices&colon; a suite of open-ended challenge problems for teaching MT,http://cis.upenn.edu/~ccb/publications/teaching-machine-translation.pdf,teaching-machine-translation,Table,1,https://www.dropbox.com/request/CC0qu5tterYRoBpUPVOT,teaching-machine-translation-table-1.pdf,Are Two Heads are Better than One? Crowdsourced Translation via a Two-Step Collaboration between Translators and Editors,http://cis.upenn.edu/~ccb/publications/crowdsourced-translation-via-collaboration-between-translators-and-editors.pdf,crowdsourced-translation-via-collaboration-between-translators-and-editors,Figure,1,https://www.dropbox.com/request/DmaZDIIxRE5vWwZFZC2O,crowdsourced-translation-via-collaboration-between-translators-and-editors-figure-1.pdf,"Submission history for the alignment challenge. Dashed lines represent the default and baseline system performance. Each colored line represents a student, and each dot represents a submission. For clarity, we show only submissions that improved the student’s AER.",Submission history for the decoding challenge. The dotted green line represents the oracle over submissions.,Submission history for the evaluation challenge.,Response to student survey questions on a Likert scale from 1 (strongly disagree) to 5 (strongly agree).,"Relationship between editor aggressiveness and effectiveness. Each point represents an editor/translation pair. Aggressiveness (x-axis) is measured as the TER between the pre-edit and post-edit version of the translation, and effective- ness (y-axis) is measured as the average amount by which the editing reduces the translation’s TERgold. While many editors make only a few changes, those who make many changes can bring the translation substantially closer to professional quality."
5,Are Two Heads are Better than One? Crowdsourced Translation via a Two-Step Collaboration between Translators and Editors,http://cis.upenn.edu/~ccb/publications/crowdsourced-translation-via-collaboration-between-translators-and-editors.pdf,crowdsourced-translation-via-collaboration-between-translators-and-editors,Figure,2,https://www.dropbox.com/request/DmaZDIIxRE5vWwZFZC2O,crowdsourced-translation-via-collaboration-between-translators-and-editors-figure-2.pdf,Are Two Heads are Better than One? Crowdsourced Translation via a Two-Step Collaboration between Translators and Editors,http://cis.upenn.edu/~ccb/publications/crowdsourced-translation-via-collaboration-between-translators-and-editors.pdf,crowdsourced-translation-via-collaboration-between-translators-and-editors,Figure,3,https://www.dropbox.com/request/DmaZDIIxRE5vWwZFZC2O,crowdsourced-translation-via-collaboration-between-translators-and-editors-figure-3.pdf,Are Two Heads are Better than One? Crowdsourced Translation via a Two-Step Collaboration between Translators and Editors,http://cis.upenn.edu/~ccb/publications/crowdsourced-translation-via-collaboration-between-translators-and-editors.pdf,crowdsourced-translation-via-collaboration-between-translators-and-editors,Figure,4,https://www.dropbox.com/request/DmaZDIIxRE5vWwZFZC2O,crowdsourced-translation-via-collaboration-between-translators-and-editors-figure-4.pdf,Are Two Heads are Better than One? Crowdsourced Translation via a Two-Step Collaboration between Translators and Editors,http://cis.upenn.edu/~ccb/publications/crowdsourced-translation-via-collaboration-between-translators-and-editors.pdf,crowdsourced-translation-via-collaboration-between-translators-and-editors,Figure,5,https://www.dropbox.com/request/DmaZDIIxRE5vWwZFZC2O,crowdsourced-translation-via-collaboration-between-translators-and-editors-figure-5.pdf,Are Two Heads are Better than One? Crowdsourced Translation via a Two-Step Collaboration between Translators and Editors,http://cis.upenn.edu/~ccb/publications/crowdsourced-translation-via-collaboration-between-translators-and-editors.pdf,crowdsourced-translation-via-collaboration-between-translators-and-editors,Table,1,https://www.dropbox.com/request/DmaZDIIxRE5vWwZFZC2O,crowdsourced-translation-via-collaboration-between-translators-and-editors-table-1.pdf,"Effect of editing on translations of varying quality. Rows reflect bins of editors, with the worse editors (those whose changes result in increased TERgold) on the top and the most effective editors (those whose changes result in the largest reduction in TERgold) on the bottom. Bars reflect bins of translations, with the highest TERgold translations on the left, and the lowest on the right. We can see from the consistently negative","Three alternative translations (left) and the edited versions of each (right). Each edit on the right was produced by a different editor. Order reflects the TERgold of each translation, with the lowest TERgold on the top. Some translators receive low TERgold scores due to superficial errors, which can be easily improved through editing. In the above example, the middle-ranked translation (green) becomes the best translation after being revised by a good editor.","2-step collaborative crowdsourcing translation model based on graph ranking framework including three sub-networks. The undirected links between users denotes translation-editing collaboration. The undirected links between candidate translations indicate lexical similarity between candidates. A bipartite graph ties candidate and Turker networks together by authorship (to make the figure clearer, some linkage is omitted). A dashed circle indicates the group of candidate translations for a single source sentence to translate.",Effect of candidate-Turker coupling (,Different versions of translations.
5,Are Two Heads are Better than One? Crowdsourced Translation via a Two-Step Collaboration between Translators and Editors,http://cis.upenn.edu/~ccb/publications/crowdsourced-translation-via-collaboration-between-translators-and-editors.pdf,crowdsourced-translation-via-collaboration-between-translators-and-editors,Table,2,https://www.dropbox.com/request/DmaZDIIxRE5vWwZFZC2O,crowdsourced-translation-via-collaboration-between-translators-and-editors-table-2.pdf,Are Two Heads are Better than One? Crowdsourced Translation via a Two-Step Collaboration between Translators and Editors,http://cis.upenn.edu/~ccb/publications/crowdsourced-translation-via-collaboration-between-translators-and-editors.pdf,crowdsourced-translation-via-collaboration-between-translators-and-editors,Table,3,https://www.dropbox.com/request/DmaZDIIxRE5vWwZFZC2O,crowdsourced-translation-via-collaboration-between-translators-and-editors-table-3.pdf,Crowd-Workers&colon; Aggregating Information Across Turkers To Help Them Find Higher Paying Work,http://cis.upenn.edu/~ccb/publications/crowd-workers.pdf,crowd-workers,Figure,1,https://www.dropbox.com/request/TLDGBElIEx1fjsUNlpSr,crowd-workers-figure-1.pdf,Crowd-Workers&colon; Aggregating Information Across Turkers To Help Them Find Higher Paying Work,http://cis.upenn.edu/~ccb/publications/crowd-workers.pdf,crowd-workers,Figure,2,https://www.dropbox.com/request/TLDGBElIEx1fjsUNlpSr,crowd-workers-figure-2.pdf,Expectations of Word Sense in Parallel Corpora,http://cis.upenn.edu/~ccb/publications/expectations-of-word-sense-in-parallel-corpora.pdf,expectations-of-word-sense-in-parallel-corpora,Figure,1,https://www.dropbox.com/request/fplBJeTiKSyK5uXn6Yea,expectations-of-word-sense-in-parallel-corpora-figure-1.pdf,"Overall BLEU performance for all methods (with and without post-editing). The highlighted result indicates the best performance, which is based on both candidate sentences and Turker information.",Variations of all component settings.,"The crowd-workers.com web service allows users to sort HITs based on hourly rate. The hourly rate is estimated by tracking how long it took other workers to complete the task (using our browser extension), along with the reward amount (which MTurk makes available).",The hourly earnings of the 65 Turkers in our pilot study who completed more than 100 HITs using the Crowd-Workers browser plugin.,"French-English values, by number of senses."
5,Expectations of Word Sense in Parallel Corpora,http://cis.upenn.edu/~ccb/publications/expectations-of-word-sense-in-parallel-corpora.pdf,expectations-of-word-sense-in-parallel-corpora,Figure,2,https://www.dropbox.com/request/fplBJeTiKSyK5uXn6Yea,expectations-of-word-sense-in-parallel-corpora-figure-2.pdf,Expectations of Word Sense in Parallel Corpora,http://cis.upenn.edu/~ccb/publications/expectations-of-word-sense-in-parallel-corpora.pdf,expectations-of-word-sense-in-parallel-corpora,Table,1,https://www.dropbox.com/request/fplBJeTiKSyK5uXn6Yea,expectations-of-word-sense-in-parallel-corpora-table-1.pdf,Expectations of Word Sense in Parallel Corpora,http://cis.upenn.edu/~ccb/publications/expectations-of-word-sense-in-parallel-corpora.pdf,expectations-of-word-sense-in-parallel-corpora,Table,2,https://www.dropbox.com/request/fplBJeTiKSyK5uXn6Yea,expectations-of-word-sense-in-parallel-corpora-table-2.pdf,Hallucinating Phrase Translations for Low Resource MT,http://cis.upenn.edu/~ccb/publications/hallucinating-phrase-translations.pdf,hallucinating-phrase-translations,Figure,1,https://www.dropbox.com/request/eVQ5IzYTCoQBxSr48aht,hallucinating-phrase-translations-figure-1.pdf,Hallucinating Phrase Translations for Low Resource MT,http://cis.upenn.edu/~ccb/publications/hallucinating-phrase-translations.pdf,hallucinating-phrase-translations,Figure,2,https://www.dropbox.com/request/eVQ5IzYTCoQBxSr48aht,hallucinating-phrase-translations-figure-2.pdf,"French-English values, by number of senses.","MTurk result on testing Turker reliability. Krippendorff’s Alpha is used to measure agreement. ↵- Turker: how Turkers agree among themselves, ↵-maj.: how the majority agrees with true value, maj.-agr.: agreement between the majority vote and true value. ↵-maj. indicates the confidence level about the maj.-agr. value. Subscripts denote either 5 Turkers, or 3 randomly selected of the 5.","Statistics for words sampled from parallel corpora. Average #senses observed over all words: 2.6 (French-English), and 2.4 (Chinese-English). The sampled word keep has 18 senses in OntoNotes, with 5 observed.","Example of loosely composed translations for the Spanish input in A, la casa linda. In B, we remove the stop word la. Then, in C, we enumerate the cartesian product of all unigram translations in the bilingual dictionary and sort the words within each alphabetically. Finally, we look up each list of words in C in the inverted index, and corresponding target phrases are enumerated in D. The inverted index contains all phrasal combinations and permutations of the word lists in C which also appear monolingually with some frequency and with, optionally, any number of stop words.","Example output from motivating experiment: a comparison of the baseline and full oracle translations of Spanish no hab ́ıa nadie en los centros electorales, which translates correctly as there was nobody at the voting offices. The full oracle is augmented with translations composed from the seed model as well as induced unigram translations. The phrase was no one is composeable from hab ́ıa nadie given the seed model. In contrast, the phrase polling stations is composeable from centros electorales using induced translations. For each translation, the phrase segmentations used by the decoder are highlighted."
5,Hallucinating Phrase Translations for Low Resource MT,http://cis.upenn.edu/~ccb/publications/hallucinating-phrase-translations.pdf,hallucinating-phrase-translations,Figure,3,https://www.dropbox.com/request/eVQ5IzYTCoQBxSr48aht,hallucinating-phrase-translations-figure-3.pdf,Hallucinating Phrase Translations for Low Resource MT,http://cis.upenn.edu/~ccb/publications/hallucinating-phrase-translations.pdf,hallucinating-phrase-translations,Table,1,https://www.dropbox.com/request/eVQ5IzYTCoQBxSr48aht,hallucinating-phrase-translations-table-1.pdf,Hallucinating Phrase Translations for Low Resource MT,http://cis.upenn.edu/~ccb/publications/hallucinating-phrase-translations.pdf,hallucinating-phrase-translations,Table,2,https://www.dropbox.com/request/eVQ5IzYTCoQBxSr48aht,hallucinating-phrase-translations-table-2.pdf,Hallucinating Phrase Translations for Low Resource MT,http://cis.upenn.edu/~ccb/publications/hallucinating-phrase-translations.pdf,hallucinating-phrase-translations,Table,3,https://www.dropbox.com/request/eVQ5IzYTCoQBxSr48aht,hallucinating-phrase-translations-table-3.pdf,Hallucinating Phrase Translations for Low Resource MT,http://cis.upenn.edu/~ccb/publications/hallucinating-phrase-translations.pdf,hallucinating-phrase-translations,Table,4,https://www.dropbox.com/request/eVQ5IzYTCoQBxSr48aht,hallucinating-phrase-translations-table-4.pdf,Precision Recall curve with BLEU scores for the top-k scored hallucinated translations. k varies from 1 to 200. Baseline model performance is shown with a red triangle.,Motivating Experiment: BLEU results using the baseline SMT model and composeable oracle translations with and without induced unigram translations.,"Top five induced translations for several source
words. Correct translations are bolded. aceite translates as oil.",Top three compositional translations for several source phrases and their model scores. Correct translations are bolded.,"Experimental results. First, the baseline models are augmented with monolingual phrase table features and then also with the top-5 induced translations for all OOV unigrams. Then, we append the top-k hallucinated phrase translations to the third baseline models. BLEU scores are aver- aged over three tuning runs. We measure the statistical significance of each +Phrase Trans model in comparison with the highest performing (bolded) baseline for each language; * indicates statistical significance with p ă0.01."
5,Processing Informal Romanized Pakistani Text Messages,http://cis.upenn.edu/~ccb/publications/pakistani-SMS-corpus.pdf,pakistani-SMS-corpus,Table,2,https://www.dropbox.com/request/H2C99uCBzGd24WV0bvLN,pakistani-SMS-corpus-table-2.pdf,A Computer Model of a Grammar for English Questions,http://cis.upenn.edu/~ccb/publications/computer-model-of-a-grammar-for-english-questions.pdf,computer-model-of-a-grammar-for-english-questions,Figure,1,https://www.dropbox.com/request/nKm1uDTYDZdHfJhbbsSW,computer-model-of-a-grammar-for-english-questions-figure-1.pdf,A Computer Model of a Grammar for English Questions,http://cis.upenn.edu/~ccb/publications/computer-model-of-a-grammar-for-english-questions.pdf,computer-model-of-a-grammar-for-english-questions,Figure,2,https://www.dropbox.com/request/nKm1uDTYDZdHfJhbbsSW,computer-model-of-a-grammar-for-english-questions-figure-2.pdf,A Computer Model of a Grammar for English Questions,http://cis.upenn.edu/~ccb/publications/computer-model-of-a-grammar-for-english-questions.pdf,computer-model-of-a-grammar-for-english-questions,Figure,3,https://www.dropbox.com/request/nKm1uDTYDZdHfJhbbsSW,computer-model-of-a-grammar-for-english-questions-figure-3.pdf,A Computer Model of a Grammar for English Questions,http://cis.upenn.edu/~ccb/publications/computer-model-of-a-grammar-for-english-questions.pdf,computer-model-of-a-grammar-for-english-questions,Figure,4,https://www.dropbox.com/request/nKm1uDTYDZdHfJhbbsSW,computer-model-of-a-grammar-for-english-questions-figure-4.pdf,"Results on tokens in the test set, binned by training frequency or difference in character length with their reference. Length differences are number of characters in romanized token minus the number of characters in its deromanization. α = 0.5 for all.",{},{},{},{}
5,A Computer Model of a Grammar for English Questions,http://cis.upenn.edu/~ccb/publications/computer-model-of-a-grammar-for-english-questions.pdf,computer-model-of-a-grammar-for-english-questions,Figure,5,https://www.dropbox.com/request/nKm1uDTYDZdHfJhbbsSW,computer-model-of-a-grammar-for-english-questions-figure-5.pdf,A Computer Model of a Grammar for English Questions,http://cis.upenn.edu/~ccb/publications/computer-model-of-a-grammar-for-english-questions.pdf,computer-model-of-a-grammar-for-english-questions,Figure,6,https://www.dropbox.com/request/nKm1uDTYDZdHfJhbbsSW,computer-model-of-a-grammar-for-english-questions-figure-6.pdf,A Computer Model of a Grammar for English Questions,http://cis.upenn.edu/~ccb/publications/computer-model-of-a-grammar-for-english-questions.pdf,computer-model-of-a-grammar-for-english-questions,Figure,7,https://www.dropbox.com/request/nKm1uDTYDZdHfJhbbsSW,computer-model-of-a-grammar-for-english-questions-figure-7.pdf,A Computer Model of a Grammar for English Questions,http://cis.upenn.edu/~ccb/publications/computer-model-of-a-grammar-for-english-questions.pdf,computer-model-of-a-grammar-for-english-questions,Figure,8,https://www.dropbox.com/request/nKm1uDTYDZdHfJhbbsSW,computer-model-of-a-grammar-for-english-questions-figure-8.pdf,A Computer Model of a Grammar for English Questions,http://cis.upenn.edu/~ccb/publications/computer-model-of-a-grammar-for-english-questions.pdf,computer-model-of-a-grammar-for-english-questions,Figure,9,https://www.dropbox.com/request/nKm1uDTYDZdHfJhbbsSW,computer-model-of-a-grammar-for-english-questions-figure-9.pdf,{},{},{},{},{}
5,A Computer Model of a Grammar for English Questions,http://cis.upenn.edu/~ccb/publications/computer-model-of-a-grammar-for-english-questions.pdf,computer-model-of-a-grammar-for-english-questions,Figure,10,https://www.dropbox.com/request/nKm1uDTYDZdHfJhbbsSW,computer-model-of-a-grammar-for-english-questions-figure-10.pdf,A Computer Model of a Grammar for English Questions,http://cis.upenn.edu/~ccb/publications/computer-model-of-a-grammar-for-english-questions.pdf,computer-model-of-a-grammar-for-english-questions,Figure,11,https://www.dropbox.com/request/nKm1uDTYDZdHfJhbbsSW,computer-model-of-a-grammar-for-english-questions-figure-11.pdf,Crowdsourcing for Grammatical Error Correction,http://cis.upenn.edu/~ccb/publications/crowdsourcing-for-grammatical-error-correction.pdf,crowdsourcing-for-grammatical-error-correction,Figure,1,https://www.dropbox.com/request/IEonw1oEhvOcxULnDrU2,crowdsourcing-for-grammatical-error-correction-figure-1.pdf,Crowdsourcing for Grammatical Error Correction,http://cis.upenn.edu/~ccb/publications/crowdsourcing-for-grammatical-error-correction.pdf,crowdsourcing-for-grammatical-error-correction,Figure,2,https://www.dropbox.com/request/IEonw1oEhvOcxULnDrU2,crowdsourcing-for-grammatical-error-correction-figure-2.pdf,Crowdsourcing for Grammatical Error Correction,http://cis.upenn.edu/~ccb/publications/crowdsourcing-for-grammatical-error-correction.pdf,crowdsourcing-for-grammatical-error-correction,Figure,3,https://www.dropbox.com/request/IEonw1oEhvOcxULnDrU2,crowdsourcing-for-grammatical-error-correction-figure-3.pdf,{},{},"F1s (x100) of automated systems in CoNLL 2013 shared task (blue) and of Turkers (red). Turker performance is measured by taking the edits produced by the single highest-scoring Turker for each sentence. Turkers edited a subset of the training data, not the final test data.","F1 scores of each Turker vs. # of sentences corrected. Red lines show F1 scores of best CoNLL systems, black line is the average F1 of CoNLL systems. Omitted are 7 Turkers with >1000 sentences corrected. All had F1<0.15.","Data structure allows us to meausre agreement on a specific edit, even if final versions of the sentence vary considerable. Here, we are able to tell that the two workers agree that ’into’ should be changed to ’in’, even though they each perform the edit on a different version of the sentence."
5,Crowdsourcing for Grammatical Error Correction,http://cis.upenn.edu/~ccb/publications/crowdsourcing-for-grammatical-error-correction.pdf,crowdsourcing-for-grammatical-error-correction,Table,1,https://www.dropbox.com/request/IEonw1oEhvOcxULnDrU2,crowdsourcing-for-grammatical-error-correction-table-1.pdf,Crowdsourcing for Grammatical Error Correction,http://cis.upenn.edu/~ccb/publications/crowdsourcing-for-grammatical-error-correction.pdf,crowdsourcing-for-grammatical-error-correction,Table,2,https://www.dropbox.com/request/IEonw1oEhvOcxULnDrU2,crowdsourcing-for-grammatical-error-correction-table-2.pdf,Improved Speech-to-Text Translation with the Fisher and Callhome Spanish–English Speech Translation Corpus,http://cis.upenn.edu/~ccb/publications/improved-speech-to-speech-translation.pdf,improved-speech-to-speech-translation,Figure,1,https://www.dropbox.com/request/1LDzPYBTyTVeWtSV10fT,improved-speech-to-speech-translation-figure-1.pdf,Improved Speech-to-Text Translation with the Fisher and Callhome Spanish–English Speech Translation Corpus,http://cis.upenn.edu/~ccb/publications/improved-speech-to-speech-translation.pdf,improved-speech-to-speech-translation,Figure,2,https://www.dropbox.com/request/1LDzPYBTyTVeWtSV10fT,improved-speech-to-speech-translation-figure-2.pdf,Improved Speech-to-Text Translation with the Fisher and Callhome Spanish–English Speech Translation Corpus,http://cis.upenn.edu/~ccb/publications/improved-speech-to-speech-translation.pdf,improved-speech-to-speech-translation,Table,1,https://www.dropbox.com/request/1LDzPYBTyTVeWtSV10fT,improved-speech-to-speech-translation-table-1.pdf,Multiple ways of producing a correct sentence for the same input.,Edit distance may favor lazy workers over workers who make a concientious effort.,"A subgraph of a lattice (sentence 17 of Fisher/Dev2) representing an ASR ambiguity. The oracle path is in bold. With access to the lattice, the MT system avoids the untranslatable word incorporamos, found in the 1-best output, producing a better translation. Above the line are inputs and the reference, with the Lattice line denoting the path selected by the MT system. The Google line is suggestive of the general difficulty in translating conversational speech.","Conversation-level WER and BLEU, for conversations found in Fisher/Dev (open points) and Fisher/Dev2 (solid points). The Pearson’s correlation coefficient is -0.72.",Corpus size and cost. Counts of segments and words were computed after pre-processing (§2).
5,Improved Speech-to-Text Translation with the Fisher and Callhome Spanish–English Speech Translation Corpus,http://cis.upenn.edu/~ccb/publications/improved-speech-to-speech-translation.pdf,improved-speech-to-speech-translation,Table,2,https://www.dropbox.com/request/1LDzPYBTyTVeWtSV10fT,improved-speech-to-speech-translation-table-2.pdf,Improved Speech-to-Text Translation with the Fisher and Callhome Spanish–English Speech Translation Corpus,http://cis.upenn.edu/~ccb/publications/improved-speech-to-speech-translation.pdf,improved-speech-to-speech-translation,Table,3,https://www.dropbox.com/request/1LDzPYBTyTVeWtSV10fT,improved-speech-to-speech-translation-table-3.pdf,Improved Speech-to-Text Translation with the Fisher and Callhome Spanish–English Speech Translation Corpus,http://cis.upenn.edu/~ccb/publications/improved-speech-to-speech-translation.pdf,improved-speech-to-speech-translation,Table,4,https://www.dropbox.com/request/1LDzPYBTyTVeWtSV10fT,improved-speech-to-speech-translation-table-4.pdf,Improved Speech-to-Text Translation with the Fisher and Callhome Spanish–English Speech Translation Corpus,http://cis.upenn.edu/~ccb/publications/improved-speech-to-speech-translation.pdf,improved-speech-to-speech-translation,Table,5,https://www.dropbox.com/request/1LDzPYBTyTVeWtSV10fT,improved-speech-to-speech-translation-table-5.pdf,PARADIGM&colon; Paraphrase Diagnostics through Grammar Matching,http://cis.upenn.edu/~ccb/publications/paradigm-paraphrase-evaluation.pdf,paradigm-paraphrase-evaluation,Figure,1,https://www.dropbox.com/request/DjyFJnHa5f5E9ohefbOq,paradigm-paraphrase-evaluation-figure-1.pdf,"Data splits for Fisher Spanish (top), Callhome Spanish (middle), and Europarl + News Commentary (bottom; for comparison). Words is the number of Spanish word tokens (after tokenization). The mean number of words per sentences ranges from 11.8 to 13.1.","Lattice statistics for the three Fisher and two Callhome test sets. Word error rates correspond to the 1-best and oracle paths from the lattice, and # Paths denotes the average number of distinct paths through each lattice. The average node density (the number of outgoing arcs) is 1.3 for Fisher and 1.4 for Callhome.","BLEU scores (four references) on Fisher/Dev2. The columns vary the data used to train the MT system, and the rows alter the interface between the ASR and MT systems.",BLEU scores (one reference) on Callhome/Evltest.,"PARADIGM extracts lexical, phrasal and syntactic paraphrases from parsed, word-aligned sentence pairs."
5,PARADIGM&colon; Paraphrase Diagnostics through Grammar Matching,http://cis.upenn.edu/~ccb/publications/paradigm-paraphrase-evaluation.pdf,paradigm-paraphrase-evaluation,Figure,2,https://www.dropbox.com/request/DjyFJnHa5f5E9ohefbOq,paradigm-paraphrase-evaluation-figure-2.pdf,PARADIGM&colon; Paraphrase Diagnostics through Grammar Matching,http://cis.upenn.edu/~ccb/publications/paradigm-paraphrase-evaluation.pdf,paradigm-paraphrase-evaluation,Figure,3,https://www.dropbox.com/request/DjyFJnHa5f5E9ohefbOq,paradigm-paraphrase-evaluation-figure-3.pdf,PARADIGM&colon; Paraphrase Diagnostics through Grammar Matching,http://cis.upenn.edu/~ccb/publications/paradigm-paraphrase-evaluation.pdf,paradigm-paraphrase-evaluation,Figure,4,https://www.dropbox.com/request/DjyFJnHa5f5E9ohefbOq,paradigm-paraphrase-evaluation-figure-4.pdf,PARADIGM&colon; Paraphrase Diagnostics through Grammar Matching,http://cis.upenn.edu/~ccb/publications/paradigm-paraphrase-evaluation.pdf,paradigm-paraphrase-evaluation,Table,1,https://www.dropbox.com/request/DjyFJnHa5f5E9ohefbOq,paradigm-paraphrase-evaluation-table-1.pdf,PARADIGM&colon; Paraphrase Diagnostics through Grammar Matching,http://cis.upenn.edu/~ccb/publications/paradigm-paraphrase-evaluation.pdf,paradigm-paraphrase-evaluation,Table,2,https://www.dropbox.com/request/DjyFJnHa5f5E9ohefbOq,paradigm-paraphrase-evaluation-table-2.pdf,"Four examples each of lexical, phrasal, and syntactic paraphrases that can be extracted from the sentence pair in Figure 1.",We measure the goodness of paraphrase grammars by determine how often they can be used to synchronously parse gold-standard sentential paraphrases. Note we do not require the synchronous derivation to match a gold-standard parse tree.,Precision lower bound and relative recall when overlapping different sizes of PPDB with the syntactic ParaMetric grammar.,Amount of English–English parallel data. LDC data has 4 parallel translations per sentence. Literature data is from Barzilay and McKeown (2001). MSR data is from Quirk et al. (2004) and Dolan et al. (2004). ParaMertic data is from Callison-Burch et al. (2008).,Size of various paraphrase grammars.
5,PARADIGM&colon; Paraphrase Diagnostics through Grammar Matching,http://cis.upenn.edu/~ccb/publications/paradigm-paraphrase-evaluation.pdf,paradigm-paraphrase-evaluation,Table,3,https://www.dropbox.com/request/DjyFJnHa5f5E9ohefbOq,paradigm-paraphrase-evaluation-table-3.pdf,PARADIGM&colon; Paraphrase Diagnostics through Grammar Matching,http://cis.upenn.edu/~ccb/publications/paradigm-paraphrase-evaluation.pdf,paradigm-paraphrase-evaluation,Table,4,https://www.dropbox.com/request/DjyFJnHa5f5E9ohefbOq,paradigm-paraphrase-evaluation-table-4.pdf,PARADIGM&colon; Paraphrase Diagnostics through Grammar Matching,http://cis.upenn.edu/~ccb/publications/paradigm-paraphrase-evaluation.pdf,paradigm-paraphrase-evaluation,Table,5,https://www.dropbox.com/request/DjyFJnHa5f5E9ohefbOq,paradigm-paraphrase-evaluation-table-5.pdf,PARADIGM&colon; Paraphrase Diagnostics through Grammar Matching,http://cis.upenn.edu/~ccb/publications/paradigm-paraphrase-evaluation.pdf,paradigm-paraphrase-evaluation,Table,6,https://www.dropbox.com/request/DjyFJnHa5f5E9ohefbOq,paradigm-paraphrase-evaluation-table-6.pdf,PARADIGM&colon; Paraphrase Diagnostics through Grammar Matching,http://cis.upenn.edu/~ccb/publications/paradigm-paraphrase-evaluation.pdf,paradigm-paraphrase-evaluation,Table,7,https://www.dropbox.com/request/DjyFJnHa5f5E9ohefbOq,paradigm-paraphrase-evaluation-table-7.pdf,Size of strict overlap (number of rules and % of the gold standard) of each grammar with a syntactic grammar derived from ParaMetric. freq. ≥ 2 means we first removed all rules that appeared only once from the ParaMetric grammar. The number in parentheses shows the percentage of ParaMetric rules that are present in the overlap.,Size of non-strict overlap of each grammar with the syntactic grammar derived from ParaMetric. The number in parentheses shows the percentage of ParaMetric rules that are present in the overlap.,Number of paraphrases of each type in each grammar’s strict overlap with the syntactic ParaMetric grammar. Numbers in parentheses show the percentage of ParaMetric rules of each type.,Parse coverage on held-out LDC data. The all column considers every possible sentential paraphrase in the test set. The any column considers a sentence parsed if any of its paraphrases was able to parsed.,The correlation (Spearman’s ρ) of different automatic evaluation metrics with human judgments of paraphrase quality for the text-to- text generation task of sentence compression.
5,Poetry of the Crowd&colon; A Human Computation Algorithm to Convert Prose into Rhyming Verse,http://cis.upenn.edu/~ccb/publications/poetry-generation-with-crowdsourcing.pdf,poetry-generation-with-crowdsourcing,Figure,1,https://www.dropbox.com/request/amOtL3tL7KGazLEKWjG6,poetry-generation-with-crowdsourcing-figure-1.pdf,Poetry of the Crowd&colon; A Human Computation Algorithm to Convert Prose into Rhyming Verse,http://cis.upenn.edu/~ccb/publications/poetry-generation-with-crowdsourcing.pdf,poetry-generation-with-crowdsourcing,Figure,2,https://www.dropbox.com/request/amOtL3tL7KGazLEKWjG6,poetry-generation-with-crowdsourcing-figure-2.pdf,Poetry of the Crowd&colon; A Human Computation Algorithm to Convert Prose into Rhyming Verse,http://cis.upenn.edu/~ccb/publications/poetry-generation-with-crowdsourcing.pdf,poetry-generation-with-crowdsourcing,Figure,3,https://www.dropbox.com/request/amOtL3tL7KGazLEKWjG6,poetry-generation-with-crowdsourcing-figure-3.pdf,Problems in Current Text Simplification Research&colon; New Data Can Help,http://cis.upenn.edu/~ccb/publications/new-data-for-text-simplification.pdf,new-data-for-text-simplification,Figure,1,https://www.dropbox.com/request/m1xqlTPZ1xkIoTAlJVrz,new-data-for-text-simplification-figure-1.pdf,Problems in Current Text Simplification Research&colon; New Data Can Help,http://cis.upenn.edu/~ccb/publications/new-data-for-text-simplification.pdf,new-data-for-text-simplification,Figure,2,https://www.dropbox.com/request/m1xqlTPZ1xkIoTAlJVrz,new-data-for-text-simplification-figure-2.pdf,"We combine the creative powers and language skills of people with computational algorithms for expanding paraphrase, identifying rhymes and calculating meter",Appropriate lexical substitutions (in shaded boxes) selected by crowdsourcing workers regarding to the context.,Figure 3: A crowd worker composes a sentence path with the “- + - + - + - + - +” stress pattern (iambic pentameter). The ending word stalls rhymes with other words that terminate sentences constructed in another worker’s task.,Manual classification of aligned sentence pairs from the Newsela corpus. We categorize randomly sampled 50 sentence pairs drawn from the Original-Simp2 and 50 sentences from the Original- Simp4.,"Distribution of document-level compression ratio, displayed as a histogram smoothed by kernel density estimation. The Newsela corpus is more normally distributed, suggesting more consistent quality."
5,Problems in Current Text Simplification Research&colon; New Data Can Help,http://cis.upenn.edu/~ccb/publications/new-data-for-text-simplification.pdf,new-data-for-text-simplification,Figure,3,https://www.dropbox.com/request/m1xqlTPZ1xkIoTAlJVrz,new-data-for-text-simplification-figure-3.pdf,Problems in Current Text Simplification Research&colon; New Data Can Help,http://cis.upenn.edu/~ccb/publications/new-data-for-text-simplification.pdf,new-data-for-text-simplification,Table,1,https://www.dropbox.com/request/m1xqlTPZ1xkIoTAlJVrz,new-data-for-text-simplification-table-1.pdf,Problems in Current Text Simplification Research&colon; New Data Can Help,http://cis.upenn.edu/~ccb/publications/new-data-for-text-simplification.pdf,new-data-for-text-simplification,Table,2,https://www.dropbox.com/request/m1xqlTPZ1xkIoTAlJVrz,new-data-for-text-simplification-table-2.pdf,Problems in Current Text Simplification Research&colon; New Data Can Help,http://cis.upenn.edu/~ccb/publications/new-data-for-text-simplification.pdf,new-data-for-text-simplification,Table,3,https://www.dropbox.com/request/m1xqlTPZ1xkIoTAlJVrz,new-data-for-text-simplification-table-3.pdf,Problems in Current Text Simplification Research&colon; New Data Can Help,http://cis.upenn.edu/~ccb/publications/new-data-for-text-simplification.pdf,new-data-for-text-simplification,Table,4,https://www.dropbox.com/request/m1xqlTPZ1xkIoTAlJVrz,new-data-for-text-simplification-table-4.pdf,"A radar chart that visualizes the odds ratio (radius axis) of discourse connectives in simple side vs. complex side. An odds ratio larger than 1 indicates the word is more likely to occur in the simplified text than in the complex text, and vice versa. Simple cue words (in the shaded region), except “hence”, are more likely to be added during Newsela’s simplification process than in Wikipedia’s. Complex conjunction connectives (in the unshaded region) are more likely to be retained in Wikipedia’s simplifications than in Newsela’s.",Example sentence pairs (NORM-SIMP) aligned between English Wikipedia and Simple English Wikipedia. The breakdown in percentages is obtained through manual examination of 200 randomly sampled sentence pairs in the Parallel Wikipedia Simplification (PWKP) corpus.,The vocabulary size of the Parallel Wikipedia Simplification (PWKP) corpus and the vocabulary difference between its normal and simple sides (as a 2×2 matrix). Only words consisting of the 26 English letters are counted.,"Example of sentences written at multiple levels of text complexity from the Newsela data set. The Lexile readability score and grade level apply to the whole article rather than individual sentences, so the same sentences may receive different scores, e.g. the above sentences for the 6th and 7th grades. The bold font highlights the parts of sentence that are different from the adjacent version(s).","Basic statistics of the Newsela Simplification corpus vs. the Parallel Wikipedia Simplification (PWKP) corpus. The Newsela corpus consists of 1130 articles with original and 4 simplified versions each. Simp-1 is of the least simplified level, while Simp-4 is the most simplified. The numbers marked by * are slightly different from previously reported, because of the use of different tokenizers."
5,Problems in Current Text Simplification Research&colon; New Data Can Help,http://cis.upenn.edu/~ccb/publications/new-data-for-text-simplification.pdf,new-data-for-text-simplification,Table,5,https://www.dropbox.com/request/m1xqlTPZ1xkIoTAlJVrz,new-data-for-text-simplification-table-5.pdf,Problems in Current Text Simplification Research&colon; New Data Can Help,http://cis.upenn.edu/~ccb/publications/new-data-for-text-simplification.pdf,new-data-for-text-simplification,Table,6,https://www.dropbox.com/request/m1xqlTPZ1xkIoTAlJVrz,new-data-for-text-simplification-table-6.pdf,Problems in Current Text Simplification Research&colon; New Data Can Help,http://cis.upenn.edu/~ccb/publications/new-data-for-text-simplification.pdf,new-data-for-text-simplification,Table,7,https://www.dropbox.com/request/m1xqlTPZ1xkIoTAlJVrz,new-data-for-text-simplification-table-7.pdf,Problems in Current Text Simplification Research&colon; New Data Can Help,http://cis.upenn.edu/~ccb/publications/new-data-for-text-simplification.pdf,new-data-for-text-simplification,Table,8,https://www.dropbox.com/request/m1xqlTPZ1xkIoTAlJVrz,new-data-for-text-simplification-table-8.pdf,Problems in Current Text Simplification Research&colon; New Data Can Help,http://cis.upenn.edu/~ccb/publications/new-data-for-text-simplification.pdf,new-data-for-text-simplification,Table,9,https://www.dropbox.com/request/m1xqlTPZ1xkIoTAlJVrz,new-data-for-text-simplification-table-9.pdf,"This table shows the vocabulary changes between different levels of simplification in the Newsela corpus (as a 5×5 matrix). Each cell shows the number of unique word types that appear in the corpus listed in the column but do not appear in the corpus listed in the row. We also list the average frequency of those vocabulary items. For example, in the cell marked *, the Simp-4 version contains 583 unique words that do not appear in the Original version. By comparing the cells marked **, we see about half of the words (19,197 out of 39,046) in the Original version are not in the Simp-4 version. Most of the vocabulary that is removed consists of low-frequency words (with an average frequency of 2.6 in the Original).","Top 50 tokens associated with the complex text, computed using the Monroe et al. (2008) method. Bold words are shared by the complex version of Newsela and the complex version of Wikipedia.",Top 50 tokens associated with the simplified text.,Frequency of example words from Table 6. These complex words are reduced at a much greater rate in the simplified Newsela than they are in the Simple English Wikipedia. A smaller odds ratio indicates greater reduction.,Top 30 syntax patterns associated with the complex text (left) and simplified text (right). Bold patterns are the top patterns shared by Newsela and Wikipedia.
5,Using Comparable Corpora to Adapt MT Models to New Domains,http://cis.upenn.edu/~ccb/publications/using-comparable-corpora-for-mt-adaptation.pdf,using-comparable-corpora-for-mt-adaptation,Table,1,https://www.dropbox.com/request/Q7SIioMvO5W3nNqHTXce,using-comparable-corpora-for-mt-adaptation-table-1.pdf,Using Comparable Corpora to Adapt MT Models to New Domains,http://cis.upenn.edu/~ccb/publications/using-comparable-corpora-for-mt-adaptation.pdf,using-comparable-corpora-for-mt-adaptation,Table,2,https://www.dropbox.com/request/Q7SIioMvO5W3nNqHTXce,using-comparable-corpora-for-mt-adaptation-table-2.pdf,Using Comparable Corpora to Adapt MT Models to New Domains,http://cis.upenn.edu/~ccb/publications/using-comparable-corpora-for-mt-adaptation.pdf,using-comparable-corpora-for-mt-adaptation,Table,3,https://www.dropbox.com/request/Q7SIioMvO5W3nNqHTXce,using-comparable-corpora-for-mt-adaptation-table-3.pdf,Adding Semantics to Data-Driven Paraphrasing,http://cis.upenn.edu/~ccb/publications/adding-semantics-to-data-driven-paraphrasing.pdf,adding-semantics-to-data-driven-paraphrasing,Figure,1,https://www.dropbox.com/request/ywJ0k3Xv3T9YT4CDrDWS,adding-semantics-to-data-driven-paraphrasing-figure-1.pdf,Adding Semantics to Data-Driven Paraphrasing,http://cis.upenn.edu/~ccb/publications/adding-semantics-to-data-driven-paraphrasing.pdf,adding-semantics-to-data-driven-paraphrasing,Figure,2,https://www.dropbox.com/request/ywJ0k3Xv3T9YT4CDrDWS,adding-semantics-to-data-driven-paraphrasing-figure-2.pdf,Summary of the size of each corpus of text used in this work in terms of the number of source and target word tokens.,Top 10 Wikipedia articles ranked by their similarity to large new-domain English monolingual corpora.,"Comparison between the performance of baseline old-domain translation models and domain-adapted models in translating science and medical domain text. We experiment with two language models: old, trained on the English side of our Hansard old-domain training corpus and new, trained on the English side of the parallel training data in each new domain. We use comparable corpora of 5, 000 (1) random, and (2) the most new-domain-like document pairs to score phrase tables. All results are averaged over three tuning runs, and we perform statistical significance testing comparing each system augmented with additional features with the baseline system that uses the same language model(s). * indicates that the BLEU scores are statistically significant with p † 0.01.","An example sentence pair for the RTE task. In order for a system to conclude that the premise (top) does not entail the hypothesis (bottom), it should recognize that sparked implies caused but that in Denmark precludes in Jordan. These phrase-level entailment relationships are modeled by natural logic.","Distribution of entailment relations in different sizes of PPDB. Distributions are estimated from our manual annotations of randomly sampled pairs. PPDB-XXXL contains over 77MM paraphrase pairs (where the majority type is independent), compared to only 700K in PPDB-S (where the majority type is equivalent)."
5,Adding Semantics to Data-Driven Paraphrasing,http://cis.upenn.edu/~ccb/publications/adding-semantics-to-data-driven-paraphrasing.pdf,adding-semantics-to-data-driven-paraphrasing,Figure,3,https://www.dropbox.com/request/ywJ0k3Xv3T9YT4CDrDWS,adding-semantics-to-data-driven-paraphrasing-figure-3.pdf,Adding Semantics to Data-Driven Paraphrasing,http://cis.upenn.edu/~ccb/publications/adding-semantics-to-data-driven-paraphrasing.pdf,adding-semantics-to-data-driven-paraphrasing,Figure,4,https://www.dropbox.com/request/ywJ0k3Xv3T9YT4CDrDWS,adding-semantics-to-data-driven-paraphrasing-figure-4.pdf,Adding Semantics to Data-Driven Paraphrasing,http://cis.upenn.edu/~ccb/publications/adding-semantics-to-data-driven-paraphrasing.pdf,adding-semantics-to-data-driven-paraphrasing,Figure,5,https://www.dropbox.com/request/ywJ0k3Xv3T9YT4CDrDWS,adding-semantics-to-data-driven-paraphrasing-figure-5.pdf,Adding Semantics to Data-Driven Paraphrasing,http://cis.upenn.edu/~ccb/publications/adding-semantics-to-data-driven-paraphrasing.pdf,adding-semantics-to-data-driven-paraphrasing,Figure,6,https://www.dropbox.com/request/ywJ0k3Xv3T9YT4CDrDWS,adding-semantics-to-data-driven-paraphrasing-figure-6.pdf,Adding Semantics to Data-Driven Paraphrasing,http://cis.upenn.edu/~ccb/publications/adding-semantics-to-data-driven-paraphrasing.pdf,adding-semantics-to-data-driven-paraphrasing,Figure,7,https://www.dropbox.com/request/ywJ0k3Xv3T9YT4CDrDWS,adding-semantics-to-data-driven-paraphrasing-figure-7.pdf,"Summary of features extracted for each phrase pair ⟨p1,p2⟩. Full descriptions of the features used are given in the supplementary material.","Confusion matrices for classifier trained using only monolingual features (distributional and path) versus bilingual features (paraphrase and translation). True labels are shown along rows, predicted along columns. The matrix is normalized along rows, so that the predictions for each (true) class sum to 100%.",ENTAILMENT,CONTRADICTION,NEUTRAL
5,Cost Optimization for Crowdsourcing Translation,http://cis.upenn.edu/~ccb/publications/cost-optimization-for-crowdsourcing-translation.pdf,cost-optimization-for-crowdsourcing-translation,Figure,1,https://www.dropbox.com/request/yOkXESxqA1LNGlMeEJFh,cost-optimization-for-crowdsourcing-translation-figure-1.pdf,Cost Optimization for Crowdsourcing Translation,http://cis.upenn.edu/~ccb/publications/cost-optimization-for-crowdsourcing-translation.pdf,cost-optimization-for-crowdsourcing-translation,Figure,2,https://www.dropbox.com/request/yOkXESxqA1LNGlMeEJFh,cost-optimization-for-crowdsourcing-translation-figure-2.pdf,Cost Optimization for Crowdsourcing Translation,http://cis.upenn.edu/~ccb/publications/cost-optimization-for-crowdsourcing-translation.pdf,cost-optimization-for-crowdsourcing-translation,Figure,3,https://www.dropbox.com/request/yOkXESxqA1LNGlMeEJFh,cost-optimization-for-crowdsourcing-translation-figure-3.pdf,Cost Optimization for Crowdsourcing Translation,http://cis.upenn.edu/~ccb/publications/cost-optimization-for-crowdsourcing-translation.pdf,cost-optimization-for-crowdsourcing-translation,Figure,4,https://www.dropbox.com/request/yOkXESxqA1LNGlMeEJFh,cost-optimization-for-crowdsourcing-translation-figure-4.pdf,Cost Optimization for Crowdsourcing Translation,http://cis.upenn.edu/~ccb/publications/cost-optimization-for-crowdsourcing-translation.pdf,cost-optimization-for-crowdsourcing-translation,Table,1,https://www.dropbox.com/request/yOkXESxqA1LNGlMeEJFh,cost-optimization-for-crowdsourcing-translation-table-1.pdf,"Example bilingual features for two crowdsourced translations of an Urdu sentence. The numbers are alignment probabilities for each aligned word. The bilingual feature is the average of these probabilities, thus 0.240 for the good translation and 0.043 for the bad translation. Some words are not aligned if potential word pairs don’t exist in bilingual training corpus.",A time-series plot of all of the translations produced by Turkers (identified by their WorkerID serial number). Turkers are sorted with the best translator at the top of the y-axis. Each tick represent a single translation and black means better than average quality.,Correlation between gold standard ranking and ranking computed using the first 20 sentences as calibration. Each bubble represents a worker. The radius of each bubble shows the relative volume of translations completed by the worker. The weighted correlation is 0.94.,Correlation between gold standard ranking and our model’s ranking. The corresponding weighted correlation is 0.95.,"The relationship between _ (the allowable deviation from the expected upper bound on BLEU score), the BLEU score for translations selected by models from partial sets and the average number of translation candidates set for each source sentence (# Trans)."
5,Cost Optimization for Crowdsourcing Translation,http://cis.upenn.edu/~ccb/publications/cost-optimization-for-crowdsourcing-translation.pdf,cost-optimization-for-crowdsourcing-translation,Table,2,https://www.dropbox.com/request/yOkXESxqA1LNGlMeEJFh,cost-optimization-for-crowdsourcing-translation-table-2.pdf,Cost Optimization for Crowdsourcing Translation,http://cis.upenn.edu/~ccb/publications/cost-optimization-for-crowdsourcing-translation.pdf,cost-optimization-for-crowdsourcing-translation,Table,3,https://www.dropbox.com/request/yOkXESxqA1LNGlMeEJFh,cost-optimization-for-crowdsourcing-translation-table-3.pdf,Cost Optimization for Crowdsourcing Translation,http://cis.upenn.edu/~ccb/publications/cost-optimization-for-crowdsourcing-translation.pdf,cost-optimization-for-crowdsourcing-translation,Table,4,https://www.dropbox.com/request/yOkXESxqA1LNGlMeEJFh,cost-optimization-for-crowdsourcing-translation-table-4.pdf,Findings of the 2012 Workshop on Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-the-wmt12-shared-tasks.pdf,findings-of-the-wmt12-shared-tasks,Figure,1,https://www.dropbox.com/request/OHSPihXt2D3RcmClBFNh,findings-of-the-wmt12-shared-tasks-figure-1.pdf,Findings of the 2012 Workshop on Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-the-wmt12-shared-tasks.pdf,findings-of-the-wmt12-shared-tasks,Figure,2,https://www.dropbox.com/request/OHSPihXt2D3RcmClBFNh,findings-of-the-wmt12-shared-tasks-figure-2.pdf,Pearson Correlations for calibration data in different proportion. The percentage column shows what proportion of the whole data set is used for calibration.,Correlation (ρ) and translation quality for the various features used by our model. Translation quality is computed by selecting best translations based on model-predicted ranking for workers (rank) and model-predicted scores for translations (score). Here we do not filter out bad workers when selecting the best translation.,"A comparison of the translation quality when we retain the top translators under different rankings. The rankings shown are random, the model’s ranking (using all features from Table 3) and the gold ranking. ∆ is the difference between the BLEU scores for the gold ranking and the model ranking. # Trans is the average number of translations needed for each source sentence.",Statistics for the training and test sets used in the translation task. The number of words and the number of distinct words (case-insensitive) is based on the provided tokenizer.,"Ratio of statistically significant pairwise comparisons at different p-levels, based on number of pair-wise judgments collected."
5,Findings of the 2012 Workshop on Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-the-wmt12-shared-tasks.pdf,findings-of-the-wmt12-shared-tasks,Table,1,https://www.dropbox.com/request/OHSPihXt2D3RcmClBFNh,findings-of-the-wmt12-shared-tasks-table-1.pdf,Findings of the 2012 Workshop on Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-the-wmt12-shared-tasks.pdf,findings-of-the-wmt12-shared-tasks,Table,2,https://www.dropbox.com/request/OHSPihXt2D3RcmClBFNh,findings-of-the-wmt12-shared-tasks-table-2.pdf,Findings of the 2012 Workshop on Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-the-wmt12-shared-tasks.pdf,findings-of-the-wmt12-shared-tasks,Table,3,https://www.dropbox.com/request/OHSPihXt2D3RcmClBFNh,findings-of-the-wmt12-shared-tasks-table-3.pdf,Findings of the 2012 Workshop on Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-the-wmt12-shared-tasks.pdf,findings-of-the-wmt12-shared-tasks,Table,4,https://www.dropbox.com/request/OHSPihXt2D3RcmClBFNh,findings-of-the-wmt12-shared-tasks-table-4.pdf,Findings of the 2012 Workshop on Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-the-wmt12-shared-tasks.pdf,findings-of-the-wmt12-shared-tasks,Table,5,https://www.dropbox.com/request/OHSPihXt2D3RcmClBFNh,findings-of-the-wmt12-shared-tasks-table-5.pdf,"Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the commercial, online, and rule-based systems were crawled by us, not submitted by the respective companies, and are therefore anonymized. Anonymized identifiers were chosen so as to correspond with the WMT11 systems.","A summary of the WMT12 ranking task, showing the number of systems and number of labels (rankings) collected for each of the language translation tasks.","Inter- and intra-annotator agreement rates for the WMT12 manual evaluation. For comparison, the WMT11 rows contain the results from the European languages individual systems task (Callison-Burch et al. (2011), Table 7).","Official results for the WMT12 translation task. Systems are ordered by their > others score, reflecting how often their translations won in pairwise comparisons. For detailed head-to-head comparisons, see Appendix A.",Overall ranking with different methods (English–German)
5,Findings of the 2012 Workshop on Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-the-wmt12-shared-tasks.pdf,findings-of-the-wmt12-shared-tasks,Table,11,https://www.dropbox.com/request/OHSPihXt2D3RcmClBFNh,findings-of-the-wmt12-shared-tasks-table-11.pdf,Findings of the 2013 Workshop on Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-the-wmt13-shared-tasks.pdf,findings-of-the-wmt13-shared-tasks,Figure,1,https://www.dropbox.com/request/hSGZ97lJpzWSdKlNRZt0,findings-of-the-wmt13-shared-tasks-figure-1.pdf,Findings of the 2013 Workshop on Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-the-wmt13-shared-tasks.pdf,findings-of-the-wmt13-shared-tasks,Figure,2,https://www.dropbox.com/request/hSGZ97lJpzWSdKlNRZt0,findings-of-the-wmt13-shared-tasks-figure-2.pdf,Findings of the 2013 Workshop on Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-the-wmt13-shared-tasks.pdf,findings-of-the-wmt13-shared-tasks,Figure,3,https://www.dropbox.com/request/hSGZ97lJpzWSdKlNRZt0,findings-of-the-wmt13-shared-tasks-figure-3.pdf,Findings of the 2013 Workshop on Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-the-wmt13-shared-tasks.pdf,findings-of-the-wmt13-shared-tasks,Figure,4,https://www.dropbox.com/request/hSGZ97lJpzWSdKlNRZt0,findings-of-the-wmt13-shared-tasks-figure-4.pdf,Participants in the WMT12 Quality Evaluation shared task.,Statistics for the training and test sets used in the translation task. The number of words and the number of distinct words (case-insensitive) is based on the provided tokenizer.,"Screenshot of the Appraise interface used in the human evaluation campaign. The annotator is presented with a source segment, a reference translation, and the outputs of five systems (anonymized and randomly-ordered) and has to rank these according to their translation quality, ties are allowed. For technical reasons, annotators on Amazon’s Mechanical Turk received all three ranking tasks for a single HIT on a single page, one upon the other.","In this screen, the annotator is expected to correct the MT output given only the context of at most two neighbouring machine-translated sentences.","In this screen, the annotator is expected to validate the monolingual edit, correcting it if necessary. The annotator is expected to add the prefix ‘OK:’ if the correction was more or less cosmetic."
5,Findings of the 2013 Workshop on Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-the-wmt13-shared-tasks.pdf,findings-of-the-wmt13-shared-tasks,Figure,5,https://www.dropbox.com/request/hSGZ97lJpzWSdKlNRZt0,findings-of-the-wmt13-shared-tasks-figure-5.pdf,Findings of the 2013 Workshop on Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-the-wmt13-shared-tasks.pdf,findings-of-the-wmt13-shared-tasks,Figure,6,https://www.dropbox.com/request/hSGZ97lJpzWSdKlNRZt0,findings-of-the-wmt13-shared-tasks-figure-6.pdf,Findings of the 2013 Workshop on Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-the-wmt13-shared-tasks.pdf,findings-of-the-wmt13-shared-tasks,Figure,7,https://www.dropbox.com/request/hSGZ97lJpzWSdKlNRZt0,findings-of-the-wmt13-shared-tasks-figure-7.pdf,Findings of the 2013 Workshop on Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-the-wmt13-shared-tasks.pdf,findings-of-the-wmt13-shared-tasks,Figure,8,https://www.dropbox.com/request/hSGZ97lJpzWSdKlNRZt0,findings-of-the-wmt13-shared-tasks-figure-8.pdf,Findings of the 2013 Workshop on Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-the-wmt13-shared-tasks.pdf,findings-of-the-wmt13-shared-tasks,Table,1,https://www.dropbox.com/request/hSGZ97lJpzWSdKlNRZt0,findings-of-the-wmt13-shared-tasks-table-1.pdf,Correlation of BLEU and WMT13 manual ranks for English→Czech translation,Correlation of NIST and WMT13 manual ranks for English→Czech translation,Projections from Figure 5 of BLEU and WMT13 manual ranks for English→Czech translation,Projections from Figure 6 of NIST and WMT13 manual ranks for English→Czech translation,"Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous years of the workshop."
5,Findings of the 2013 Workshop on Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-the-wmt13-shared-tasks.pdf,findings-of-the-wmt13-shared-tasks,Table,2,https://www.dropbox.com/request/hSGZ97lJpzWSdKlNRZt0,findings-of-the-wmt13-shared-tasks-table-2.pdf,Findings of the 2013 Workshop on Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-the-wmt13-shared-tasks.pdf,findings-of-the-wmt13-shared-tasks,Table,3,https://www.dropbox.com/request/hSGZ97lJpzWSdKlNRZt0,findings-of-the-wmt13-shared-tasks-table-3.pdf,Findings of the 2013 Workshop on Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-the-wmt13-shared-tasks.pdf,findings-of-the-wmt13-shared-tasks,Table,4,https://www.dropbox.com/request/hSGZ97lJpzWSdKlNRZt0,findings-of-the-wmt13-shared-tasks-table-4.pdf,Findings of the 2013 Workshop on Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-the-wmt13-shared-tasks.pdf,findings-of-the-wmt13-shared-tasks,Table,5,https://www.dropbox.com/request/hSGZ97lJpzWSdKlNRZt0,findings-of-the-wmt13-shared-tasks-table-5.pdf,Findings of the 2013 Workshop on Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-the-wmt13-shared-tasks.pdf,findings-of-the-wmt13-shared-tasks,Table,6,https://www.dropbox.com/request/hSGZ97lJpzWSdKlNRZt0,findings-of-the-wmt13-shared-tasks-table-6.pdf,Amount of data collected in the WMT13 manual evaluation. The final two rows report summary information from the previous two workshops.,"κ scores measuring inter-annotator agreement. The WMT13r and WMT13m columns provide breakdowns for re- searcher annotations and MTurk annotations, respectively. See Table 4 for corresponding intra-annotator agreement scores.","κ scores measuring intra-annotator agreement, i.e., self-consistency of judges, across for the past few years of the human evaluation. The WMT13r and WMT13m columns provide breakdowns for researcher annotations and MTurk annota-
tions, respectively. The perfect inter-annotator agreement for Spanish-English is a result of there being very little data for that language pair.",Agreement as a function of threshold for Turkers on the Russian–English task. The threshold is the percentage of controls a Turker must pass for her rankings to be accepted.,Official results for the WMT13 translation task. Systems are ordered by the expected win score. Lines between systems indicate clusters according to bootstrap resampling at p-level p ≤ .05. This method is also used to determine the range of ranks into which system falls. Systems with grey background indicate use of resources that fall outside the constraints provided for the shared task.
5,Findings of the 2013 Workshop on Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-the-wmt13-shared-tasks.pdf,findings-of-the-wmt13-shared-tasks,Table,7,https://www.dropbox.com/request/hSGZ97lJpzWSdKlNRZt0,findings-of-the-wmt13-shared-tasks-table-7.pdf,Findings of the 2013 Workshop on Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-the-wmt13-shared-tasks.pdf,findings-of-the-wmt13-shared-tasks,Table,8,https://www.dropbox.com/request/hSGZ97lJpzWSdKlNRZt0,findings-of-the-wmt13-shared-tasks-table-8.pdf,Findings of the 2013 Workshop on Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-the-wmt13-shared-tasks.pdf,findings-of-the-wmt13-shared-tasks,Table,9,https://www.dropbox.com/request/hSGZ97lJpzWSdKlNRZt0,findings-of-the-wmt13-shared-tasks-table-9.pdf,Findings of the 2013 Workshop on Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-the-wmt13-shared-tasks.pdf,findings-of-the-wmt13-shared-tasks,Table,10,https://www.dropbox.com/request/hSGZ97lJpzWSdKlNRZt0,findings-of-the-wmt13-shared-tasks-table-10.pdf,Findings of the 2013 Workshop on Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-the-wmt13-shared-tasks.pdf,findings-of-the-wmt13-shared-tasks,Table,11,https://www.dropbox.com/request/hSGZ97lJpzWSdKlNRZt0,findings-of-the-wmt13-shared-tasks-table-11.pdf,Distribution of review statuses.,Annotator agreement when reviewing monolingual edits.,"Understandability of English!Czech systems. The _ values indicate empirical confidence bounds at 95%. Rank ranges were also obtained in the same resampling: in 95% of observations, the system was ranked in the given range",Number of source sentences with the given number of distinct reference translations.,Participants in the WMT13 Quality Estimation shared task.
5,The American Local News Corpus,http://cis.upenn.edu/~ccb/publications/american-local-news-corpus.pdf,american-local-news-corpus,Figure,1,https://www.dropbox.com/request/W2qldSXzkFY5fjXy5SAS,american-local-news-corpus-figure-1.pdf,The American Local News Corpus,http://cis.upenn.edu/~ccb/publications/american-local-news-corpus.pdf,american-local-news-corpus,Figure,2,https://www.dropbox.com/request/W2qldSXzkFY5fjXy5SAS,american-local-news-corpus-figure-2.pdf,The American Local News Corpus,http://cis.upenn.edu/~ccb/publications/american-local-news-corpus.pdf,american-local-news-corpus,Figure,3,https://www.dropbox.com/request/W2qldSXzkFY5fjXy5SAS,american-local-news-corpus-figure-3.pdf,The American Local News Corpus,http://cis.upenn.edu/~ccb/publications/american-local-news-corpus.pdf,american-local-news-corpus,Figure,4,https://www.dropbox.com/request/W2qldSXzkFY5fjXy5SAS,american-local-news-corpus-figure-4.pdf,The American Local News Corpus,http://cis.upenn.edu/~ccb/publications/american-local-news-corpus.pdf,american-local-news-corpus,Figure,5,https://www.dropbox.com/request/W2qldSXzkFY5fjXy5SAS,american-local-news-corpus-figure-5.pdf,"Millions of words of newspaper data contained in the ALNC, by state.","Relative frequency of words related to different sports across time, for all locations. The beginning of professional sports leagues’ seasons are indicated with dotted vertical lines and the end of seasons with dashed vertical lines. For example, the National Football League’s season began in early September and ended in early February.","Ratio of football words to hockey words, by state. The darkest shade indicates that football words are mentioned 80 times as often as hockey words; the lightest that football words are mentioned only about 20 times as often.","Relative frequency (x10k) of ‘murder,’ by state. The lightest shade indicates that ‘murder’ appears once in every 20, 000 words; the darkest, about four times as often.","Relative frequency (x10k) of ‘murder’ vs. the number of murders per 100, 000 people."
5,Translations of the CALLHOME Egyptian Arabic corpus for conversational speech translation,http://cis.upenn.edu/~ccb/publications/callhome-egyptian-arabic-speech-translations.pdf,callhome-egyptian-arabic-speech-translations,Table,1,https://www.dropbox.com/request/Sm0zBIRrvjAegXt9kzki,callhome-egyptian-arabic-speech-translations-table-1.pdf,Translations of the CALLHOME Egyptian Arabic corpus for conversational speech translation,http://cis.upenn.edu/~ccb/publications/callhome-egyptian-arabic-speech-translations.pdf,callhome-egyptian-arabic-speech-translations,Table,2,https://www.dropbox.com/request/Sm0zBIRrvjAegXt9kzki,callhome-egyptian-arabic-speech-translations-table-2.pdf,Translations of the CALLHOME Egyptian Arabic corpus for conversational speech translation,http://cis.upenn.edu/~ccb/publications/callhome-egyptian-arabic-speech-translations.pdf,callhome-egyptian-arabic-speech-translations,Table,3,https://www.dropbox.com/request/Sm0zBIRrvjAegXt9kzki,callhome-egyptian-arabic-speech-translations-table-3.pdf,Translations of the CALLHOME Egyptian Arabic corpus for conversational speech translation,http://cis.upenn.edu/~ccb/publications/callhome-egyptian-arabic-speech-translations.pdf,callhome-egyptian-arabic-speech-translations,Table,4,https://www.dropbox.com/request/Sm0zBIRrvjAegXt9kzki,callhome-egyptian-arabic-speech-translations-table-4.pdf,Translations of the CALLHOME Egyptian Arabic corpus for conversational speech translation,http://cis.upenn.edu/~ccb/publications/callhome-egyptian-arabic-speech-translations.pdf,callhome-egyptian-arabic-speech-translations,Table,5,https://www.dropbox.com/request/Sm0zBIRrvjAegXt9kzki,callhome-egyptian-arabic-speech-translations-table-5.pdf,"Sizes (in # conversations) of the Callhome Egyptian Arabic corpus, supplements and evaluation datasets. The conversations last between 5-30 minutes.","Partition statistics for the Callhome Egyptian Arabic corpus, supplements and evaluation datasets. Column 2,3 and 4 represent number of utterances, numbers of words and average number of words per utterance respectively.","A sample of the special symbols using in the Arabic transcripts. These represent non-conventional speech segments such as non-verbal vocalizations, disfluencies, back- ground noise and distortion.",The results of the translation task described in section 4. Each utterance in the original partitions has about four redundant translations. The number of utterances in column 2 has hence effectively been multiplied by 4. The last column represents the number of words per utterance in the translations.,"A sample of the translations obtained using the translation task described in section 4. The translations are lower-cased, tokenized and punctuation has been normalized."
5,Moses&colon; Open source toolkit for statistical machine translation,http://cis.upenn.edu/~ccb/publications/moses-toolkit.pdf,moses-toolkit,Figure,1,https://www.dropbox.com/request/74RK0QcO1OhiPaaOpzoN,moses-toolkit-figure-1.pdf,Moses&colon; Open source toolkit for statistical machine translation,http://cis.upenn.edu/~ccb/publications/moses-toolkit.pdf,moses-toolkit,Figure,2,https://www.dropbox.com/request/74RK0QcO1OhiPaaOpzoN,moses-toolkit-figure-2.pdf,Moses&colon; Open source toolkit for statistical machine translation,http://cis.upenn.edu/~ccb/publications/moses-toolkit.pdf,moses-toolkit,Figure,3,https://www.dropbox.com/request/74RK0QcO1OhiPaaOpzoN,moses-toolkit-figure-3.pdf,Findings of the 2011 Workshop on Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-the-wmt11-shared-tasks.pdf,findings-of-the-wmt11-shared-tasks,Figure,1,https://www.dropbox.com/request/61mdnFHXY23xK31YGUrj,findings-of-the-wmt11-shared-tasks-figure-1.pdf,Findings of the 2011 Workshop on Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-the-wmt11-shared-tasks.pdf,findings-of-the-wmt11-shared-tasks,Table,1,https://www.dropbox.com/request/61mdnFHXY23xK31YGUrj,findings-of-the-wmt11-shared-tasks-table-1.pdf,Non-factored translation,Factored translation,Example of graph of decoding steps,"Statistics for the training and test sets used in the translation task. The number of words and the number of
distinct words (case-insensitive) is based on the provided tokenizer.","Participants in the shared translation task (European language pairs; individual system track). Not all teams
participated in all language pairs. The translations from commercial and online systems were crawled by us, not
submitted by the respective companies, and are therefore anonymized."
5,Paraphrasing with Bilingual Parallel Corpora,http://cis.upenn.edu/~ccb/publications/paraphrasing-with-bilingual-parallel-corpora.pdf,paraphrasing-with-bilingual-parallel-corpora,Figure,4,https://www.dropbox.com/request/HyKRFabZDOD6JLTGcAez,paraphrasing-with-bilingual-parallel-corpora-figure-4.pdf,Paraphrasing with Bilingual Parallel Corpora,http://cis.upenn.edu/~ccb/publications/paraphrasing-with-bilingual-parallel-corpora.pdf,paraphrasing-with-bilingual-parallel-corpora,Table,1,https://www.dropbox.com/request/HyKRFabZDOD6JLTGcAez,paraphrasing-with-bilingual-parallel-corpora-table-1.pdf,Paraphrasing with Bilingual Parallel Corpora,http://cis.upenn.edu/~ccb/publications/paraphrasing-with-bilingual-parallel-corpora.pdf,paraphrasing-with-bilingual-parallel-corpora,Table,2,https://www.dropbox.com/request/HyKRFabZDOD6JLTGcAez,paraphrasing-with-bilingual-parallel-corpora-table-2.pdf,Paraphrasing with Bilingual Parallel Corpora,http://cis.upenn.edu/~ccb/publications/paraphrasing-with-bilingual-parallel-corpora.pdf,paraphrasing-with-bilingual-parallel-corpora,Table,3,https://www.dropbox.com/request/HyKRFabZDOD6JLTGcAez,paraphrasing-with-bilingual-parallel-corpora-table-3.pdf,Fast Cheap and Creative&colon; Evaluating Translation Quality Using Amazon's Mechanical Turk,http://cis.upenn.edu/~ccb/publications/mechanical-turk-for-machine-translation-evaluation.pdf,mechanical-turk-for-machine-translation-evaluation,Figure,1,https://www.dropbox.com/request/VltxwB9gFmcmXhavx1zy,mechanical-turk-for-machine-translation-evaluation-figure-1.pdf,Paraphrases substituted in for the original phrase,Phrases that were selected to paraphrase,Paraphrases extracted from a manually word-aligned parallel corpus,Paraphrase accuracy and correct meaning for the different data conditions,Agreement on ranking translated sentences increases as more non-experts vote. Weighting non-experts' votes based on agreement with either experts or other non-expert increases it up further. Five weighted non-experts reached the top line agreement between experts.
5,Edinburgh System Description for the 2005 IWSLT Speech Translation Evaluation,http://cis.upenn.edu/~ccb/publications/iwslt05-report.pdf,iwslt05-report,Table,4,https://www.dropbox.com/request/ukJ263jwOLgFgvJqSaMp,iwslt05-report-table-4.pdf,Edinburgh System Description for the 2005 IWSLT Speech Translation Evaluation,http://cis.upenn.edu/~ccb/publications/iwslt05-report.pdf,iwslt05-report,Table,5,https://www.dropbox.com/request/ukJ263jwOLgFgvJqSaMp,iwslt05-report-table-5.pdf,Edinburgh System Description for the 2005 IWSLT Speech Translation Evaluation,http://cis.upenn.edu/~ccb/publications/iwslt05-report.pdf,iwslt05-report,Table,6,https://www.dropbox.com/request/ukJ263jwOLgFgvJqSaMp,iwslt05-report-table-6.pdf,Improved Statistical Machine Translation Using Paraphrases,http://cis.upenn.edu/~ccb/publications/improved-statistical-machine-translation-using-paraphrases.pdf,improved-statistical-machine-translation-using-paraphrases,Figure,1,https://www.dropbox.com/request/wWKFUTP281Fi4thTrIDA,improved-statistical-machine-translation-using-paraphrases-figure-1.pdf,Improved Statistical Machine Translation Using Paraphrases,http://cis.upenn.edu/~ccb/publications/improved-statistical-machine-translation-using-paraphrases.pdf,improved-statistical-machine-translation-using-paraphrases,Figure,2,https://www.dropbox.com/request/wWKFUTP281Fi4thTrIDA,improved-statistical-machine-translation-using-paraphrases-figure-2.pdf,Optimising the reordering limit (maximum word distance for phrase movement). The table also shows the effect of dropping unknown words instead of passing them to the output.,"Official Results: The scores for our official submission to the IWSLT’05 Evaluation Campaign (length penalty in parenthesis), and rank among participants according to the BLEU score.",Optimisation to average reference sentence length instead of shortest reference length (length penalty in parenthesis): Note the improved length penalties and vastly improved NIST scores. 4 out of 5 BLEU scores are higher as well (exception is Chinese-English).,"Percent of unique unigrams, bigrams, trigrams, and 4-grams from the Europarl Spanish test sentences for which translations were learned in increasingly large training corpora",Using a bilingual parallel corpus to extract paraphrases
5,Improved Statistical Machine Translation Using Paraphrases,http://cis.upenn.edu/~ccb/publications/improved-statistical-machine-translation-using-paraphrases.pdf,improved-statistical-machine-translation-using-paraphrases,Figure,3,https://www.dropbox.com/request/wWKFUTP281Fi4thTrIDA,improved-statistical-machine-translation-using-paraphrases-figure-3.pdf,Improved Statistical Machine Translation Using Paraphrases,http://cis.upenn.edu/~ccb/publications/improved-statistical-machine-translation-using-paraphrases.pdf,improved-statistical-machine-translation-using-paraphrases,Figure,4,https://www.dropbox.com/request/wWKFUTP281Fi4thTrIDA,improved-statistical-machine-translation-using-paraphrases-figure-4.pdf,Improved Statistical Machine Translation Using Paraphrases,http://cis.upenn.edu/~ccb/publications/improved-statistical-machine-translation-using-paraphrases.pdf,improved-statistical-machine-translation-using-paraphrases,Figure,5,https://www.dropbox.com/request/wWKFUTP281Fi4thTrIDA,improved-statistical-machine-translation-using-paraphrases-figure-5.pdf,Improved Statistical Machine Translation Using Paraphrases,http://cis.upenn.edu/~ccb/publications/improved-statistical-machine-translation-using-paraphrases.pdf,improved-statistical-machine-translation-using-paraphrases,Figure,6,https://www.dropbox.com/request/wWKFUTP281Fi4thTrIDA,improved-statistical-machine-translation-using-paraphrases-figure-6.pdf,Improved Statistical Machine Translation Using Paraphrases,http://cis.upenn.edu/~ccb/publications/improved-statistical-machine-translation-using-paraphrases.pdf,improved-statistical-machine-translation-using-paraphrases,Table,1,https://www.dropbox.com/request/wWKFUTP281Fi4thTrIDA,improved-statistical-machine-translation-using-paraphrases-table-1.pdf,Test sentences and reference translations were manually word-aligned. This allowed us to equate unseen phrases with their corresponding English phrase. In this case enumeradas with listed.,Judges were asked whether the highlighted phrase retained the same meaning as the highlighted phrase in the reference translation (top),{},{},Example of automatically generated para- phrases for the Spanish words encargarnos and us- ado along with their English translations which were automatically learned from the Europarl corpus
5,Cheap Fast and Good Enough&colon; Automatic Speech Recognition with Non-Expert Transcription,http://cis.upenn.edu/~ccb/publications/automatic-speech-recognition-with-non-expert-transcription.pdf,automatic-speech-recognition-with-non-expert-transcription,Figure,3,https://www.dropbox.com/request/vWv26qPbORYlXfHBCo53,automatic-speech-recognition-with-non-expert-transcription-figure-3.pdf,Cheap Fast and Good Enough&colon; Automatic Speech Recognition with Non-Expert Transcription,http://cis.upenn.edu/~ccb/publications/automatic-speech-recognition-with-non-expert-transcription.pdf,automatic-speech-recognition-with-non-expert-transcription,Figure,4,https://www.dropbox.com/request/vWv26qPbORYlXfHBCo53,automatic-speech-recognition-with-non-expert-transcription-figure-4.pdf,Cheap Fast and Good Enough&colon; Automatic Speech Recognition with Non-Expert Transcription,http://cis.upenn.edu/~ccb/publications/automatic-speech-recognition-with-non-expert-transcription.pdf,automatic-speech-recognition-with-non-expert-transcription,Figure,5,https://www.dropbox.com/request/vWv26qPbORYlXfHBCo53,automatic-speech-recognition-with-non-expert-transcription-figure-5.pdf,Cheap Fast and Good Enough&colon; Automatic Speech Recognition with Non-Expert Transcription,http://cis.upenn.edu/~ccb/publications/automatic-speech-recognition-with-non-expert-transcription.pdf,automatic-speech-recognition-with-non-expert-transcription,Figure,6,https://www.dropbox.com/request/vWv26qPbORYlXfHBCo53,automatic-speech-recognition-with-non-expert-transcription-figure-6.pdf,Cheap Fast and Good Enough&colon; Automatic Speech Recognition with Non-Expert Transcription,http://cis.upenn.edu/~ccb/publications/automatic-speech-recognition-with-non-expert-transcription.pdf,automatic-speech-recognition-with-non-expert-transcription,Figure,7,https://www.dropbox.com/request/vWv26qPbORYlXfHBCo53,automatic-speech-recognition-with-non-expert-transcription-figure-7.pdf,"Historical cost estimates are $150 per hour of transcription (blue cirlces). The company Casting Words uses Turkers to transcribe English at $90 per hour which we estimated to be high quality (green triangles). Transcription without quality control on Mechanical Turk (red squares) is drastically cheaper at $5 per hour. With a fixed budget, it is better to transcribe more data at lower quality than to improve quality. Contrast the oracle WER for 20 hours transcribed three times (red diamond) with 60 hours transcribed once (bottom red square).","Each Turker was judged against professional and non-professional reference and assigned an overall disagreement. The distribution of Turker disagreement follows a gamma distribution, with a tight cluster of average
Turkers and a long-tail of bad Turkers. Estimating
with non-professionals (even though the reference is 23% wrong on average) is surprisingly well matched to professional estimate. Turker estimation over-estimated
disagreement by only 2%.","Boxplot of the difference of non-professional disagreement with a fixed number of utterances to professional
disagreement over all utterances. While error is expectedly high with one utterance, 50% of the estimates are within 3% of the truth after ten utterances and 75% of the estimates are within 6% after fifteen utterances.","Each Turker is a point with professional (X axis) plotted against non-professional (Y axis) disagreement. The non-professional disagreement correlates surprisingly well with professional disagreement even though the transcripts used as reference are 23% wrong on average. By setting a selection threshold, the space is divided into four quadrants. The bottom left are correctly accepted: both non-professional and professional
disagreement are below the threshold. The top left are incorrectly rejected: using their transcripts would have helped, but they don’t hurt system performance, just waste money. The top right are correctly rejected for having high disagreement. The bottom right are the troublesome false positives that are included in training but actually may hurt performance. Luckily, the ratio of false negatives to false positives is usually much larger.","It is difficult to find only good Turkers since the false positives outnumber the few good workers. However, rejecting bad Turkers becomes very easy once past the mean error rate of 23%. It is better to use disagreement estimation to reject poor workers instead of finding good workers."
5,(Meta-) Evaluation of Machine Translation,http://cis.upenn.edu/~ccb/publications/meta-evaluation-of-machine-tranlsion.pdf,meta-evaluation-of-machine-tranlsion,Table,1,https://www.dropbox.com/request/M8iFUCZdnGumqf75XpK3,meta-evaluation-of-machine-tranlsion-table-1.pdf,(Meta-) Evaluation of Machine Translation,http://cis.upenn.edu/~ccb/publications/meta-evaluation-of-machine-tranlsion.pdf,meta-evaluation-of-machine-tranlsion,Table,2,https://www.dropbox.com/request/M8iFUCZdnGumqf75XpK3,meta-evaluation-of-machine-tranlsion-table-2.pdf,(Meta-) Evaluation of Machine Translation,http://cis.upenn.edu/~ccb/publications/meta-evaluation-of-machine-tranlsion.pdf,meta-evaluation-of-machine-tranlsion,Table,3,https://www.dropbox.com/request/M8iFUCZdnGumqf75XpK3,meta-evaluation-of-machine-tranlsion-table-3.pdf,(Meta-) Evaluation of Machine Translation,http://cis.upenn.edu/~ccb/publications/meta-evaluation-of-machine-tranlsion.pdf,meta-evaluation-of-machine-tranlsion,Table,4,https://www.dropbox.com/request/M8iFUCZdnGumqf75XpK3,meta-evaluation-of-machine-tranlsion-table-4.pdf,(Meta-) Evaluation of Machine Translation,http://cis.upenn.edu/~ccb/publications/meta-evaluation-of-machine-tranlsion.pdf,meta-evaluation-of-machine-tranlsion,Table,5,https://www.dropbox.com/request/M8iFUCZdnGumqf75XpK3,meta-evaluation-of-machine-tranlsion-table-5.pdf,Participants in the shared task. Not all groups participated in all translation directions.,The number of items that were judged for each task during the manual evaluation,"The proportion of time that participants’
entries were top-ranked in the human evaluation","The proportion of time that participants’
entries were top-ranked by the automatic evaluation
metrics","Kappa coefficient values representing the
inter-annotator agreement for the different types of
manual evaluation"
5,PPDB&colon; The Paraphrase Database,http://cis.upenn.edu/~ccb/publications/ppdb.pdf,ppdb,Figure,2,https://www.dropbox.com/request/t6Q1EvUQRlr1R0WhcQk5,ppdb-figure-2.pdf,PPDB&colon; The Paraphrase Database,http://cis.upenn.edu/~ccb/publications/ppdb.pdf,ppdb,Figure,3,https://www.dropbox.com/request/t6Q1EvUQRlr1R0WhcQk5,ppdb-figure-3.pdf,PPDB&colon; The Paraphrase Database,http://cis.upenn.edu/~ccb/publications/ppdb.pdf,ppdb,Figure,4,https://www.dropbox.com/request/t6Q1EvUQRlr1R0WhcQk5,ppdb-figure-4.pdf,PPDB&colon; The Paraphrase Database,http://cis.upenn.edu/~ccb/publications/ppdb.pdf,ppdb,Table,1,https://www.dropbox.com/request/t6Q1EvUQRlr1R0WhcQk5,ppdb-table-1.pdf,PPDB&colon; The Paraphrase Database,http://cis.upenn.edu/~ccb/publications/ppdb.pdf,ppdb,Table,2,https://www.dropbox.com/request/t6Q1EvUQRlr1R0WhcQk5,ppdb-table-2.pdf,"Features extracted for the phrase the long term from the n-gram corpus (2a) and Annotated Gigaword (2b).

(a) The n-gram corpus records the long-term as preceded by revise (43 times), and followed by plans (97 times). We add corresponding features to the phrase’s distributional signature retaining the counts of the original n-grams.
(b) Here, position-aware lexical and part-of-speech n-gram features, labeled dependency links , and features reflecting the phrase’s CCG-style label NP/NN are included in the context vector.","To inspect our coverage, we use the Penn Treebank’s parses to map from Propbank annotations to PPDB’s syntactic patterns. For the above annotation predicate, we extract VBP ! expect, which is matched by paraphrase rules like VBP ! expect | anticipate and VBP ! expect | hypothesize. To search for the entire relation, we replace the argument spans with syntactic nonterminals. Here, we obtain S ! NP expect S, for which PPDB has matching rules like S ! NP expect S | NP would hope S, and S ! NP expect S | NP trust S. This allows us to apply sophisticated paraphrases to the predicate while capturing its arguments in a generalized fashion.","An illustration of PPDB’s coverage of the manually annotated Propbank predicate phrases (4a) and binary relations with argument non-terminals (4b). The curves indicate the coverage on tokens (solid) and types (dotted), as well as the average number of paraphrases per covered type (dashed) at the given pruning level.

(a) PPDB:Eng coverage of Propbank predicates (top), and average human judgment score (bottom) for varying pruning thresholds.
(b) PPDB:Eng’s coverage of Propbank predicates with up to two arguments. Here we consider rules that paraphrase the full predicate-argument expression.","A breakdown of PPDB:Eng size by paraphrase type. We distinguish lexical (i.e. one-word) paraphrases, phrasal paraphrases and syntactically labeled paraphrase patterns.","An overview of PPDB:Spa. Again, we partition the resource into lexical (i.e. one-word) paraphrases, phrasal paraphrases and syntactically labeled paraphrase patterns."
5,A program for automatically selecting the best output from multiple machine translation engines,http://cis.upenn.edu/~ccb/publications/multi-engine-mt-with-language-models.pdf,multi-engine-mt-with-language-models,Table,1,https://www.dropbox.com/request/vTiVBbQndb7Cv5yKkJRL,multi-engine-mt-with-language-models-table-1.pdf,A program for automatically selecting the best output from multiple machine translation engines,http://cis.upenn.edu/~ccb/publications/multi-engine-mt-with-language-models.pdf,multi-engine-mt-with-language-models,Table,2,https://www.dropbox.com/request/vTiVBbQndb7Cv5yKkJRL,multi-engine-mt-with-language-models-table-2.pdf,A program for automatically selecting the best output from multiple machine translation engines,http://cis.upenn.edu/~ccb/publications/multi-engine-mt-with-language-models.pdf,multi-engine-mt-with-language-models,Table,3,https://www.dropbox.com/request/vTiVBbQndb7Cv5yKkJRL,multi-engine-mt-with-language-models-table-3.pdf,A program for automatically selecting the best output from multiple machine translation engines,http://cis.upenn.edu/~ccb/publications/multi-engine-mt-with-language-models.pdf,multi-engine-mt-with-language-models,Table,4,https://www.dropbox.com/request/vTiVBbQndb7Cv5yKkJRL,multi-engine-mt-with-language-models-table-4.pdf,Constructing Corpora for the Development and Evaluation of Paraphrase Systems,http://cis.upenn.edu/~ccb/publications/constructing-corpora-for-paraphrase-systems.pdf,constructing-corpora-for-paraphrase-systems,Figure,1,https://www.dropbox.com/request/3TQzC3pEb7vIN72K3N4H,constructing-corpora-for-paraphrase-systems-figure-1.pdf,Japanese,French,French,English,"Manual alignment between two sentence pairs from the MTC corpus, displayed as a grid."
5,Constructing Corpora for the Development and Evaluation of Paraphrase Systems,http://cis.upenn.edu/~ccb/publications/constructing-corpora-for-paraphrase-systems.pdf,constructing-corpora-for-paraphrase-systems,Table,2,https://www.dropbox.com/request/3TQzC3pEb7vIN72K3N4H,constructing-corpora-for-paraphrase-systems-table-2.pdf,Constructing Corpora for the Development and Evaluation of Paraphrase Systems,http://cis.upenn.edu/~ccb/publications/constructing-corpora-for-paraphrase-systems.pdf,constructing-corpora-for-paraphrase-systems,Table,3,https://www.dropbox.com/request/3TQzC3pEb7vIN72K3N4H,constructing-corpora-for-paraphrase-systems-table-3.pdf,Constructing Corpora for the Development and Evaluation of Paraphrase Systems,http://cis.upenn.edu/~ccb/publications/constructing-corpora-for-paraphrase-systems.pdf,constructing-corpora-for-paraphrase-systems,Table,4,https://www.dropbox.com/request/3TQzC3pEb7vIN72K3N4H,constructing-corpora-for-paraphrase-systems-table-4.pdf,Constructing Corpora for the Development and Evaluation of Paraphrase Systems,http://cis.upenn.edu/~ccb/publications/constructing-corpora-for-paraphrase-systems.pdf,constructing-corpora-for-paraphrase-systems,Table,5,https://www.dropbox.com/request/3TQzC3pEb7vIN72K3N4H,constructing-corpora-for-paraphrase-systems-table-5.pdf,Paraphrasing and Translation,http://cis.upenn.edu/~ccb/publications/callison-burch-thesis.pdf,callison-burch-thesis,Figure,1,https://www.dropbox.com/request/oTN6kiN90fKyXZyNnqRF,callison-burch-thesis-figure-1.pdf,"Phrase pairs specified by the word alignments from Figure 2, using the possible alignments. The entire set of atomic phrase pairs for either annotator (labeled A or B) are shown and a selection of the remaining 57 composite phrase pairs. The italics denote lexically identical phrase pairs. ∗This phrase pair is atomic in A but composite in B.","Inter-annotator agreement using precision, recall, F1, and Cˆ; the agreement is measured over words in the (monolingual) parallel corpus.","Inter-annotator agreement using precision, recall, F1 and Cˆ; the agreement is measured over atomic phrase pairs in the (monolingual) parallel corpus.","Agreement between automatic Giza++ predicted word-alignments and our manually corrected alignments, measured over atomic phrase pairs.",The Spanish word cada ́veres can be used to discover that the English phrase dead bodies can be paraphrased as corpses.
5,Improving Translation Lexicon Induction from Monolingual Corpora via Dependency Contexts and Part-of-Speech Equivalences,http://cis.upenn.edu/~ccb/publications/improving-translation-lexicon-induction.pdf,improving-translation-lexicon-induction,Table,5,https://www.dropbox.com/request/f8fMLfzazCWDk0pNAmdx,improving-translation-lexicon-induction-table-5.pdf,Improving Translation Lexicon Induction from Monolingual Corpora via Dependency Contexts and Part-of-Speech Equivalences,http://cis.upenn.edu/~ccb/publications/improving-translation-lexicon-induction.pdf,improving-translation-lexicon-induction,Table,6,https://www.dropbox.com/request/f8fMLfzazCWDk0pNAmdx,improving-translation-lexicon-induction-table-6.pdf,Stream-based Translation Models for Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/stream-based-translation-models.pdf,stream-based-translation-models,Figure,1,https://www.dropbox.com/request/bZKpZT19Sr8wFcI1ya0G,stream-based-translation-models-figure-1.pdf,Stream-based Translation Models for Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/stream-based-translation-models.pdf,stream-based-translation-models,Figure,2,https://www.dropbox.com/request/bZKpZT19Sr8wFcI1ya0G,stream-based-translation-models-figure-2.pdf,Stream-based Translation Models for Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/stream-based-translation-models.pdf,stream-based-translation-models,Figure,3,https://www.dropbox.com/request/bZKpZT19Sr8wFcI1ya0G,stream-based-translation-models-figure-3.pdf,Performance of dependency context-based model along with addition of part-of-speech mapping model on translating all word-types.,"List of 20 most confident mappings using the dependency context with the part-of-speech mapping model translating all word-types. Note that although the second best mapping in Table4 for noun-translation is for xenofobia with score 0.87, xenofobia is not among the 1000 most frequent words (of all word-types) and thus is not in this test set.",Streaming coverage conditions. In traditional batch based modeling the coverage of a trained model never changes. Unbounded coverage operates without any memory constraints so the model is able to continually add data from the input stream. Bounded coverage uses just a fixed window.,Recency effects to SMT performance. Depicted are the differences in BLEU scores for multiple test points decoded by a static baseline system and a system batched retrained on a fixed sized window prior to the test point in question. The results are accentuated at the end of the timeline when more time has passed confirming that recent data impacts translation performance.,Static vs. online TM performance. Gains in translation performance measured by BLEU are achieved when recent German-English sentence pairs are automatically incorporated into the TM. Shown are relative BLEU improvements for the online models against the static baseline.
5,Stream-based Translation Models for Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/stream-based-translation-models.pdf,stream-based-translation-models,Figure,4,https://www.dropbox.com/request/bZKpZT19Sr8wFcI1ya0G,stream-based-translation-models-figure-4.pdf,Stream-based Translation Models for Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/stream-based-translation-models.pdf,stream-based-translation-models,Table,1,https://www.dropbox.com/request/bZKpZT19Sr8wFcI1ya0G,stream-based-translation-models-table-1.pdf,Stream-based Translation Models for Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/stream-based-translation-models.pdf,stream-based-translation-models,Table,2,https://www.dropbox.com/request/bZKpZT19Sr8wFcI1ya0G,stream-based-translation-models-table-2.pdf,Stream-based Translation Models for Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/stream-based-translation-models.pdf,stream-based-translation-models,Table,3,https://www.dropbox.com/request/bZKpZT19Sr8wFcI1ya0G,stream-based-translation-models-table-3.pdf,Stream-based Translation Models for Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/stream-based-translation-models.pdf,stream-based-translation-models,Table,4,https://www.dropbox.com/request/bZKpZT19Sr8wFcI1ya0G,stream-based-translation-models-table-4.pdf,Example sentences and improvements to their translation fluency by the adaptation of the TM with recent sentences. In both examples we get longer matching phrases in the online translation compared to the static one.,"Date ranges, total sentence pairs, and source and target word counts encountered in the input stream for example epochs. Epoch 00 is baseline data that is also used as a seed corpus for the online models.","Test and Hiero grammar rules extracted for the corresponding test set.
Translation model statistics for example epochs and the next test dates grouped by experimental condition. Train Sent. is the number of sentence pairs in test and training data respectively. Rules is the count of unique",Sample BLEU results for all baseline and online EM model conditions. The static baseline is a traditional model that is never retrained. The batch unbounded and batch bounded models incorporate new data from the stream but retraining is slow and computationally expensive (best results are bolded). In contrast both unbounded and bounded online models incrementally retrain only the mini-batch of new sentences collected from the incoming stream so quickly adopt the new data (best results are italicized).,Unbounded LM coverage improvements. Shown are the BLEU scores for each experimental conditional when we allow the LM coverage to increase.
5,Co-Training for Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/co-training-for-smt.pdf,co-training-for-smt,Table,1,https://www.dropbox.com/request/anuCu09cu9ZyfapRaoLH,co-training-for-smt-table-1.pdf,Learning Sentential Paraphrases from Bilingual Parallel Corpora for Text-to-Text Generation,http://cis.upenn.edu/~ccb/publications/learning-sentential-paraphrases-from-bilingual-parallel-corpora.pdf,learning-sentential-paraphrases-from-bilingual-parallel-corpora,Figure,1,https://www.dropbox.com/request/NXdIZpZjW82lv4TePquv,learning-sentential-paraphrases-from-bilingual-parallel-corpora-figure-1.pdf,Learning Sentential Paraphrases from Bilingual Parallel Corpora for Text-to-Text Generation,http://cis.upenn.edu/~ccb/publications/learning-sentential-paraphrases-from-bilingual-parallel-corpora.pdf,learning-sentential-paraphrases-from-bilingual-parallel-corpora,Figure,2,https://www.dropbox.com/request/NXdIZpZjW82lv4TePquv,learning-sentential-paraphrases-from-bilingual-parallel-corpora-figure-2.pdf,Learning Sentential Paraphrases from Bilingual Parallel Corpora for Text-to-Text Generation,http://cis.upenn.edu/~ccb/publications/learning-sentential-paraphrases-from-bilingual-parallel-corpora.pdf,learning-sentential-paraphrases-from-bilingual-parallel-corpora,Figure,3,https://www.dropbox.com/request/NXdIZpZjW82lv4TePquv,learning-sentential-paraphrases-from-bilingual-parallel-corpora-figure-3.pdf,Learning Sentential Paraphrases from Bilingual Parallel Corpora for Text-to-Text Generation,http://cis.upenn.edu/~ccb/publications/learning-sentential-paraphrases-from-bilingual-parallel-corpora.pdf,learning-sentential-paraphrases-from-bilingual-parallel-corpora,Table,1,https://www.dropbox.com/request/NXdIZpZjW82lv4TePquv,learning-sentential-paraphrases-from-bilingual-parallel-corpora-table-1.pdf,Co-training results over three rounds,"Synchronous grammar rules for translation are extracted from sentence pairs in a bixtext which have been automatically parsed and word-aligned. Extraction methods vary on whether they extract only minimal rules for phrases dominated by nodes in the parse tree, or more complex rules that include non-constituent phrases.","An example derivation produced by a syntactic machine translation system. Although the synchronous trees are unlike the derivations found in the Penn Treebank, their yield is a good translation of the German.","An example of a synchronous paraphrastic derivation. A few of the rules applied in the parse are show in the left column, with the pivot phrases that gave rise to them on the right.",A selection of meaning-preserving transformations and hand-picked examples of syntactic paraphrases that our system extracts capturing these.
5,Constructing Parallel Corpora for Six Indian Languages via Crowdsourcing,http://cis.upenn.edu/~ccb/publications/constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing.pdf,constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing,Table,3,https://www.dropbox.com/request/jL5BrqIyX8byv03UaOEL,constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing-table-3.pdf,Constructing Parallel Corpora for Six Indian Languages via Crowdsourcing,http://cis.upenn.edu/~ccb/publications/constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing.pdf,constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing,Table,4,https://www.dropbox.com/request/jL5BrqIyX8byv03UaOEL,constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing-table-4.pdf,Constructing Parallel Corpora for Six Indian Languages via Crowdsourcing,http://cis.upenn.edu/~ccb/publications/constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing.pdf,constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing,Table,5,https://www.dropbox.com/request/jL5BrqIyX8byv03UaOEL,constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing-table-5.pdf,Constructing Parallel Corpora for Six Indian Languages via Crowdsourcing,http://cis.upenn.edu/~ccb/publications/constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing.pdf,constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing,Table,6,https://www.dropbox.com/request/jL5BrqIyX8byv03UaOEL,constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing-table-6.pdf,Constructing Parallel Corpora for Six Indian Languages via Crowdsourcing,http://cis.upenn.edu/~ccb/publications/constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing.pdf,constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing,Table,7,https://www.dropbox.com/request/jL5BrqIyX8byv03UaOEL,constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing-table-7.pdf,"Dictionary statistics. Entries is the number of source-language types, while translations lists the number of words or phrases they translated to (i.e., the number of pairs in the dictionary). Controls for Hindi were obtained using Google translate, the only one of these languages that were available at the outset of this project.","Data set sizes for each language pair: words in the first row, parallel sentences in the second. (The dictionaries contains short phrases in addition to words, which accounts for the difference in dictionary word and line counts.)",BLEU scores translating into English (four references). BLEU scores are the mean of three MERT runs.,Some example translations.,"BLEU scores translating into English on a quarter of the training data (plus dictionary), selected in two ways: best (result of vote), and random. There is little difference, suggesting quality control may not be terribly important. We did not collect votes for Malayalam."
5,Dirt Cheap Web-Scale Parallel Text from the Common Crawl,http://cis.upenn.edu/~ccb/publications/bitexts-from-common-crawl.pdf,bitexts-from-common-crawl,Table,4,https://www.dropbox.com/request/VqcWltaQJ8pv34oWRQ0y,bitexts-from-common-crawl-table-4.pdf,Dirt Cheap Web-Scale Parallel Text from the Common Crawl,http://cis.upenn.edu/~ccb/publications/bitexts-from-common-crawl.pdf,bitexts-from-common-crawl,Table,5,https://www.dropbox.com/request/VqcWltaQJ8pv34oWRQ0y,bitexts-from-common-crawl-table-5.pdf,Dirt Cheap Web-Scale Parallel Text from the Common Crawl,http://cis.upenn.edu/~ccb/publications/bitexts-from-common-crawl.pdf,bitexts-from-common-crawl,Table,6,https://www.dropbox.com/request/VqcWltaQJ8pv34oWRQ0y,bitexts-from-common-crawl-table-6.pdf,Dirt Cheap Web-Scale Parallel Text from the Common Crawl,http://cis.upenn.edu/~ccb/publications/bitexts-from-common-crawl.pdf,bitexts-from-common-crawl,Table,7,https://www.dropbox.com/request/VqcWltaQJ8pv34oWRQ0y,bitexts-from-common-crawl-table-7.pdf,Dirt Cheap Web-Scale Parallel Text from the Common Crawl,http://cis.upenn.edu/~ccb/publications/bitexts-from-common-crawl.pdf,bitexts-from-common-crawl,Table,8,https://www.dropbox.com/request/VqcWltaQJ8pv34oWRQ0y,bitexts-from-common-crawl-table-8.pdf,The top five domains from the Spanish-English portion of the data. The domains are ranked by the combined number of source and target tokens.,"A list of 20 topics generated using the MALLET toolkit (McCallum, 2002) and their most likely tokens.","A sample of topics along with the number of Europarl and CommonCrawl documents where they are the most likely topic in the mixture. We include topics that are mostly found in Europarl or CommonCrawl, and some that are somewhat prominent in both.",Percentage of useful (non-boilerplate) sentences found by domain and language pair. hotel.info was not found in our German-English data.,BLEU scores for several language pairs before and after adding the mined parallel data to systems trained on data from WMT data.
5,Reranking Bilingually Extracted Paraphrases Using Monolingual Distributional Similarity,http://cis.upenn.edu/~ccb/publications/reranking-bilingually-extracted-paraphrases-using-monolingual-distributional-similarity.pdf,reranking-bilingually-extracted-paraphrases-using-monolingual-distributional-similarity,Figure,3,https://www.dropbox.com/request/Tmy4m71jFzcaJ28cFzdi,reranking-bilingually-extracted-paraphrases-using-monolingual-distributional-similarity-figure-3.pdf,Reranking Bilingually Extracted Paraphrases Using Monolingual Distributional Similarity,http://cis.upenn.edu/~ccb/publications/reranking-bilingually-extracted-paraphrases-using-monolingual-distributional-similarity.pdf,reranking-bilingually-extracted-paraphrases-using-monolingual-distributional-similarity,Figure,4,https://www.dropbox.com/request/Tmy4m71jFzcaJ28cFzdi,reranking-bilingually-extracted-paraphrases-using-monolingual-distributional-similarity-figure-4.pdf,Reranking Bilingually Extracted Paraphrases Using Monolingual Distributional Similarity,http://cis.upenn.edu/~ccb/publications/reranking-bilingually-extracted-paraphrases-using-monolingual-distributional-similarity.pdf,reranking-bilingually-extracted-paraphrases-using-monolingual-distributional-similarity,Table,1,https://www.dropbox.com/request/Tmy4m71jFzcaJ28cFzdi,reranking-bilingually-extracted-paraphrases-using-monolingual-distributional-similarity-table-1.pdf,Reranking Bilingually Extracted Paraphrases Using Monolingual Distributional Similarity,http://cis.upenn.edu/~ccb/publications/reranking-bilingually-extracted-paraphrases-using-monolingual-distributional-similarity.pdf,reranking-bilingually-extracted-paraphrases-using-monolingual-distributional-similarity,Table,2,https://www.dropbox.com/request/Tmy4m71jFzcaJ28cFzdi,reranking-bilingually-extracted-paraphrases-using-monolingual-distributional-similarity-table-2.pdf,Reranking Bilingually Extracted Paraphrases Using Monolingual Distributional Similarity,http://cis.upenn.edu/~ccb/publications/reranking-bilingually-extracted-paraphrases-using-monolingual-distributional-similarity.pdf,reranking-bilingually-extracted-paraphrases-using-monolingual-distributional-similarity,Table,3,https://www.dropbox.com/request/Tmy4m71jFzcaJ28cFzdi,reranking-bilingually-extracted-paraphrases-using-monolingual-distributional-similarity-table-3.pdf,{},{},"Paraphrases for huge amount of according to the bilingual pivoting (BiP), syntactic-constrainted bilingual pivoting (SyntBiP) translation score and the monolingual similarity score via LSH (MonoDS), ranked by corresponding scores listed next to each paraphrase. Syntactic type of the phrase is [JJ+NN+IN].",Ordered reranked paraphrase candidates for the phrase reluctant according to monolingual distributional similarity (MonoDShand−selected) and bilingual pivoting paraphrase (BiP) method. Two hand-selected phrases are labeled with asterisks.,"Kendall’s Tau rank correlation coefficients be- tween human judgment of meaning and grammaticality for the different paraphrase scoring methods. Bottom panel: SyntBiPmatched is the same as SyntBiP except paraphrases must match with the original phrase in syn- tactic type. SyntBiP* and MonoDS* are the same as before except they share the same phrase support with SyntBiPmatched . (‡: MonoDS outperforms the corresponding BiP reranking at p-value ≤0.01, and † at ≤0.05)"
5,Bootstrapping Parallel Corpora,http://cis.upenn.edu/~ccb/publications/bootstrapping-parallel-corpora.pdf,bootstrapping-parallel-corpora,Figure,1,https://www.dropbox.com/request/Flk5PFXH9la0rLK6Qx2B,bootstrapping-parallel-corpora-figure-1.pdf,Bootstrapping Parallel Corpora,http://cis.upenn.edu/~ccb/publications/bootstrapping-parallel-corpora.pdf,bootstrapping-parallel-corpora,Figure,2,https://www.dropbox.com/request/Flk5PFXH9la0rLK6Qx2B,bootstrapping-parallel-corpora-figure-2.pdf,Bootstrapping Parallel Corpora,http://cis.upenn.edu/~ccb/publications/bootstrapping-parallel-corpora.pdf,bootstrapping-parallel-corpora,Figure,3,https://www.dropbox.com/request/Flk5PFXH9la0rLK6Qx2B,bootstrapping-parallel-corpora-figure-3.pdf,Bootstrapping Parallel Corpora,http://cis.upenn.edu/~ccb/publications/bootstrapping-parallel-corpora.pdf,bootstrapping-parallel-corpora,Figure,4,https://www.dropbox.com/request/Flk5PFXH9la0rLK6Qx2B,bootstrapping-parallel-corpora-figure-4.pdf,Bootstrapping Parallel Corpora,http://cis.upenn.edu/~ccb/publications/bootstrapping-parallel-corpora.pdf,bootstrapping-parallel-corpora,Table,1,https://www.dropbox.com/request/Flk5PFXH9la0rLK6Qx2B,bootstrapping-parallel-corpora-table-1.pdf,Translation accuracy plotted against training corpus size,"Co-training using German, French, and Spanish sources to produce English machine translations",“Coaching” of German to English by a French to English translation model,“Coaching” of German to English by multiple translation models,Co-training results over three rounds
5,Paraphrase Fragment Extraction from Monolingual Comparable Corpora,http://cis.upenn.edu/~ccb/publications/paraphrase-fragment-extraction-from-monolingual-comparable-corpora.pdf,paraphrase-fragment-extraction-from-monolingual-comparable-corpora,Figure,1,https://www.dropbox.com/request/LtgwnOpmt1J1zJqQx30F,paraphrase-fragment-extraction-from-monolingual-comparable-corpora-figure-1.pdf,Paraphrase Fragment Extraction from Monolingual Comparable Corpora,http://cis.upenn.edu/~ccb/publications/paraphrase-fragment-extraction-from-monolingual-comparable-corpora.pdf,paraphrase-fragment-extraction-from-monolingual-comparable-corpora,Figure,2,https://www.dropbox.com/request/LtgwnOpmt1J1zJqQx30F,paraphrase-fragment-extraction-from-monolingual-comparable-corpora-figure-2.pdf,Paraphrase Fragment Extraction from Monolingual Comparable Corpora,http://cis.upenn.edu/~ccb/publications/paraphrase-fragment-extraction-from-monolingual-comparable-corpora.pdf,paraphrase-fragment-extraction-from-monolingual-comparable-corpora,Figure,3,https://www.dropbox.com/request/LtgwnOpmt1J1zJqQx30F,paraphrase-fragment-extraction-from-monolingual-comparable-corpora-figure-3.pdf,Paraphrase Fragment Extraction from Monolingual Comparable Corpora,http://cis.upenn.edu/~ccb/publications/paraphrase-fragment-extraction-from-monolingual-comparable-corpora.pdf,paraphrase-fragment-extraction-from-monolingual-comparable-corpora,Table,1,https://www.dropbox.com/request/LtgwnOpmt1J1zJqQx30F,paraphrase-fragment-extraction-from-monolingual-comparable-corpora-table-1.pdf,Paraphrase Fragment Extraction from Monolingual Comparable Corpora,http://cis.upenn.edu/~ccb/publications/paraphrase-fragment-extraction-from-monolingual-comparable-corpora.pdf,paraphrase-fragment-extraction-from-monolingual-comparable-corpora,Table,2,https://www.dropbox.com/request/LtgwnOpmt1J1zJqQx30F,paraphrase-fragment-extraction-from-monolingual-comparable-corpora-table-2.pdf,A three stage pipeline is used to extract paraphrases from monolingual texts,Results of the sentence pair extraction. The x-axis is the threshold for the comparability scores; and the y-axis is the distribution of the annotations.,"An example of fragment pair extraction. Stop words are all set to 1 initially. Zero is the threshold, and the underscored phrases are the outputs.",Previous work in paraphrase acquisition and machine translation.,Distribution of the Extracted Fragment Pairs of our Corpus and MSR Corpus. We manually evaluated 1051 sentence pairs in all. We use LCS or word aligner as the initialization and apply n-gram-based or chunk-based phrase extraction. The first column serves as the baseline.
5,Paraphrase Fragment Extraction from Monolingual Comparable Corpora,http://cis.upenn.edu/~ccb/publications/paraphrase-fragment-extraction-from-monolingual-comparable-corpora.pdf,paraphrase-fragment-extraction-from-monolingual-comparable-corpora,Table,3,https://www.dropbox.com/request/LtgwnOpmt1J1zJqQx30F,paraphrase-fragment-extraction-from-monolingual-comparable-corpora-table-3.pdf,Paraphrase Fragment Extraction from Monolingual Comparable Corpora,http://cis.upenn.edu/~ccb/publications/paraphrase-fragment-extraction-from-monolingual-comparable-corpora.pdf,paraphrase-fragment-extraction-from-monolingual-comparable-corpora,Table,4,https://www.dropbox.com/request/LtgwnOpmt1J1zJqQx30F,paraphrase-fragment-extraction-from-monolingual-comparable-corpora-table-4.pdf,Paraphrase Fragment Extraction from Monolingual Comparable Corpora,http://cis.upenn.edu/~ccb/publications/paraphrase-fragment-extraction-from-monolingual-comparable-corpora.pdf,paraphrase-fragment-extraction-from-monolingual-comparable-corpora,Table,5,https://www.dropbox.com/request/LtgwnOpmt1J1zJqQx30F,paraphrase-fragment-extraction-from-monolingual-comparable-corpora-table-5.pdf,Use of Modality and Negation in Semantically-Informed Syntactic MT,http://cis.upenn.edu/~ccb/publications/modality-and-negation-in-semantically-informed-syntactic-mt.pdf,modality-and-negation-in-semantically-informed-syntactic-mt,Figure,1,https://www.dropbox.com/request/dK8xCckJV139gDJWcPOH,modality-and-negation-in-semantically-informed-syntactic-mt-figure-1.pdf,Use of Modality and Negation in Semantically-Informed Syntactic MT,http://cis.upenn.edu/~ccb/publications/modality-and-negation-in-semantically-informed-syntactic-mt.pdf,modality-and-negation-in-semantically-informed-syntactic-mt,Figure,2,https://www.dropbox.com/request/dK8xCckJV139gDJWcPOH,modality-and-negation-in-semantically-informed-syntactic-mt-figure-2.pdf,"The size of our corpus. We only used ca. 10%
of the GIGAWORD corpus in the experiments and the size of the collection at each stage are shown in the table",The (partial) distribution of N-grams (N=1-4) in different paraphrase collections,Some examples of the extracted paraphrase fragment pairs,Modality/Negation Tagging Examples,"An example of Urdu-English translation. Shown are an Urdu source document, a reference
translation produced by a professional human translator, and machine translation output from a
phrase-based model (Moses) without linguistic information, which is representative of
state-of-the-art MT quality before the SIMT effort"
5,Use of Modality and Negation in Semantically-Informed Syntactic MT,http://cis.upenn.edu/~ccb/publications/modality-and-negation-in-semantically-informed-syntactic-mt.pdf,modality-and-negation-in-semantically-informed-syntactic-mt,Figure,3,https://www.dropbox.com/request/dK8xCckJV139gDJWcPOH,modality-and-negation-in-semantically-informed-syntactic-mt-figure-3.pdf,Use of Modality and Negation in Semantically-Informed Syntactic MT,http://cis.upenn.edu/~ccb/publications/modality-and-negation-in-semantically-informed-syntactic-mt.pdf,modality-and-negation-in-semantically-informed-syntactic-mt,Figure,4,https://www.dropbox.com/request/dK8xCckJV139gDJWcPOH,modality-and-negation-in-semantically-informed-syntactic-mt-figure-4.pdf,Use of Modality and Negation in Semantically-Informed Syntactic MT,http://cis.upenn.edu/~ccb/publications/modality-and-negation-in-semantically-informed-syntactic-mt.pdf,modality-and-negation-in-semantically-informed-syntactic-mt,Figure,5,https://www.dropbox.com/request/dK8xCckJV139gDJWcPOH,modality-and-negation-in-semantically-informed-syntactic-mt-figure-5.pdf,Use of Modality and Negation in Semantically-Informed Syntactic MT,http://cis.upenn.edu/~ccb/publications/modality-and-negation-in-semantically-informed-syntactic-mt.pdf,modality-and-negation-in-semantically-informed-syntactic-mt,Figure,6,https://www.dropbox.com/request/dK8xCckJV139gDJWcPOH,modality-and-negation-in-semantically-informed-syntactic-mt-figure-6.pdf,Use of Modality and Negation in Semantically-Informed Syntactic MT,http://cis.upenn.edu/~ccb/publications/modality-and-negation-in-semantically-informed-syntactic-mt.pdf,modality-and-negation-in-semantically-informed-syntactic-mt,Figure,7,https://www.dropbox.com/request/dK8xCckJV139gDJWcPOH,modality-and-negation-in-semantically-informed-syntactic-mt-figure-7.pdf,"The evolution of a semantically informed approach to our synchronous context free grammars (SCFGs). At the start of summer the decoder used translation rules with a single generic non-terminal symbol, later syntactic categories were used, and by the end of the summer the translation rules included semantic elements such as modalities and negation, as well as named entities.","Eight Modalities Used for Tagging. H stands for the Holder of the modality, and P is the proposition over which the modality has scope.",Thirteen Menu Choices for Modality/Negation Annotation,Modality Lexicon Entry for need,Sample output from the structure-based MN tagger
5,Answer Extraction as Sequence Tagging with Tree Edit Distance,http://cis.upenn.edu/~ccb/publications/answer-extraction-as-sequence-tagging.pdf,answer-extraction-as-sequence-tagging,Figure,6,https://www.dropbox.com/request/LHQv7fa12JRDEzPn2dpL,answer-extraction-as-sequence-tagging-figure-6.pdf,Answer Extraction as Sequence Tagging with Tree Edit Distance,http://cis.upenn.edu/~ccb/publications/answer-extraction-as-sequence-tagging.pdf,answer-extraction-as-sequence-tagging,Table,1,https://www.dropbox.com/request/LHQv7fa12JRDEzPn2dpL,answer-extraction-as-sequence-tagging-table-1.pdf,Answer Extraction as Sequence Tagging with Tree Edit Distance,http://cis.upenn.edu/~ccb/publications/answer-extraction-as-sequence-tagging.pdf,answer-extraction-as-sequence-tagging,Table,2,https://www.dropbox.com/request/LHQv7fa12JRDEzPn2dpL,answer-extraction-as-sequence-tagging-table-2.pdf,Answer Extraction as Sequence Tagging with Tree Edit Distance,http://cis.upenn.edu/~ccb/publications/answer-extraction-as-sequence-tagging.pdf,answer-extraction-as-sequence-tagging,Table,3,https://www.dropbox.com/request/LHQv7fa12JRDEzPn2dpL,answer-extraction-as-sequence-tagging-table-3.pdf,Answer Extraction as Sequence Tagging with Tree Edit Distance,http://cis.upenn.edu/~ccb/publications/answer-extraction-as-sequence-tagging.pdf,answer-extraction-as-sequence-tagging,Table,4,https://www.dropbox.com/request/LHQv7fa12JRDEzPn2dpL,answer-extraction-as-sequence-tagging-table-4.pdf,{},Features for ranking QA pairs.,"Distribution of data, with imbalance towards negative examples (sentences without an answer).",{},Features based on edit script for answer se- quence tagging.
5,Decoding in Joshua&colon; Open Source Parsing-Based Machine Translation,http://cis.upenn.edu/~ccb/publications/decoding-in-joshua.pdf,decoding-in-joshua,Table,1,https://www.dropbox.com/request/QtMOw0cYJOS6HoYFmvOR,decoding-in-joshua-table-1.pdf,Decoding in Joshua&colon; Open Source Parsing-Based Machine Translation,http://cis.upenn.edu/~ccb/publications/decoding-in-joshua.pdf,decoding-in-joshua,Table,2,https://www.dropbox.com/request/QtMOw0cYJOS6HoYFmvOR,decoding-in-joshua-table-2.pdf,Decoding in Joshua&colon; Open Source Parsing-Based Machine Translation,http://cis.upenn.edu/~ccb/publications/decoding-in-joshua.pdf,decoding-in-joshua,Table,3,https://www.dropbox.com/request/QtMOw0cYJOS6HoYFmvOR,decoding-in-joshua-table-3.pdf,Secondary Benefits of Feedback and User Interaction in Machine Translation Tools,http://cis.upenn.edu/~ccb/publications/secondary-benefits-of-user-feedback-in-mt.pdf,secondary-benefits-of-user-feedback-in-mt,Figure,1,https://www.dropbox.com/request/L1AGDVATEVvx3jtPoBzU,secondary-benefits-of-user-feedback-in-mt-figure-1.pdf,Secondary Benefits of Feedback and User Interaction in Machine Translation Tools,http://cis.upenn.edu/~ccb/publications/secondary-benefits-of-user-feedback-in-mt.pdf,secondary-benefits-of-user-feedback-in-mt,Figure,2,https://www.dropbox.com/request/L1AGDVATEVvx3jtPoBzU,secondary-benefits-of-user-feedback-in-mt-figure-2.pdf,"An example configuration file. For conciseness, this file neglects some standard configuration options (e.g. k-best size).",Decoder Comparison: Translation speed and quality on the 2003 and 2005 NIST MT benchmark tests.,"Distributed language model: the 7-gram LM cannot be loaded alongside the SCFG on a single machine; via distributed computing, it yields significant improvement in BLEU-4 over a 5-gram.",The “Huh?” button in AmiChat,AmiWeb
5,Incremental Syntactic Language Models for Phrase-based Translation,http://cis.upenn.edu/~ccb/publications/incremental-syntactic-language-models-for-phrase-based-translation.pdf,incremental-syntactic-language-models-for-phrase-based-translation,Figure,1,https://www.dropbox.com/request/Oqtuiwy9F3QJzQwo2ijY,incremental-syntactic-language-models-for-phrase-based-translation-figure-1.pdf,Incremental Syntactic Language Models for Phrase-based Translation,http://cis.upenn.edu/~ccb/publications/incremental-syntactic-language-models-for-phrase-based-translation.pdf,incremental-syntactic-language-models-for-phrase-based-translation,Figure,2,https://www.dropbox.com/request/Oqtuiwy9F3QJzQwo2ijY,incremental-syntactic-language-models-for-phrase-based-translation-figure-2.pdf,Incremental Syntactic Language Models for Phrase-based Translation,http://cis.upenn.edu/~ccb/publications/incremental-syntactic-language-models-for-phrase-based-translation.pdf,incremental-syntactic-language-models-for-phrase-based-translation,Figure,3,https://www.dropbox.com/request/Oqtuiwy9F3QJzQwo2ijY,incremental-syntactic-language-models-for-phrase-based-translation-figure-3.pdf,Incremental Syntactic Language Models for Phrase-based Translation,http://cis.upenn.edu/~ccb/publications/incremental-syntactic-language-models-for-phrase-based-translation.pdf,incremental-syntactic-language-models-for-phrase-based-translation,Figure,4,https://www.dropbox.com/request/Oqtuiwy9F3QJzQwo2ijY,incremental-syntactic-language-models-for-phrase-based-translation-figure-4.pdf,Incremental Syntactic Language Models for Phrase-based Translation,http://cis.upenn.edu/~ccb/publications/incremental-syntactic-language-models-for-phrase-based-translation.pdf,incremental-syntactic-language-models-for-phrase-based-translation,Figure,5,https://www.dropbox.com/request/Oqtuiwy9F3QJzQwo2ijY,incremental-syntactic-language-models-for-phrase-based-translation-figure-5.pdf,"Partial decoding lattice for standard phrase-based decoding stack algorithm translating the German
sentence Der Pra ̈sident trifft am Freitag den Vorstand. Each node h in decoding stack t represents the
application of a translation option, and includes the source sentence coverage vector, target language n-gram state, and syntactic language model state τ ̃ . Hypothesis combination is also shown, indicating where lattice paths with identical n-gram histories converge. We use the English translation The president meets the board on Friday as a running example throughout all Figures.",Sample binarized phrase structure tree.,Sample binarized phrase structure tree af- ter application of right-corner transform.,"Graphical representation of the dependency structure in a standard Hierarchic Hidden Markov Model with D = 3 hidden levels that can be used to parse syntax. Circles denote random variables, and edges denote conditional dependencies. Shaded circles denote variables with observed values.",Graphical representation of the Hierarchic Hidden Markov Model after parsing input sentence The president meets the board on Friday. The shaded path through the parse lattice illustrates the recognized right-corner tree structure of Figure 3.
5,Linear B System Description for the 2005 NIST MT Evaluation Exercise,http://cis.upenn.edu/~ccb/publications/linear-b-system-description-for-nist-mt-eval-2005.pdf,linear-b-system-description-for-nist-mt-eval-2005,Figure,2,https://www.dropbox.com/request/FJt8NzXGn2CaV9gyi0M2,linear-b-system-description-for-nist-mt-eval-2005-figure-2.pdf,Linear B System Description for the 2005 NIST MT Evaluation Exercise,http://cis.upenn.edu/~ccb/publications/linear-b-system-description-for-nist-mt-eval-2005.pdf,linear-b-system-description-for-nist-mt-eval-2005,Figure,3,https://www.dropbox.com/request/FJt8NzXGn2CaV9gyi0M2,linear-b-system-description-for-nist-mt-eval-2005-figure-3.pdf,Linear B System Description for the 2005 NIST MT Evaluation Exercise,http://cis.upenn.edu/~ccb/publications/linear-b-system-description-for-nist-mt-eval-2005.pdf,linear-b-system-description-for-nist-mt-eval-2005,Figure,4,https://www.dropbox.com/request/FJt8NzXGn2CaV9gyi0M2,linear-b-system-description-for-nist-mt-eval-2005-figure-4.pdf,Semantically Informed Machine Translation (SIMT),http://cis.upenn.edu/~ccb/publications/scale-2009-report.pdf,scale-2009-report,Figure,1,https://www.dropbox.com/request/8vETuiezk5lHkdlOXJwo,scale-2009-report-figure-1.pdf,Semantically Informed Machine Translation (SIMT),http://cis.upenn.edu/~ccb/publications/scale-2009-report.pdf,scale-2009-report,Figure,2,https://www.dropbox.com/request/8vETuiezk5lHkdlOXJwo,scale-2009-report-figure-2.pdf,In the visualization condition subjects first constructed the translations by selecting from a set of probable translations of the phrases in each Arabic sentence,A comparison of Linear B’s human-aided and Edinburgh University’s fully automatic translation for article AFP20041201.0189,A comparison for article XIA20050101.0119,"An example of Urdu-English translation. Shown are an Urdu source document, a reference translation produced by a professional human translator, and machine translation output from a state-of-the-art system before the SIMT SCALE.",Translation Errors due to Missing or Incorrect Named Entities
5,SemEval-2015 Task 1&colon; Paraphrase and Semantic Similarity in Twitter,http://cis.upenn.edu/~ccb/publications/paraphrase-and-semantic-similarity-in-twitter.pdf,paraphrase-and-semantic-similarity-in-twitter,Figure,1,https://www.dropbox.com/request/1AyE3987yJaLnc3cEba6,paraphrase-and-semantic-similarity-in-twitter-figure-1.pdf,SemEval-2015 Task 1&colon; Paraphrase and Semantic Similarity in Twitter,http://cis.upenn.edu/~ccb/publications/paraphrase-and-semantic-similarity-in-twitter.pdf,paraphrase-and-semantic-similarity-in-twitter,Figure,2,https://www.dropbox.com/request/1AyE3987yJaLnc3cEba6,paraphrase-and-semantic-similarity-in-twitter-figure-2.pdf,SemEval-2015 Task 1&colon; Paraphrase and Semantic Similarity in Twitter,http://cis.upenn.edu/~ccb/publications/paraphrase-and-semantic-similarity-in-twitter.pdf,paraphrase-and-semantic-similarity-in-twitter,Figure,3,https://www.dropbox.com/request/1AyE3987yJaLnc3cEba6,paraphrase-and-semantic-similarity-in-twitter-figure-3.pdf,SemEval-2015 Task 1&colon; Paraphrase and Semantic Similarity in Twitter,http://cis.upenn.edu/~ccb/publications/paraphrase-and-semantic-similarity-in-twitter.pdf,paraphrase-and-semantic-similarity-in-twitter,Figure,4,https://www.dropbox.com/request/1AyE3987yJaLnc3cEba6,paraphrase-and-semantic-similarity-in-twitter-figure-4.pdf,SemEval-2015 Task 1&colon; Paraphrase and Semantic Similarity in Twitter,http://cis.upenn.edu/~ccb/publications/paraphrase-and-semantic-similarity-in-twitter.pdf,paraphrase-and-semantic-similarity-in-twitter,Table,1,https://www.dropbox.com/request/1AyE3987yJaLnc3cEba6,paraphrase-and-semantic-similarity-in-twitter-table-1.pdf,"A heat-map showing overlap between ex- pert and crowdsourcing annotation. The intensity along the diagonal indicates good reliability of crowdsourcing workers for this particular task; and the shift above the di- agonal reflects the difference between the two annotation schemas. For crowdsourcing (turk), the numbers indicate how many annotators out of 5 picked the sentence pair as paraphrases; 0,1 are considered non-paraphrases; 3,4,5 are paraphrases. For expert annotation, all 0,1,2 are non- paraphrases; 4,5 are paraphrases. Medium-scored cases (2 for crowdsourcing; 3 for expert annotation) are dis- carded in the system evaluation of the PI sub-task.",The proportion of paraphrases (percentage of positive votes from annotators) vary greatly across different topics. Automatic filtering in Section 4.4 roughly doubles the paraphrase yield.,"Numbers of paraphrases collected by different methods. The annotation efficiency (3,4,5 are regarded as paraphrases) is significantly improved by the sentence filtering and Multi-Armed Bandits (MAB) based topic selection.","PINC scores of paraphrases collected. The higher the PINC, the more significant the rewording. Our proposed annotation strategy quadruples paraphrase yield, while not greatly reducing diversity as measured by PINC.",Representative examples from PIT-2015 Twitter Paraphrase Corpus
5,Extracting Lexically Divergent Paraphrases from Twitter,http://cis.upenn.edu/~ccb/publications/extracting-paraphrases-from-twitter.pdf,extracting-paraphrases-from-twitter,Figure,5,https://www.dropbox.com/request/I6UkCNxBIn0jIP13S3aK,extracting-paraphrases-from-twitter-figure-5.pdf,Extracting Lexically Divergent Paraphrases from Twitter,http://cis.upenn.edu/~ccb/publications/extracting-paraphrases-from-twitter.pdf,extracting-paraphrases-from-twitter,Figure,6,https://www.dropbox.com/request/I6UkCNxBIn0jIP13S3aK,extracting-paraphrases-from-twitter-figure-6.pdf,Extracting Lexically Divergent Paraphrases from Twitter,http://cis.upenn.edu/~ccb/publications/extracting-paraphrases-from-twitter.pdf,extracting-paraphrases-from-twitter,Figure,7,https://www.dropbox.com/request/I6UkCNxBIn0jIP13S3aK,extracting-paraphrases-from-twitter-figure-7.pdf,Extracting Lexically Divergent Paraphrases from Twitter,http://cis.upenn.edu/~ccb/publications/extracting-paraphrases-from-twitter.pdf,extracting-paraphrases-from-twitter,Table,1,https://www.dropbox.com/request/I6UkCNxBIn0jIP13S3aK,extracting-paraphrases-from-twitter-table-1.pdf,Extracting Lexically Divergent Paraphrases from Twitter,http://cis.upenn.edu/~ccb/publications/extracting-paraphrases-from-twitter.pdf,extracting-paraphrases-from-twitter,Table,2,https://www.dropbox.com/request/I6UkCNxBIn0jIP13S3aK,extracting-paraphrases-from-twitter-table-2.pdf,The proportion of paraphrases (percentage of positive votes from annotators) vary greatly across different topics. Automatic filtering in Section 4.4 roughly doubles the paraphrase yield.,"Numbers of paraphrases collected by different methods. The annotation efficiency (3,4,5 are regarded as paraphrases) is significantly improved by the sentence filtering and Multi-Armed Bandits (MAB) based topic selection.","PINC scores of paraphrases collected. The higher the PINC, the more significant the rewording. Our proposed annotation strategy quadruples paraphrase yield, while not greatly reducing diversity as measured by PINC.",Representative examples from paraphrase corpora. The average sentence length is 11.9 words in Twitter vs. 18.6 in the news corpus.,Performance of different paraphrase identification approaches on Twitter version that uses additional 1.6 million sentences from Twitter. ** Reimplementation of a strong baseline used by Das and Smith (2009).
5,Joshua 5.0&colon; Sparser better faster server,http://cis.upenn.edu/~ccb/publications/joshua-5.0.pdf,joshua-5.0,Figure,2,https://www.dropbox.com/request/LkgceRLCE2gewNBvdF0u,joshua-5.0-figure-2.pdf,Joshua 5.0&colon; Sparser better faster server,http://cis.upenn.edu/~ccb/publications/joshua-5.0.pdf,joshua-5.0,Figure,3,https://www.dropbox.com/request/LkgceRLCE2gewNBvdF0u,joshua-5.0-figure-3.pdf,Joshua 5.0&colon; Sparser better faster server,http://cis.upenn.edu/~ccb/publications/joshua-5.0.pdf,joshua-5.0,Table,1,https://www.dropbox.com/request/LkgceRLCE2gewNBvdF0u,joshua-5.0-table-1.pdf,Semi-Markov Phrase-based Monolingual Alignment,http://cis.upenn.edu/~ccb/publications/semi-markov-phrase-based-monolingual-alignment.pdf,semi-markov-phrase-based-monolingual-alignment,Figure,1,https://www.dropbox.com/request/B5AUdEECPbqSDiV3cIyL,semi-markov-phrase-based-monolingual-alignment-figure-1.pdf,Semi-Markov Phrase-based Monolingual Alignment,http://cis.upenn.edu/~ccb/publications/semi-markov-phrase-based-monolingual-alignment.pdf,semi-markov-phrase-based-monolingual-alignment,Table,1,https://www.dropbox.com/request/B5AUdEECPbqSDiV3cIyL,semi-markov-phrase-based-monolingual-alignment-table-1.pdf,Decoding time alone.,"Here, position-aware lexical and part-of-speech n-gram features, labeled dependency links, and features reflecting the phrase’s CCG-style label NP/NN are included in the context vector.","Comparing Hadoop’s intermediate disk space use and extraction time on a selection of Europarl v.7 Hiero grammar extractions. Disk space was measured at its maximum, at the input of Thrax’s final grammar aggregation stage. Runtime was measured on our Hadoop cluster with a capacity of 52 mappers and 26 reducers. On average Thrax 2.0, bundled with Joshua 5.0, is up to 300% faster and more compact.","A semi-Markov phrase-based model example and the desired Viterbi decoding path. Shaded horizontal circles represent the source sentence (Shops are closed up for now until March) and hollow vertical circles represent the hidden states with state IDs for the target sentence (Shops are temporarily closed down). State 0, a NULL state, is designated for deletion. One state (e.g. state 3 and 15) can span multiple consecutive source words (a semi-Markov property) for aligning phrases on the source side. States with an ID larger than the target sentence length indicate “phrasal states” (states 6-15 in this example), where consecutive target tokens are merged for aligning phrases on the target side. Combining the semi-Markov property and phrasal states yields for instance, a 2x2 alignment between closed up in the source and closed down in the target.","Statistics of the two manually aligned corpora, divided into training and test in sentence pairs. The length column shows average lengths of source and target sentences in a pair. %align. is the percentage of aligned tokens."
5,Semi-Markov Phrase-based Monolingual Alignment,http://cis.upenn.edu/~ccb/publications/semi-markov-phrase-based-monolingual-alignment.pdf,semi-markov-phrase-based-monolingual-alignment,Table,2,https://www.dropbox.com/request/B5AUdEECPbqSDiV3cIyL,semi-markov-phrase-based-monolingual-alignment-table-2.pdf,Semi-Markov Phrase-based Monolingual Alignment,http://cis.upenn.edu/~ccb/publications/semi-markov-phrase-based-monolingual-alignment.pdf,semi-markov-phrase-based-monolingual-alignment,Table,3,https://www.dropbox.com/request/B5AUdEECPbqSDiV3cIyL,semi-markov-phrase-based-monolingual-alignment-table-3.pdf,Semi-Markov Phrase-based Monolingual Alignment,http://cis.upenn.edu/~ccb/publications/semi-markov-phrase-based-monolingual-alignment.pdf,semi-markov-phrase-based-monolingual-alignment,Table,4,https://www.dropbox.com/request/B5AUdEECPbqSDiV3cIyL,semi-markov-phrase-based-monolingual-alignment-table-4.pdf,Semi-Markov Phrase-based Monolingual Alignment,http://cis.upenn.edu/~ccb/publications/semi-markov-phrase-based-monolingual-alignment.pdf,semi-markov-phrase-based-monolingual-alignment,Table,5,https://www.dropbox.com/request/B5AUdEECPbqSDiV3cIyL,semi-markov-phrase-based-monolingual-alignment-table-5.pdf,Supervised Bilingual Lexicon Induction with Multiple Monolingual Signals,http://cis.upenn.edu/~ccb/publications/supervised-bilingual-lexicon-induction.pdf,supervised-bilingual-lexicon-induction,Figure,1,https://www.dropbox.com/request/oJs9sbAa6uUYmj49qlYg,supervised-bilingual-lexicon-induction-figure-1.pdf,"Percentage of various alignment sizes (undirectional, e.g., 1x2 and 2x1 are merged) after synthesizing phrasal alignment from token alignment in the training portion of two corpora.","esults on original (mostly token) and phrasal (P) alignment corpora, where (x%) indicates how much alignment is identical alignment, such as New$New. E% stands for exact (perfect) match rate. Subscript i stands for corresponding scores for “identical” alignment and n for “non-identical”. *: scores of MANLI-joint were for the original Edinburgh corpus instead of Edinburgh++ (with hand corrections) so it is not a direct comparison.",Same results on the phrasal Edinburgh++ corpus but with scores divided by token-only alignment (subscript t) and phrase-only alignment (subscript p).,"Results (Accuracy, Precision, Recall, Mean Average Precision, Mean Reciprocal Rank) on the tasks of RTE, PP and QA.","Each box-and-whisker plot summarizes performance on the development set using the given feature(s) across all 22 languages. For each source word in our development sets, we rank all English target words according to the monolingual similarity metric(s) listed. All but the last plot show the performance of individual features. Discrim-All uses supervised data to train classifiers for each language based on all of the features."
5,A Multi-Dialect Multi-Genre Corpus of Informal Written Arabic,http://cis.upenn.edu/~ccb/publications/arabic-dialect-corpus-2.pdf,arabic-dialect-corpus-2,Figure,6,https://www.dropbox.com/request/En36r1s7SUs7QSFYQyZb,arabic-dialect-corpus-2-figure-6.pdf,A Multi-Dialect Multi-Genre Corpus of Informal Written Arabic,http://cis.upenn.edu/~ccb/publications/arabic-dialect-corpus-2.pdf,arabic-dialect-corpus-2,Figure,7,https://www.dropbox.com/request/En36r1s7SUs7QSFYQyZb,arabic-dialect-corpus-2-figure-7.pdf,A Multi-Dialect Multi-Genre Corpus of Informal Written Arabic,http://cis.upenn.edu/~ccb/publications/arabic-dialect-corpus-2.pdf,arabic-dialect-corpus-2,Figure,8,https://www.dropbox.com/request/En36r1s7SUs7QSFYQyZb,arabic-dialect-corpus-2-figure-8.pdf,A Multi-Dialect Multi-Genre Corpus of Informal Written Arabic,http://cis.upenn.edu/~ccb/publications/arabic-dialect-corpus-2.pdf,arabic-dialect-corpus-2,Figure,9,https://www.dropbox.com/request/En36r1s7SUs7QSFYQyZb,arabic-dialect-corpus-2-figure-9.pdf,A Multi-Dialect Multi-Genre Corpus of Informal Written Arabic,http://cis.upenn.edu/~ccb/publications/arabic-dialect-corpus-2.pdf,arabic-dialect-corpus-2,Figure,10,https://www.dropbox.com/request/En36r1s7SUs7QSFYQyZb,arabic-dialect-corpus-2-figure-10.pdf,"Workers’ Performance on Arabic Commentary
HIT",{},Experiments on Extended AOC (accuracy reported),Experiments on Twitter (accuracy reported),Experiments on Extended AOC
5,Affinity Measures based on the Graph Laplacian,http://cis.upenn.edu/~ccb/publications/graph-laplacian-affinity-measures.pdf,graph-laplacian-affinity-measures,Figure,1,https://www.dropbox.com/request/Ig63UD4vPgYpLgPBfzK6,graph-laplacian-affinity-measures-figure-1.pdf,Affinity Measures based on the Graph Laplacian,http://cis.upenn.edu/~ccb/publications/graph-laplacian-affinity-measures.pdf,graph-laplacian-affinity-measures,Figure,2,https://www.dropbox.com/request/Ig63UD4vPgYpLgPBfzK6,graph-laplacian-affinity-measures-figure-2.pdf,Affinity Measures based on the Graph Laplacian,http://cis.upenn.edu/~ccb/publications/graph-laplacian-affinity-measures.pdf,graph-laplacian-affinity-measures,Figure,3,https://www.dropbox.com/request/Ig63UD4vPgYpLgPBfzK6,graph-laplacian-affinity-measures-figure-3.pdf,Affinity Measures based on the Graph Laplacian,http://cis.upenn.edu/~ccb/publications/graph-laplacian-affinity-measures.pdf,graph-laplacian-affinity-measures,Table,1,https://www.dropbox.com/request/Ig63UD4vPgYpLgPBfzK6,graph-laplacian-affinity-measures-table-1.pdf,Affinity Measures based on the Graph Laplacian,http://cis.upenn.edu/~ccb/publications/graph-laplacian-affinity-measures.pdf,graph-laplacian-affinity-measures,Table,2,https://www.dropbox.com/request/Ig63UD4vPgYpLgPBfzK6,graph-laplacian-affinity-measures-table-2.pdf,Shortest path distances on graphs,Effect of m in Bounded walk,Noise reduction via SVD.,Similarity using shortest-path measure,Similarity using bounded random walks (m = 20).
5,Using Categorial Grammar to Label Translation Rules,http://cis.upenn.edu/~ccb/publications/using-categorial-grammar-to-label-translation-rules.pdf,using-categorial-grammar-to-label-translation-rules,Figure,2,https://www.dropbox.com/request/Bgdaar9UC0LtGXG32EHR,using-categorial-grammar-to-label-translation-rules-figure-2.pdf,Using Categorial Grammar to Label Translation Rules,http://cis.upenn.edu/~ccb/publications/using-categorial-grammar-to-label-translation-rules.pdf,using-categorial-grammar-to-label-translation-rules,Figure,3,https://www.dropbox.com/request/Bgdaar9UC0LtGXG32EHR,using-categorial-grammar-to-label-translation-rules-figure-3.pdf,Using Categorial Grammar to Label Translation Rules,http://cis.upenn.edu/~ccb/publications/using-categorial-grammar-to-label-translation-rules.pdf,using-categorial-grammar-to-label-translation-rules,Figure,4,https://www.dropbox.com/request/Bgdaar9UC0LtGXG32EHR,using-categorial-grammar-to-label-translation-rules-figure-4.pdf,Using Categorial Grammar to Label Translation Rules,http://cis.upenn.edu/~ccb/publications/using-categorial-grammar-to-label-translation-rules.pdf,using-categorial-grammar-to-label-translation-rules,Figure,5,https://www.dropbox.com/request/Bgdaar9UC0LtGXG32EHR,using-categorial-grammar-to-label-translation-rules-figure-5.pdf,Using Categorial Grammar to Label Translation Rules,http://cis.upenn.edu/~ccb/publications/using-categorial-grammar-to-label-translation-rules.pdf,using-categorial-grammar-to-label-translation-rules,Table,1,https://www.dropbox.com/request/Bgdaar9UC0LtGXG32EHR,using-categorial-grammar-to-label-translation-rules-table-1.pdf,"A word-aligned sentence pair fragment, with a box indicating a consistent phrase pair.",A consistent phrase pair with a sub-phrase that is also consistent. We may extract a hierarchical SCFG rule from this training example.,A portion of the parse chart for a sentence starting with “For most people ....” Note that the gray chart cell is not included in the 1-best derivation of this fragment in Section 3.5.,"Histograms of label frequency for each model, illustrating the sparsity of each model.","An example lexicon, mapping words to categories."
5,Semantically-Informed Syntactic Machine Translation&colon; A Tree-Grafting Approach,http://cis.upenn.edu/~ccb/publications/semantically-informed-syntactic-machine-translation.pdf,semantically-informed-syntactic-machine-translation,Table,2,https://www.dropbox.com/request/xRmVR88B1h7GUwIA5Jhw,semantically-informed-syntactic-machine-translation-table-2.pdf,Semantically-Informed Syntactic Machine Translation&colon; A Tree-Grafting Approach,http://cis.upenn.edu/~ccb/publications/semantically-informed-syntactic-machine-translation.pdf,semantically-informed-syntactic-machine-translation,Table,3,https://www.dropbox.com/request/xRmVR88B1h7GUwIA5Jhw,semantically-informed-syntactic-machine-translation-table-3.pdf,Statistical Natural Language Processing,http://cis.upenn.edu/~ccb/publications/statistical-natural-language-processing-chapter.pdf,statistical-natural-language-processing-chapter,Figure,1,https://www.dropbox.com/request/i9vbguSy2uldTxBoLEUf,statistical-natural-language-processing-chapter-figure-1.pdf,Statistical Natural Language Processing,http://cis.upenn.edu/~ccb/publications/statistical-natural-language-processing-chapter.pdf,statistical-natural-language-processing-chapter,Figure,2,https://www.dropbox.com/request/i9vbguSy2uldTxBoLEUf,statistical-natural-language-processing-chapter-figure-2.pdf,Statistical Natural Language Processing,http://cis.upenn.edu/~ccb/publications/statistical-natural-language-processing-chapter.pdf,statistical-natural-language-processing-chapter,Figure,3,https://www.dropbox.com/request/i9vbguSy2uldTxBoLEUf,statistical-natural-language-processing-chapter-figure-3.pdf,Named entity tags,Modality tags with their negated versions,Two possible parses (T1 and T2) for “Do you sell Apple laptops?”.,A very simple probabilistic grammar for English,Various commonly used corpora
5,Visualizing Data Structures in Parsing-Based Machine Translation,http://cis.upenn.edu/~ccb/publications/visualizing-data-structures-in-parsing-based-machine-translation.pdf,visualizing-data-structures-in-parsing-based-machine-translation,Figure,1,https://www.dropbox.com/request/2hrOiE5hSGE3ZjnamKlL,visualizing-data-structures-in-parsing-based-machine-translation-figure-1.pdf,Visualizing Data Structures in Parsing-Based Machine Translation,http://cis.upenn.edu/~ccb/publications/visualizing-data-structures-in-parsing-based-machine-translation.pdf,visualizing-data-structures-in-parsing-based-machine-translation,Figure,2,https://www.dropbox.com/request/2hrOiE5hSGE3ZjnamKlL,visualizing-data-structures-in-parsing-based-machine-translation-figure-2.pdf,Visualizing Data Structures in Parsing-Based Machine Translation,http://cis.upenn.edu/~ccb/publications/visualizing-data-structures-in-parsing-based-machine-translation.pdf,visualizing-data-structures-in-parsing-based-machine-translation,Figure,3,https://www.dropbox.com/request/2hrOiE5hSGE3ZjnamKlL,visualizing-data-structures-in-parsing-based-machine-translation-figure-3.pdf,Visualizing Data Structures in Parsing-Based Machine Translation,http://cis.upenn.edu/~ccb/publications/visualizing-data-structures-in-parsing-based-machine-translation.pdf,visualizing-data-structures-in-parsing-based-machine-translation,Figure,4,https://www.dropbox.com/request/2hrOiE5hSGE3ZjnamKlL,visualizing-data-structures-in-parsing-based-machine-translation-figure-4.pdf,Visualizing Data Structures in Parsing-Based Machine Translation,http://cis.upenn.edu/~ccb/publications/visualizing-data-structures-in-parsing-based-machine-translation.pdf,visualizing-data-structures-in-parsing-based-machine-translation,Figure,5,https://www.dropbox.com/request/2hrOiE5hSGE3ZjnamKlL,visualizing-data-structures-in-parsing-based-machine-translation-figure-5.pdf,A hypergraph showing two candidate translations of Je suis mon maître.,The Derivation Tree browser’s sentence selection and tree-viewing windows.,An example visualization of two derivation trees for SCFGs that use a Hiero-style grammar and a syntactically-motivated grammar.,The visualization window for the hypergraph browser.,An example of bad production rules that parse pieces of the source sentence without producing any target-side output.
5,WikiTopics&colon; What is Popular on Wikipedia and Why,http://cis.upenn.edu/~ccb/publications/wikitopics-what-is-popular-on-wikipedia-and-why.pdf,wikitopics-what-is-popular-on-wikipedia-and-why,Figure,4,https://www.dropbox.com/request/DmSE834FGg4Ig3feQWQr,wikitopics-what-is-popular-on-wikipedia-and-why-figure-4.pdf,WikiTopics&colon; What is Popular on Wikipedia and Why,http://cis.upenn.edu/~ccb/publications/wikitopics-what-is-popular-on-wikipedia-and-why.pdf,wikitopics-what-is-popular-on-wikipedia-and-why,Figure,5,https://www.dropbox.com/request/DmSE834FGg4Ig3feQWQr,wikitopics-what-is-popular-on-wikipedia-and-why-figure-5.pdf,WikiTopics&colon; What is Popular on Wikipedia and Why,http://cis.upenn.edu/~ccb/publications/wikitopics-what-is-popular-on-wikipedia-and-why.pdf,wikitopics-what-is-popular-on-wikipedia-and-why,Figure,6,https://www.dropbox.com/request/DmSE834FGg4Ig3feQWQr,wikitopics-what-is-popular-on-wikipedia-and-why-figure-6.pdf,WikiTopics&colon; What is Popular on Wikipedia and Why,http://cis.upenn.edu/~ccb/publications/wikitopics-what-is-popular-on-wikipedia-and-why.pdf,wikitopics-what-is-popular-on-wikipedia-and-why,Figure,7,https://www.dropbox.com/request/DmSE834FGg4Ig3feQWQr,wikitopics-what-is-popular-on-wikipedia-and-why-figure-7.pdf,WikiTopics&colon; What is Popular on Wikipedia and Why,http://cis.upenn.edu/~ccb/publications/wikitopics-what-is-popular-on-wikipedia-and-why.pdf,wikitopics-what-is-popular-on-wikipedia-and-why,Table,1,https://www.dropbox.com/request/DmSE834FGg4Ig3feQWQr,wikitopics-what-is-popular-on-wikipedia-and-why-table-1.pdf,Log ratio of the increase in pageviews: log􏰅i = 115dik/􏰅i = 1630. Zero means no change in pageviews. WikiTopics articles show pageviews increase in a few orders of magnitude as opposed to hand-curated articles.,"Illustrative articles for January 27, 2009. WikiTopics articles here do not appear in hand-curated articles within fifteen days before or after, and vice versa. The hand-curated articles shown here are all linked from a single event “Florida hedge fund manager Arthur Nadel is arrested by the United States Federal Bureau of Investigation and charged with fraud.”","Examples of clusters: K-means clustering on the articles of January 27, 2009 and May 12, 2009. The centroid article for each cluster, defined as the closest article to the center of the cluster in vector space, is in bold.",Selected examples of temporal expressions identified by Serif from 247 such date and time expressions extracted from the article Abraham Lincoln.,Clustering evaluation: F-scores are averaged across gold standard datasets. ConComp and OneHop are using the link structure. K-means clustering with tf-idf performs best. Manual clusters were evaluated against those of the other two annotators to determine inter-annotator agreement.
5,Processing Informal Romanized Pakistani Text Messages,http://cis.upenn.edu/~ccb/publications/pakistani-SMS-corpus.pdf,pakistani-SMS-corpus,Figure,1,https://www.dropbox.com/request/H2C99uCBzGd24WV0bvLN,pakistani-SMS-corpus-figure-1.pdf,Processing Informal Romanized Pakistani Text Messages,http://cis.upenn.edu/~ccb/publications/pakistani-SMS-corpus.pdf,pakistani-SMS-corpus,Figure,2,https://www.dropbox.com/request/H2C99uCBzGd24WV0bvLN,pakistani-SMS-corpus-figure-2.pdf,Processing Informal Romanized Pakistani Text Messages,http://cis.upenn.edu/~ccb/publications/pakistani-SMS-corpus.pdf,pakistani-SMS-corpus,Figure,3,https://www.dropbox.com/request/H2C99uCBzGd24WV0bvLN,pakistani-SMS-corpus-figure-3.pdf,Processing Informal Romanized Pakistani Text Messages,http://cis.upenn.edu/~ccb/publications/pakistani-SMS-corpus.pdf,pakistani-SMS-corpus,Figure,4,https://www.dropbox.com/request/H2C99uCBzGd24WV0bvLN,pakistani-SMS-corpus-figure-4.pdf,Processing Informal Romanized Pakistani Text Messages,http://cis.upenn.edu/~ccb/publications/pakistani-SMS-corpus.pdf,pakistani-SMS-corpus,Table,1,https://www.dropbox.com/request/H2C99uCBzGd24WV0bvLN,pakistani-SMS-corpus-table-1.pdf,Example of SMS with MTurk annotations,Productivity vs. percent of annotations voted best among three deromanizations gathered on MTurk.,Urdu words romanized in multiple ways. The Urdu word for “2” is pronounced approximately “du.”,Illustration of HMM with an example from SMS data. English translation: “What’s the situation?”,"Deromanization and normalization results on 500 SMS test set. Evaluation is by character (CER) and word error rate (WER); lower scores are better. “LM” indicates the data used to estimate the language model probabilities: News refers to Urdu news corpus and SMS to deromanized side of our SMS training data. “Translit” column refers to the training data that was used to train the transliterator: SMS; SMS training data; Eng; English-Urdu transliterations. α refers to the data used to estimate emissions: transliterations, dictionary entries, or both."
5,Searchable Translation Memories,http://cis.upenn.edu/~ccb/publications/searchable-translation-memories.pdf,searchable-translation-memories,Figure,1,https://www.dropbox.com/request/q6vrQuKe6VBmbW33AeGx,searchable-translation-memories-figure-1.pdf,Searchable Translation Memories,http://cis.upenn.edu/~ccb/publications/searchable-translation-memories.pdf,searchable-translation-memories,Figure,2,https://www.dropbox.com/request/q6vrQuKe6VBmbW33AeGx,searchable-translation-memories-figure-2.pdf,Searchable Translation Memories,http://cis.upenn.edu/~ccb/publications/searchable-translation-memories.pdf,searchable-translation-memories,Figure,3,https://www.dropbox.com/request/q6vrQuKe6VBmbW33AeGx,searchable-translation-memories-figure-3.pdf,Searchable Translation Memories,http://cis.upenn.edu/~ccb/publications/searchable-translation-memories.pdf,searchable-translation-memories,Figure,4,https://www.dropbox.com/request/q6vrQuKe6VBmbW33AeGx,searchable-translation-memories-figure-4.pdf,Searchable Translation Memories,http://cis.upenn.edu/~ccb/publications/searchable-translation-memories.pdf,searchable-translation-memories,Figure,5,https://www.dropbox.com/request/q6vrQuKe6VBmbW33AeGx,searchable-translation-memories-figure-5.pdf,Search results for the French phrase “paiement initial”,A word-level alignment for a sentence pair that occurs in our training data,Extracting incrementally larger phrases from a word alignment,A sample of the evaluation data used to produce precision and recall results,"A chart of the various sub-sentential segments that were retrieved for an Arabic sentence, along
with their associated probabilities"
5,Adding Semantics to Data-Driven Paraphrasing,http://cis.upenn.edu/~ccb/publications/adding-semantics-to-data-driven-paraphrasing.pdf,adding-semantics-to-data-driven-paraphrasing,Figure,8,https://www.dropbox.com/request/ywJ0k3Xv3T9YT4CDrDWS,adding-semantics-to-data-driven-paraphrasing-figure-8.pdf,Adding Semantics to Data-Driven Paraphrasing,http://cis.upenn.edu/~ccb/publications/adding-semantics-to-data-driven-paraphrasing.pdf,adding-semantics-to-data-driven-paraphrasing,Table,1,https://www.dropbox.com/request/ywJ0k3Xv3T9YT4CDrDWS,adding-semantics-to-data-driven-paraphrasing-table-1.pdf,Adding Semantics to Data-Driven Paraphrasing,http://cis.upenn.edu/~ccb/publications/adding-semantics-to-data-driven-paraphrasing.pdf,adding-semantics-to-data-driven-paraphrasing,Table,2,https://www.dropbox.com/request/ywJ0k3Xv3T9YT4CDrDWS,adding-semantics-to-data-driven-paraphrasing-table-2.pdf,Adding Semantics to Data-Driven Paraphrasing,http://cis.upenn.edu/~ccb/publications/adding-semantics-to-data-driven-paraphrasing.pdf,adding-semantics-to-data-driven-paraphrasing,Table,3,https://www.dropbox.com/request/ywJ0k3Xv3T9YT4CDrDWS,adding-semantics-to-data-driven-paraphrasing-table-3.pdf,Adding Semantics to Data-Driven Paraphrasing,http://cis.upenn.edu/~ccb/publications/adding-semantics-to-data-driven-paraphrasing.pdf,adding-semantics-to-data-driven-paraphrasing,Table,4,https://www.dropbox.com/request/ywJ0k3Xv3T9YT4CDrDWS,adding-semantics-to-data-driven-paraphrasing-table-4.pdf,"F1 measures achieved by Nutcracker on SICK test data when using various KBs. Baselines are in gray, this work in blue, human references in gold. PPDB-XL refers to a run in which every pair which appears in PPDB is assumed to be equivalent. PPDB-H refers to a run in which manual labels were used to generate axioms. PPDB+ refers to runs in which the automatic classifications were used to generate axioms. In some cases, better proof coverage causes NC to find incorrect proofs, illustrated by the decreased performance on CONTRADICTION when using PPDB-H. For example, using PPDB-H, NC finds an inconsistency for the pair Someone is not playing piano./A person is playing a keyboard. Using the PPDB+, in which piano/keyboard is falsely classified as #, NC fails to find a proof and so correctly guesses NEUTRAL.",Examples of different types of entailment relations appearing in PPDB.,Column 1 gives the semantics of each label under MacCartney’s Natural Logic. Column 2 gives the notation we use throughout the remainder of this paper. Column 3 gives the description that was shown to Turkers.,"Top scoring pairs (x/y) according to various similarity measures, along with their manually classified entailment labels. Column 1 is cosine similarity based on dependency contexts. Column 2 is based on Lin (1998), column 3 on Weeds (2004), and column 4 is a novel feature. Precise definitions of each metric are given in the supplementary material.",Top paths associated with the ¬ class.
5,Adding Semantics to Data-Driven Paraphrasing,http://cis.upenn.edu/~ccb/publications/adding-semantics-to-data-driven-paraphrasing.pdf,adding-semantics-to-data-driven-paraphrasing,Table,5,https://www.dropbox.com/request/ywJ0k3Xv3T9YT4CDrDWS,adding-semantics-to-data-driven-paraphrasing-table-5.pdf,Adding Semantics to Data-Driven Paraphrasing,http://cis.upenn.edu/~ccb/publications/adding-semantics-to-data-driven-paraphrasing.pdf,adding-semantics-to-data-driven-paraphrasing,Table,6,https://www.dropbox.com/request/ywJ0k3Xv3T9YT4CDrDWS,adding-semantics-to-data-driven-paraphrasing-table-6.pdf,Adding Semantics to Data-Driven Paraphrasing,http://cis.upenn.edu/~ccb/publications/adding-semantics-to-data-driven-paraphrasing.pdf,adding-semantics-to-data-driven-paraphrasing,Table,7,https://www.dropbox.com/request/ywJ0k3Xv3T9YT4CDrDWS,adding-semantics-to-data-driven-paraphrasing-table-7.pdf,Adding Semantics to Data-Driven Paraphrasing,http://cis.upenn.edu/~ccb/publications/adding-semantics-to-data-driven-paraphrasing.pdf,adding-semantics-to-data-driven-paraphrasing,Table,8,https://www.dropbox.com/request/ywJ0k3Xv3T9YT4CDrDWS,adding-semantics-to-data-driven-paraphrasing-table-8.pdf,Adding Semantics to Data-Driven Paraphrasing,http://cis.upenn.edu/~ccb/publications/adding-semantics-to-data-driven-paraphrasing.pdf,adding-semantics-to-data-driven-paraphrasing,Table,9,https://www.dropbox.com/request/ywJ0k3Xv3T9YT4CDrDWS,adding-semantics-to-data-driven-paraphrasing-table-9.pdf,F1 measure (×100) achieved by entailment classifier using 10-fold cross validation on the training data.,Example misclassifications from some of the most frequent and most interesting error categories.,F1 measure (×100) achieved by entailment classifier on the held out phrase pairs from the sentences in SICK test.,"Nutcracker’s overall system accuracy and proof coverage when using different sources of axioms. Coverage is measured as the percent of sentence pairs for which NC’s theorem prover or model builder is able to find a complete logical proof of either entailment or contradiction. When NC fails to find either type of proof, it guesses the most frequent class, NEUTRAL. NC alone uses no axioms. PPDB+ refers to the axioms generated automatically using the classifier described in this paper. PPDB-H refers axioms generated using the human labels on which the classifier was trained.",Examples of T/H pairs for which the system’s prediction differed when using PPDB+ vs. WN.
5,Findings of the 2012 Workshop on Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-the-wmt12-shared-tasks.pdf,findings-of-the-wmt12-shared-tasks,Table,6,https://www.dropbox.com/request/OHSPihXt2D3RcmClBFNh,findings-of-the-wmt12-shared-tasks-table-6.pdf,Findings of the 2012 Workshop on Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-the-wmt12-shared-tasks.pdf,findings-of-the-wmt12-shared-tasks,Table,7,https://www.dropbox.com/request/OHSPihXt2D3RcmClBFNh,findings-of-the-wmt12-shared-tasks-table-7.pdf,Findings of the 2012 Workshop on Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-the-wmt12-shared-tasks.pdf,findings-of-the-wmt12-shared-tasks,Table,8,https://www.dropbox.com/request/OHSPihXt2D3RcmClBFNh,findings-of-the-wmt12-shared-tasks-table-8.pdf,Findings of the 2012 Workshop on Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-the-wmt12-shared-tasks.pdf,findings-of-the-wmt12-shared-tasks,Table,9,https://www.dropbox.com/request/OHSPihXt2D3RcmClBFNh,findings-of-the-wmt12-shared-tasks-table-9.pdf,Findings of the 2012 Workshop on Statistical Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-the-wmt12-shared-tasks.pdf,findings-of-the-wmt12-shared-tasks,Table,10,https://www.dropbox.com/request/OHSPihXt2D3RcmClBFNh,findings-of-the-wmt12-shared-tasks-table-10.pdf,Participants in the metrics task.,"System-level Spearman’s rho correlation of the automatic evaluation metrics with the human judgments for translation into English, ordered by average absolute value.","System-level Spearman’s rho correlation of the automatic evaluation metrics with the human judgments for translation out of English, ordered by average absolute value.","Segment-level Kendall’s tau correlation of the automatic evaluation metrics with the human judgments for translation into English, ordered by average correlation.","Segment-level Kendall’s tau correlation of the automatic evaluation metrics with the human judgments for translation out of English, ordered by average correlation."
5,Findings of the 2010 Joint Workshop on Statistical Machine Translation and Metrics for Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-wmt10-and-metrics-matr.pdf,findings-of-wmt10-and-metrics-matr,Table,8,https://www.dropbox.com/request/dibLQtLbNWMhiTENIaYG,findings-of-wmt10-and-metrics-matr-table-8.pdf,Findings of the 2010 Joint Workshop on Statistical Machine Translation and Metrics for Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-wmt10-and-metrics-matr.pdf,findings-of-wmt10-and-metrics-matr,Table,9,https://www.dropbox.com/request/dibLQtLbNWMhiTENIaYG,findings-of-wmt10-and-metrics-matr-table-9.pdf,Findings of the 2010 Joint Workshop on Statistical Machine Translation and Metrics for Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-wmt10-and-metrics-matr.pdf,findings-of-wmt10-and-metrics-matr,Table,10,https://www.dropbox.com/request/dibLQtLbNWMhiTENIaYG,findings-of-wmt10-and-metrics-matr-table-10.pdf,Findings of the 2010 Joint Workshop on Statistical Machine Translation and Metrics for Machine Translation,http://cis.upenn.edu/~ccb/publications/findings-of-wmt10-and-metrics-matr.pdf,findings-of-wmt10-and-metrics-matr,Table,11,https://www.dropbox.com/request/dibLQtLbNWMhiTENIaYG,findings-of-wmt10-and-metrics-matr-table-11.pdf,Syntactic Constraints on Paraphrases Extracted from Parallel Corpora,http://cis.upenn.edu/~ccb/publications/syntactic-constraints-on-paraphrases.pdf,syntactic-constraints-on-paraphrases,Figure,1,https://www.dropbox.com/request/HBNFh42pf4ajDkiUQGcT,syntactic-constraints-on-paraphrases-figure-1.pdf,"System-level Spearman’s rho correlation of the automatic evaluation metrics with the human judgments for translation out of English, ordered by average absolute value.","Segment-level Kendall’s tau correlation of the automatic evaluation metrics with the human judgments for translation into English,  ordered by average absolute value. Number of pairs included in comparison: cz-en 3575, fr-en 5844, de-en 7585, es-en 7911.","Segment-level Kendall’s tau correlation of the automatic evaluation metrics with the human judgments for translation out of English, ordered by average absolute value. Number of pairs included in comparison: en-cz 9613, en-fr 5904, en-de 10892, en-es 3813.","Statistics for data collected on MTurk for the ranking task. In total, 55,082 rank labels were collected across the eight language pairs (145% of expert data). Each language pair had 600 sets, and we requested each set completed by 5 different workers. Since each set provides 5 labels, we could have potentially obtained 600 _ 5 _ 5 = 15,000 labels for each language pair. The Label count row indicates to what extent that potential was met (over the 30-day lifetime of our tasks), and the “Completed...” rows give a breakdown of redundancy. For instance, the right-most column indicates that, in the cz-en group, 2.0% of the 600 sets were completed by only one worker, while 67% of the sets were completed by 5 workers, with 100% of the sets completed at least once. The total cost of this data collection effort was roughly $200.","Statistics for the training and test sets used in the translation task. The number of words and
the number of distinct words is based on the provided tokenizer"
5,ParaMetric&colon; An Automatic Evaluation Metric for Paraphrasing,http://cis.upenn.edu/~ccb/publications/parametric.pdf,parametric,Table,2,https://www.dropbox.com/request/9bMDMjMmdza9lzUbBPs9,parametric-table-2.pdf,ParaMetric&colon; An Automatic Evaluation Metric for Paraphrasing,http://cis.upenn.edu/~ccb/publications/parametric.pdf,parametric,Table,3,https://www.dropbox.com/request/9bMDMjMmdza9lzUbBPs9,parametric-table-3.pdf,The Arabic Online Commentary Dataset&colon; An Annotated Dataset of Informal Arabic with High Dialectal Content,http://cis.upenn.edu/~ccb/publications/arabic-dialect-corpus.pdf,arabic-dialect-corpus,Figure,1,https://www.dropbox.com/request/v3nq1tDIp8o1Jne7l3eY,arabic-dialect-corpus-figure-1.pdf,The Arabic Online Commentary Dataset&colon; An Annotated Dataset of Informal Arabic with High Dialectal Content,http://cis.upenn.edu/~ccb/publications/arabic-dialect-corpus.pdf,arabic-dialect-corpus,Figure,2,https://www.dropbox.com/request/v3nq1tDIp8o1Jne7l3eY,arabic-dialect-corpus-figure-2.pdf,The Arabic Online Commentary Dataset&colon; An Annotated Dataset of Informal Arabic with High Dialectal Content,http://cis.upenn.edu/~ccb/publications/arabic-dialect-corpus.pdf,arabic-dialect-corpus,Figure,3,https://www.dropbox.com/request/v3nq1tDIp8o1Jne7l3eY,arabic-dialect-corpus-figure-3.pdf,Summary results for scoring the different paraphrasing techniques using our proposed automatic evaluations.,Results for paraphrases of continuous subphrases of various lengths.,"Two roughly equivalent Arabic sentences, one in MSA and one in Levantine Arabic, translated by the same MT system into English. An acceptable translation  would be When will we see this group of criminals undergo trial (or tried)?. The MSA variant is handled well, while the dialectal variant is mostly transliterated","One possible breakdown of spoken Arabic into dialect groups: Maghrebi, Egyptian, Levantine, Gulf, and Iraqi. Habash (2010) also gives a very similar breakdown",Dialect precision vs. recall for the classification task over Al-Ghad sentences (MSA vs. Levantine). The square point corresponds to the first line in Table 3.
5,Bootstrapping Parallel Corpora,http://cis.upenn.edu/~ccb/publications/bootstrapping-parallel-corpora.pdf,bootstrapping-parallel-corpora,Figure,1,https://www.dropbox.com/request/Flk5PFXH9la0rLK6Qx2B,bootstrapping-parallel-corpora-figure-1.pdf,Bootstrapping Parallel Corpora,http://cis.upenn.edu/~ccb/publications/bootstrapping-parallel-corpora.pdf,bootstrapping-parallel-corpora,Figure,2,https://www.dropbox.com/request/Flk5PFXH9la0rLK6Qx2B,bootstrapping-parallel-corpora-figure-2.pdf,Bootstrapping Parallel Corpora,http://cis.upenn.edu/~ccb/publications/bootstrapping-parallel-corpora.pdf,bootstrapping-parallel-corpora,Figure,3,https://www.dropbox.com/request/Flk5PFXH9la0rLK6Qx2B,bootstrapping-parallel-corpora-figure-3.pdf,Bootstrapping Parallel Corpora,http://cis.upenn.edu/~ccb/publications/bootstrapping-parallel-corpora.pdf,bootstrapping-parallel-corpora,Figure,4,https://www.dropbox.com/request/Flk5PFXH9la0rLK6Qx2B,bootstrapping-parallel-corpora-figure-4.pdf,Bootstrapping Parallel Corpora,http://cis.upenn.edu/~ccb/publications/bootstrapping-parallel-corpora.pdf,bootstrapping-parallel-corpora,Table,1,https://www.dropbox.com/request/Flk5PFXH9la0rLK6Qx2B,bootstrapping-parallel-corpora-table-1.pdf,Translation accuracy plotted against training corpus size,"Co-training using German, French, and Spanish sources to produce English machine translations",“Coaching” of German to English by a French to English translation model,“Coaching” of German to English by multiple translation models,Co-training results over three rounds
5,Semi-Markov Phrase-based Monolingual Alignment,http://cis.upenn.edu/~ccb/publications/semi-markov-phrase-based-monolingual-alignment.pdf,semi-markov-phrase-based-monolingual-alignment,Table,2,https://www.dropbox.com/request/B5AUdEECPbqSDiV3cIyL,semi-markov-phrase-based-monolingual-alignment-table-2.pdf,Semi-Markov Phrase-based Monolingual Alignment,http://cis.upenn.edu/~ccb/publications/semi-markov-phrase-based-monolingual-alignment.pdf,semi-markov-phrase-based-monolingual-alignment,Table,3,https://www.dropbox.com/request/B5AUdEECPbqSDiV3cIyL,semi-markov-phrase-based-monolingual-alignment-table-3.pdf,Semi-Markov Phrase-based Monolingual Alignment,http://cis.upenn.edu/~ccb/publications/semi-markov-phrase-based-monolingual-alignment.pdf,semi-markov-phrase-based-monolingual-alignment,Table,4,https://www.dropbox.com/request/B5AUdEECPbqSDiV3cIyL,semi-markov-phrase-based-monolingual-alignment-table-4.pdf,Semi-Markov Phrase-based Monolingual Alignment,http://cis.upenn.edu/~ccb/publications/semi-markov-phrase-based-monolingual-alignment.pdf,semi-markov-phrase-based-monolingual-alignment,Table,5,https://www.dropbox.com/request/B5AUdEECPbqSDiV3cIyL,semi-markov-phrase-based-monolingual-alignment-table-5.pdf,Supervised Bilingual Lexicon Induction with Multiple Monolingual Signals,http://cis.upenn.edu/~ccb/publications/supervised-bilingual-lexicon-induction.pdf,supervised-bilingual-lexicon-induction,Figure,1,https://www.dropbox.com/request/oJs9sbAa6uUYmj49qlYg,supervised-bilingual-lexicon-induction-figure-1.pdf,"Percentage of various alignment sizes (undirectional, e.g., 1x2 and 2x1 are merged) after synthesizing phrasal alignment from token alignment in the training portion of two corpora.","Results on original (mostly token) and phrasal (P) alignment corpora, where (x%) indicates how much alignment is identical alignment, such as New$New. E% stands for exact (perfect) match rate. Subscript i stands for corresponding scores for “identical” alignment and n for “non-identical”. *: scores of MANLI-joint were for the original Edinburgh corpus instead of Edinburgh++ (with hand corrections) so it is not a direct comparison.",Same results on the phrasal Edinburgh++ corpus but with scores divided by token-only alignment (subscript t) and phrase-only alignment  (subscript p).,"Results (Accuracy, Precision, Recall, Mean Average Precision, Mean Reciprocal Rank) on the tasks of RTE, PP and QA","Each box-and-whisker plot summarizes performance on the development set using the given feature(s) across all 22 languages. For each source word in our development sets, we rank all English target words according to the monolingual similarity metric(s) listed. All but the last plot show the performance of individual features. Discrim-All uses supervised data to train classifiers for each language based on all of the features."
5,Using Categorial Grammar to Label Translation Rules,http://cis.upenn.edu/~ccb/publications/using-categorial-grammar-to-label-translation-rules.pdf,using-categorial-grammar-to-label-translation-rules,Figure,2,https://www.dropbox.com/request/Bgdaar9UC0LtGXG32EHR,using-categorial-grammar-to-label-translation-rules-figure-2.pdf,Using Categorial Grammar to Label Translation Rules,http://cis.upenn.edu/~ccb/publications/using-categorial-grammar-to-label-translation-rules.pdf,using-categorial-grammar-to-label-translation-rules,Figure,3,https://www.dropbox.com/request/Bgdaar9UC0LtGXG32EHR,using-categorial-grammar-to-label-translation-rules-figure-3.pdf,Using Categorial Grammar to Label Translation Rules,http://cis.upenn.edu/~ccb/publications/using-categorial-grammar-to-label-translation-rules.pdf,using-categorial-grammar-to-label-translation-rules,Figure,4,https://www.dropbox.com/request/Bgdaar9UC0LtGXG32EHR,using-categorial-grammar-to-label-translation-rules-figure-4.pdf,Using Categorial Grammar to Label Translation Rules,http://cis.upenn.edu/~ccb/publications/using-categorial-grammar-to-label-translation-rules.pdf,using-categorial-grammar-to-label-translation-rules,Figure,5,https://www.dropbox.com/request/Bgdaar9UC0LtGXG32EHR,using-categorial-grammar-to-label-translation-rules-figure-5.pdf,Using Categorial Grammar to Label Translation Rules,http://cis.upenn.edu/~ccb/publications/using-categorial-grammar-to-label-translation-rules.pdf,using-categorial-grammar-to-label-translation-rules,Table,1,https://www.dropbox.com/request/Bgdaar9UC0LtGXG32EHR,using-categorial-grammar-to-label-translation-rules-table-1.pdf,"A word-aligned sentence pair fragment, with a box indicating a consistent phrase pair.",A consistent phrase pair with a sub-phrase that is also consistent. We may extract a hierarchical SCFG rule from this training example.,A portion of the parse chart for a sentence starting with “For most people . . . .” Note that the gray chart cell is not included in the 1-best derivation of this fragment in Section 3.5.,"Histograms of label frequency for each model, illustrating the sparsity of each model.","An example lexicon, mapping words to categories."
5,Statistical Natural Language Processing,http://cis.upenn.edu/~ccb/publications/statistical-natural-language-processing-chapter.pdf,statistical-natural-language-processing-chapter,Figure,4,https://www.dropbox.com/request/i9vbguSy2uldTxBoLEUf,statistical-natural-language-processing-chapter-figure-4.pdf,Toward Statistical Machine Translation without Parallel Corpora,http://cis.upenn.edu/~ccb/publications/toward-statistical-machine-translation-without-parallel-corpora.pdf,toward-statistical-machine-translation-without-parallel-corpora,Figure,1,https://www.dropbox.com/request/5QeuKHR0w2Od1QNFXzRv,toward-statistical-machine-translation-without-parallel-corpora-figure-1.pdf,Toward Statistical Machine Translation without Parallel Corpora,http://cis.upenn.edu/~ccb/publications/toward-statistical-machine-translation-without-parallel-corpora.pdf,toward-statistical-machine-translation-without-parallel-corpora,Figure,2,https://www.dropbox.com/request/5QeuKHR0w2Od1QNFXzRv,toward-statistical-machine-translation-without-parallel-corpora-figure-2.pdf,Toward Statistical Machine Translation without Parallel Corpora,http://cis.upenn.edu/~ccb/publications/toward-statistical-machine-translation-without-parallel-corpora.pdf,toward-statistical-machine-translation-without-parallel-corpora,Figure,3,https://www.dropbox.com/request/5QeuKHR0w2Od1QNFXzRv,toward-statistical-machine-translation-without-parallel-corpora-figure-3.pdf,Toward Statistical Machine Translation without Parallel Corpora,http://cis.upenn.edu/~ccb/publications/toward-statistical-machine-translation-without-parallel-corpora.pdf,toward-statistical-machine-translation-without-parallel-corpora,Figure,4,https://www.dropbox.com/request/5QeuKHR0w2Od1QNFXzRv,toward-statistical-machine-translation-without-parallel-corpora-figure-4.pdf,"Two possible parses (T1 and T2) for \Do you sell Apple laptops?"".","The reordering probabilities from the phrasebased models are estimated from bilingual data by calculating how often in the parallel corpus a phrase pair (f; e) is orientated with the preceding phrase pair in the 3 types of orientations (monotone, swapped, and discontinuous).",Accuracy of single-word translations induced using contextual similarity as a function of the source word corpus frequency. Accuracy is the proportion of the source words with at least one correct (bilingual dictionary) translation in the top 1 and top 10 candidate lists.,"Scoring contextual similarity of phrases: first, contextual vectors are projected using a small seed dictionary and then compared with the target language candidates.","Temporal histograms of the English phrase terrorist, its Spanish translation terrorista, and riqueza (wealth) collected from monolingual texts spanning a 13 year period. While the correct translation has a good temporal match, the non-translation riqueza has a distinctly different signature."
5,A Computer Model of a Grammar for English Questions,http://cis.upenn.edu/~ccb/publications/computer-model-of-a-grammar-for-english-questions.pdf,computer-model-of-a-grammar-for-english-questions,Figure,5,https://www.dropbox.com/request/nKm1uDTYDZdHfJhbbsSW,computer-model-of-a-grammar-for-english-questions-figure-5.pdf,A Computer Model of a Grammar for English Questions,http://cis.upenn.edu/~ccb/publications/computer-model-of-a-grammar-for-english-questions.pdf,computer-model-of-a-grammar-for-english-questions,Figure,6,https://www.dropbox.com/request/nKm1uDTYDZdHfJhbbsSW,computer-model-of-a-grammar-for-english-questions-figure-6.pdf,A Computer Model of a Grammar for English Questions,http://cis.upenn.edu/~ccb/publications/computer-model-of-a-grammar-for-english-questions.pdf,computer-model-of-a-grammar-for-english-questions,Figure,7,https://www.dropbox.com/request/nKm1uDTYDZdHfJhbbsSW,computer-model-of-a-grammar-for-english-questions-figure-7.pdf,A Computer Model of a Grammar for English Questions,http://cis.upenn.edu/~ccb/publications/computer-model-of-a-grammar-for-english-questions.pdf,computer-model-of-a-grammar-for-english-questions,Figure,8,https://www.dropbox.com/request/nKm1uDTYDZdHfJhbbsSW,computer-model-of-a-grammar-for-english-questions-figure-8.pdf,A Computer Model of a Grammar for English Questions,http://cis.upenn.edu/~ccb/publications/computer-model-of-a-grammar-for-english-questions.pdf,computer-model-of-a-grammar-for-english-questions,Figure,9,https://www.dropbox.com/request/nKm1uDTYDZdHfJhbbsSW,computer-model-of-a-grammar-for-english-questions-figure-9.pdf,{},{},{},{},{}